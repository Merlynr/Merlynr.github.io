<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>吾辈之人，自当自强不息！</title>
  
  <subtitle>博客</subtitle>
  <link href="https://merlynr.github.io/atom.xml" rel="self"/>
  
  <link href="https://merlynr.github.io/"/>
  <updated>2022-12-16T03:39:00.000Z</updated>
  <id>https://merlynr.github.io/</id>
  
  <author>
    <name>Merlynr</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>基于ESP32S3的FreeRTOS之事件组(五)</title>
    <link href="https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E4%BA%8B%E4%BB%B6%E7%BB%84(%E4%BA%94)/"/>
    <id>https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E4%BA%8B%E4%BB%B6%E7%BB%84(%E4%BA%94)/</id>
    <published>2022-12-16T03:39:00.000Z</published>
    <updated>2022-12-16T03:39:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="事件组等待"><a href="#事件组等待" class="headerlink" title="事件组等待"></a>事件组等待</h2><p><img src="https://files.shanqianche.cn/202212/1671172487143.png" alt="创建事件组，返回一个event group句柄"></p><p><img src="https://files.shanqianche.cn/202212/1671173162132.png" alt="查看当前事件组的位是否被设置，如果设置了则跳过，否则发生阻塞"></p><p><img src="https://files.shanqianche.cn/202212/1671173611753.png" alt="设置事件组的值"></p><blockquote><p>设置事件组大小，如果宏定义USE_16_BIT_TICKS设置为1则，事件组大小为8；如果为0则为24位。</p></blockquote><p><img src="https://files.shanqianche.cn/202212/1671172754108.png" alt="Esp32的配置"></p><p><img src="https://files.shanqianche.cn/202212/1671174406593.png" alt="创建事件组"></p><p><img src="https://files.shanqianche.cn/202212/1671174563092.png" alt="事件组的判断，零位 | 四位 为1."></p><p><img src="https://files.shanqianche.cn/202212/1671174626150.png" alt="Task2中对事件组0位和四位依次设置为1"></p><p><img src="https://files.shanqianche.cn/202212/1671174723965.png" alt="条件为或时的结果"></p><p><img src="https://files.shanqianche.cn/202212/1671174771887.png" alt="条件为与的结果"></p><h2 id="事件组同"><a href="#事件组同" class="headerlink" title="事件组同"></a>事件组同</h2><blockquote><p>事件组等待的执行逻辑</p></blockquote><p><img src="https://files.shanqianche.cn/202212/1671175636282.png" alt="Step1：task1阻塞，task2，task3依次设置位"></p><p><img src="https://files.shanqianche.cn/202212/1671175673767.png" alt="Step2：task1等待task2，3完成后开始设置自己的位"></p><blockquote><p>事件组同步的执行逻辑</p></blockquote><p><img src="https://files.shanqianche.cn/202212/1671175874589.png" alt="事件组同步"></p><p><img src="https://files.shanqianche.cn/202212/1671175906159.png" alt="Step1：三个事件依次设置，并等待"></p><p><img src="https://files.shanqianche.cn/202212/1671175935762.png" alt="Step2：同时开始"></p><p><strong>code</strong></p><p><img src="https://files.shanqianche.cn/202212/1671176947195.png" alt="BITS"></p><p><img src="https://files.shanqianche.cn/202212/1671177513328.png" alt="创建事件组"></p><p><img src="https://files.shanqianche.cn/202212/1671177624437.png" alt="Task0"></p><p><img src="https://files.shanqianche.cn/202212/1671177635589.png" alt="Task1"></p><p><img src="https://files.shanqianche.cn/202212/1671177641240.png" alt="Task2"></p><p><img src="https://files.shanqianche.cn/202212/1671177663458.png" alt="结果"></p>]]></content>
    
    
    <summary type="html">description</summary>
    
    
    
    <category term="RTOS" scheme="https://merlynr.github.io/categories/RTOS/"/>
    
    
    <category term="study, graduate student,RTOS" scheme="https://merlynr.github.io/tags/study-graduate-student-RTOS/"/>
    
  </entry>
  
  <entry>
    <title>基于ESP32S3的FreeRTOS之定时器(四)</title>
    <link href="https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E5%AE%9A%E6%97%B6%E5%99%A8(%E5%9B%9B)/"/>
    <id>https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E5%AE%9A%E6%97%B6%E5%99%A8(%E5%9B%9B)/</id>
    <published>2022-12-16T03:37:00.000Z</published>
    <updated>2022-12-16T03:37:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Software-Timer-软件定时器"><a href="#Software-Timer-软件定时器" class="headerlink" title="Software Timer 软件定时器"></a>Software Timer 软件定时器</h2><blockquote><p>基于Daemon Task，定时器任务通过定时器命令队列进行发送给执行指令，然后任务调用相应的程序，执行软件定时器的回调函数。</p></blockquote><p><img src="https://files.shanqianche.cn/202212/1670983470929.png" alt="Software Timer"></p><p><strong>软件定时器的特点</strong></p><ol><li>不受硬件影响，不受MCU的影响</li><li>软件定时器的数量与堆栈，TIMER_TASK_STACK_DEPTH的影响。不受影响影响</li></ol><p><img src="https://files.shanqianche.cn/202212/1670983728618.png" alt="xTimerCreate"></p><p><img src="https://files.shanqianche.cn/202212/1670984266679.png" alt="启动定时器"></p><p><img src="https://files.shanqianche.cn/202212/1670984718155.png" alt="xTimerStop"></p><p><img src="https://files.shanqianche.cn/202212/1670984850319.png" alt="取得Timer的名字"></p><p><img src="https://files.shanqianche.cn/202212/1670985084226.png" alt="获取Timer ID，返回一个指针"></p><p><img src="https://files.shanqianche.cn/202212/1670985575931.png" alt="只要执行reset函数，定时器将不执行回调函数，类似于喂狗"></p><p><img src="https://files.shanqianche.cn/202212/1670985661744.png" alt="改变定时器周期"></p><p><strong>创建并启动定时器</strong></p><p><img src="https://files.shanqianche.cn/202212/1670984663776.png" alt="创建并启动定时器"><br><strong>延时并停止</strong></p><p><img src="https://files.shanqianche.cn/202212/1670984772340.png" alt="延时等待6s并停止"></p><p><img src="https://files.shanqianche.cn/202212/1670984803629.png" alt="result"></p><p><strong>获取Timer名字</strong></p><p><img src="https://files.shanqianche.cn/202212/1670984974432.png" alt="公用一个回调函数"></p><p><img src="https://files.shanqianche.cn/202212/1670985000934.png" alt="由于两秒执行一次Timer2，一秒一次Timer1；所以结果如此"></p><p><strong>获取一个Timer ID</strong></p><p><img src="https://files.shanqianche.cn/202212/1670985351262.png" alt="方法一，Get Timer ID"></p><p><img src="https://files.shanqianche.cn/202212/1670985502589.png" alt="方法二，返回指针获取ID"></p>]]></content>
    
    
    <summary type="html">description</summary>
    
    
    
    <category term="RTOS" scheme="https://merlynr.github.io/categories/RTOS/"/>
    
    
    <category term="study, graduate student,RTOS" scheme="https://merlynr.github.io/tags/study-graduate-student-RTOS/"/>
    
  </entry>
  
  <entry>
    <title>基于ESP32S3的FreeRTOS之信号量(六)</title>
    <link href="https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E4%BF%A1%E5%8F%B7%E9%87%8F(%E5%85%AD)/"/>
    <id>https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E4%BF%A1%E5%8F%B7%E9%87%8F(%E5%85%AD)/</id>
    <published>2022-12-16T03:35:00.000Z</published>
    <updated>2022-12-16T03:35:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Binary-Semaphore-二进制信号"><a href="#Binary-Semaphore-二进制信号" class="headerlink" title="Binary Semaphore 二进制信号"></a>Binary Semaphore 二进制信号</h2><p><img src="https://files.shanqianche.cn/202212/1670986242028.png" alt="创建信号量，向下兼容，不推荐使用"></p><p><img src="https://files.shanqianche.cn/202212/1670986404539.png" alt="推荐使用这个创建信号"></p><p><img src="https://files.shanqianche.cn/202212/1671088363753.png" alt="释放信号量"></p><ol><li>使用信号量执行任务</li></ol><p><img src="https://files.shanqianche.cn/202212/1671088852864.png" alt="enter description here"></p><blockquote><p>创建信号量之后为了使用它必须要释放它。</p></blockquote><p><img src="https://files.shanqianche.cn/202212/1671089225283.png" alt="文档中提到了创建之后要释放它"></p><ol><li>创建信号量并调用任务</li></ol><p><img src="https://files.shanqianche.cn/202212/1671089333679.png" alt="创建信号量并释放，task才可以调用"></p><ol start="2"><li>任务一进行调用使用信号量</li></ol><p><img src="https://files.shanqianche.cn/202212/1671089371462.png" alt="任务一"></p><ol start="3"><li>任务二调用使用信号量</li></ol><p><img src="https://files.shanqianche.cn/202212/1671089387925.png" alt="Task2"></p><ol start="4"><li>结果</li></ol><p><img src="https://files.shanqianche.cn/202212/1671089481464.png" alt="全局变量"></p><p><img src="https://files.shanqianche.cn/202212/1671089428031.png" alt="说明Task1和Task2锁住了全局变量"></p><ol start="5"><li>未使用信号量呢</li></ol><p><img src="https://files.shanqianche.cn/202212/1671089497958.png" alt="Task"></p><ol start="6"><li>结果</li></ol><p><img src="https://files.shanqianche.cn/202212/1671089571227.png" alt="没有使用信号量则会交替进行加一"></p><h2 id="计数信号量"><a href="#计数信号量" class="headerlink" title="计数信号量"></a>计数信号量</h2><p><img src="https://files.shanqianche.cn/202212/1671091200737.png" alt="获取信号量数目"></p><p><img src="https://files.shanqianche.cn/202212/1671091310069.png" alt="取得可用信号成功则锁住"></p><p><img src="https://files.shanqianche.cn/202212/1671091477101.png" alt="创建计数信号量"></p><ol><li>创建任务</li></ol><p><img src="https://files.shanqianche.cn/202212/1671091690613.png" alt="创建任务"></p><ol start="2"><li>使用计数信号量，最大信号量减减</li></ol><p> <img src="https://files.shanqianche.cn/202212/1671091848978.png" alt="使用信号量"></p><ol start="3"><li>释放信号量</li></ol><p><img src="https://files.shanqianche.cn/202212/1671091878388.png" alt="每到第六秒释放一个信号量"></p><ol start="4"><li>结果</li></ol><p><img src="https://files.shanqianche.cn/202212/1671091971061.png" alt="第五秒后没有信号量，第六秒进行释放后进行使用"></p><p><img src="https://files.shanqianche.cn/202212/1671092027319.png" alt="每隔六秒可以获得一个信号量"></p><h2 id="互斥量-mutex"><a href="#互斥量-mutex" class="headerlink" title="互斥量 mutex"></a>互斥量 mutex</h2><p><img src="https://files.shanqianche.cn/202212/1671099188414.png" alt="创建互斥量"></p><p><strong>这块需要反复理解</strong></p><ol><li>创建信号量，挂起任务调度器，创建执行任务，开启任务调度器</li></ol><p><img src="https://files.shanqianche.cn/202212/1671107894154.png" alt="创建互斥信号量、挂起任务调度器、创建执行任务、开启任务调度器"></p><p><img src="https://files.shanqianche.cn/202212/1671108236775.png" alt="Task1"></p><p><img src="https://files.shanqianche.cn/202212/1671108106925.png" alt="Task2"></p><p><img src="https://files.shanqianche.cn/202212/1671108024642.png" alt="Task3"></p><blockquote><p>主要是优先级的继承，task1继承了task3所以可以执行任务</p></blockquote><p><img src="https://files.shanqianche.cn/202212/1671110095196.png" alt="互斥信号量"></p><p><img src="https://files.shanqianche.cn/202212/1671110147625.png" alt="互斥信号量"></p><blockquote><p>task3优先级最高首先运行，发生阻塞后task2开始执行，task2发生阻塞，task1开始执行，获取信号量并锁定。<br><strong>由于task3的优先级高，所以task3开始执行，然而信号量依旧在task1那</strong>，并未被释放task1超出时间片，看门狗被触发，idle任务因为要用于内存的清理回收，idle任务无法执行，就会触发看门狗。<br>所以task3尝试取信号量，尝试失败后进入task2while循环，循环超时触发看门狗，然后task继续3尝试获取信号量。以此反复。</p></blockquote><p><img src="https://files.shanqianche.cn/202212/1671111112169.png" alt="使用二进制信号量"></p><p><img src="https://files.shanqianche.cn/202212/1671108979371.png" alt="二进制信号量的结果"></p><h2 id="Recursive-Mutex-递归互斥锁"><a href="#Recursive-Mutex-递归互斥锁" class="headerlink" title="Recursive Mutex 递归互斥锁"></a>Recursive Mutex 递归互斥锁</h2><p><img src="https://files.shanqianche.cn/202212/1671160318532.png" alt="task1获取A信号量并锁住，A需要B，所以继续获取B并锁住"></p><p><img src="https://files.shanqianche.cn/202212/1671160501852.png" alt="创建递归互斥锁"></p><p><img src="https://files.shanqianche.cn/202212/1671165183178.png" alt="创建并执行任务调度器"></p><p><img src="https://files.shanqianche.cn/202212/1671172338333.png" alt="Task2"></p><p><img src="https://files.shanqianche.cn/202212/1671172326291.png" alt="Task1"></p><p><strong>结果：</strong></p><p><img src="https://files.shanqianche.cn/202212/1671172370707.png" alt="结果"></p><p><img src="https://files.shanqianche.cn/202212/1671172394998.png" alt="结果"></p>]]></content>
    
    
    <summary type="html">description</summary>
    
    
    
    <category term="RTOS" scheme="https://merlynr.github.io/categories/RTOS/"/>
    
    
    <category term="study, graduate student,RTOS" scheme="https://merlynr.github.io/tags/study-graduate-student-RTOS/"/>
    
  </entry>
  
  <entry>
    <title>基于ESP32S3的FreeRTOS之队列(三)</title>
    <link href="https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E9%98%9F%E5%88%97(%E4%B8%89)/"/>
    <id>https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E9%98%9F%E5%88%97(%E4%B8%89)/</id>
    <published>2022-12-16T03:09:00.000Z</published>
    <updated>2022-12-16T03:09:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="queue队列"><a href="#queue队列" class="headerlink" title="queue队列"></a>queue队列</h2><blockquote><p>常用于传递<font color="#FF00FF">整形、结构体及指针</font></p></blockquote><p><img src="https://files.shanqianche.cn/202212/1670937269638.png" alt="queue"></p><p><img src="https://files.shanqianche.cn/202212/1670937418178.png" alt="xQueueCreate"></p><h3 id="传递整形"><a href="#传递整形" class="headerlink" title="传递整形"></a>传递整形</h3><ol><li>创建队列</li></ol><p><img src="https://files.shanqianche.cn/202212/1670938179291.png" alt="创建队列"></p><ol start="2"><li>接收函数</li></ol><p><img src="https://files.shanqianche.cn/202212/1670938268578.png" alt="接收函数"></p><ol start="3"><li>接收函数</li></ol><p><img src="https://files.shanqianche.cn/202212/1670938366706.png" alt="接收函数"></p><ol start="4"><li>结果</li></ol><p><img src="https://files.shanqianche.cn/202212/1670938348411.png" alt="运行结果"></p><ol start="5"><li>对接收函数进行优化</li></ol><p><img src="https://files.shanqianche.cn/202212/1670938507920.png" alt="监控接收队列是否为空，不为才进行接收并打印"></p><h3 id="传递结构体"><a href="#传递结构体" class="headerlink" title="传递结构体"></a>传递结构体</h3><ol><li>结构体</li></ol><p><img src="https://files.shanqianche.cn/202212/1670938695866.png" alt="结构体"></p><ol start="2"><li>发送函数</li></ol><p><img src="https://files.shanqianche.cn/202212/1670938716857.png" alt="发送函数"><br>3. 接收函数</p><p><img src="https://files.shanqianche.cn/202212/1670938766379.png" alt="接收函数"><br>4. 队列</p><p><img src="https://files.shanqianche.cn/202212/1670938806106.png" alt="修改队列宽度，即数据所占位数"></p><h3 id="传递指针"><a href="#传递指针" class="headerlink" title="传递指针"></a>传递指针</h3><p> <strong>小心内存的分配和清除，其它差不多</strong></p><h3 id="队列的多近单出"><a href="#队列的多近单出" class="headerlink" title="队列的多近单出"></a>队列的多近单出</h3><ol><li>发送任务一</li></ol><blockquote><p>发送数字111</p></blockquote><p><img src="https://files.shanqianche.cn/202212/1670940015975.png" alt="task1"></p><ol start="2"><li>发送任务2</li></ol><blockquote><p>发送数字222</p></blockquote><p><img src="https://files.shanqianche.cn/202212/1670940046663.png" alt="task2"><br>3. 接收任务</p><p><img src="https://files.shanqianche.cn/202212/1670940095025.png" alt="接收并打印，并设置接收延时最大，阻塞一直等待接收"><br>4. 队列管理</p><p><img src="https://files.shanqianche.cn/202212/1670940125054.png" alt="发送任务优先级一样，接收任务级别为2"></p><h3 id="Queue-Set"><a href="#Queue-Set" class="headerlink" title="Queue Set"></a>Queue Set</h3><p><img src="https://files.shanqianche.cn/202212/1670942546618.png" alt="队列集合，当集合中中的队列哪个有数据则获取哪个队列的数据"></p><ol><li>创建发送Task1</li></ol><p><img src="https://files.shanqianche.cn/202212/1670943244590.png" alt="Task1"><br>2. 发送Task2</p><p><img src="https://files.shanqianche.cn/202212/1670943270669.png" alt="Task2"><br>3. 接收Task</p><p><img src="https://files.shanqianche.cn/202212/1670944978777.png" alt="接收Task"><br>4. Queue 将队列加入Set</p><p><img src="https://files.shanqianche.cn/202212/1670945004119.png" alt="Queue"></p><h3 id="队列邮箱"><a href="#队列邮箱" class="headerlink" title="队列邮箱"></a>队列邮箱</h3><blockquote><p>里面存放一个数据<strong>即队列长度为1</strong>，有一个写入邮箱的task，有一个或多个读出邮箱的task</p></blockquote><p><img src="https://files.shanqianche.cn/202212/1670945120071.png" alt="Queue Mail"></p><ol><li>queue mail</li></ol><p><img src="https://files.shanqianche.cn/202212/1670946095803.png" alt="三个read一个write"><br>2. readTask</p><p><img src="https://files.shanqianche.cn/202212/1670946068507.png" alt="readTask"><br>3. writeTask</p><p><img src="https://files.shanqianche.cn/202212/1670946161788.png" alt="readTask"><br>4. 结果</p><p><img src="https://files.shanqianche.cn/202212/1670946264134.png" alt="写入延迟为6秒"></p>]]></content>
    
    
    <summary type="html">description</summary>
    
    
    
    <category term="RTOS" scheme="https://merlynr.github.io/categories/RTOS/"/>
    
    
    <category term="study, graduate student,RTOS" scheme="https://merlynr.github.io/tags/study-graduate-student-RTOS/"/>
    
  </entry>
  
  <entry>
    <title>基于ESP32S3的FreeRTOS之Task(二)</title>
    <link href="https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8BTask(%E4%BA%8C)/"/>
    <id>https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8BTask(%E4%BA%8C)/</id>
    <published>2022-12-16T03:08:00.000Z</published>
    <updated>2022-12-16T03:08:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Task"><a href="#Task" class="headerlink" title="Task"></a>Task</h2><h3 id="Task-基本设置"><a href="#Task-基本设置" class="headerlink" title="Task 基本设置"></a>Task 基本设置</h3><h4 id="xTaskCrate"><a href="#xTaskCrate" class="headerlink" title="xTaskCrate()"></a>xTaskCrate()</h4><p><img src="https://files.shanqianche.cn/202211/1667291790344.png" alt="xTaskCreate"><br>pvTaskCode: task函数。<br>pcName: task name。<br>usstackDepth: 表示分配的内存。<br>*pvParmeters: 为task函数的参数指针 ==(void *) #FF9800== ，可以由xTaskCreate函数传递到pvTaskCode。<br>uxPriority: task执行优先级，空闲状态为0，所以一般设置值大于0。<br>*pxCreatedTask: task的handle，可以获得task很多信息，类似与于进程的ID。</p><h5 id="TASK四种类型"><a href="#TASK四种类型" class="headerlink" title="TASK四种类型"></a>TASK四种类型</h5><ol><li>参数为整数</li></ol><p><img src="https://files.shanqianche.cn/202211/1667293389173.png" alt="参数为整数"></p><h5 id="参数为数组"><a href="#参数为数组" class="headerlink" title="参数为数组"></a>参数为数组</h5><blockquote><p>数组名为地址，所以不需要取址</p></blockquote><p><img src="https://files.shanqianche.cn/202211/1667293510828.png" alt="参数为数组"></p><h5 id="参数为结构体"><a href="#参数为结构体" class="headerlink" title="参数为结构体"></a>参数为结构体</h5><p><img src="https://files.shanqianche.cn/202211/1667293604613.png" alt="结构体"></p><h5 id="参数为字符串常量"><a href="#参数为字符串常量" class="headerlink" title="参数为字符串常量"></a>参数为字符串常量</h5><p><img src="https://files.shanqianche.cn/202211/1667293674918.png" alt="字符串常量"></p><h4 id="vTaskDelete-xHandle"><a href="#vTaskDelete-xHandle" class="headerlink" title="vTaskDelete(xHandle)"></a>vTaskDelete(xHandle)</h4><p>删除Task</p><h4 id="vTaskDelay"><a href="#vTaskDelay" class="headerlink" title="vTaskDelay()"></a>vTaskDelay()</h4><p>时延</p><h3 id="Task-Priorities"><a href="#Task-Priorities" class="headerlink" title="Task Priorities"></a>Task Priorities</h3><blockquote><p>系统优先级的范围为[0, max-1]，当task设置的优先级超过最大值时会默认为最大值。<br>==尽量不要修改配置中的优先级最大值，会占用很大内存 #FF572</p></blockquote><p><img src="https://files.shanqianche.cn/202211/1667297323036.png" alt="该宏被设置为1时，优先级不能超过32"></p><h4 id="系统配置"><a href="#系统配置" class="headerlink" title="系统配置"></a>系统配置</h4><p><img src="https://files.shanqianche.cn/202211/1667296693484.png" alt="优先级的定义"></p><p><img src="https://files.shanqianche.cn/202211/1667296735778.png" alt="配置文件路径"></p><p><img src="https://files.shanqianche.cn/202211/1667296782216.png" alt="优先级的定义"></p><h4 id="uxTaskPriorityGet"><a href="#uxTaskPriorityGet" class="headerlink" title="uxTaskPriorityGet()"></a>uxTaskPriorityGet()</h4><p><img src="https://files.shanqianche.cn/202211/1667296913491.png" alt="获取任务优先级"></p><p><img src="https://files.shanqianche.cn/202211/1667297027490.png" alt="get task priority"></p><h4 id="Same-Priority"><a href="#Same-Priority" class="headerlink" title="Same Priority"></a>Same Priority</h4><pre><code>相同优先级的task，则顺序运行，基于robin调度机制使用时间片共享一个进程时间段。</code></pre><p><img src="https://files.shanqianche.cn/202211/1667297529065.png" alt="Same Priority"></p><h4 id="different-Priority"><a href="#different-Priority" class="headerlink" title="different Priority"></a>different Priority</h4><p>优先级高的先执行，与顺序无关。</p><h4 id="Modify-Priority"><a href="#Modify-Priority" class="headerlink" title="Modify Priority"></a>Modify Priority</h4><blockquote><p>修改优先级 vTaskPrioritySet(TaskHandle_t px, priority);</p></blockquote><p><img src="https://files.shanqianche.cn/202211/1667298265387.png" alt="修改优先级"><br>修改，马上根据新的优先级执行。</p><h3 id="Task-States"><a href="#Task-States" class="headerlink" title="Task States"></a>Task States</h3><blockquote><p>运行状态、准备状态、阻塞状态、挂起状态</p></blockquote><p><img src="https://files.shanqianche.cn/202211/1667298712509.png" alt="task state"></p><p><strong>注意：</strong><br>对于阻塞状态有超时的概念，对于挂起状态则没有这种概念。</p><h4 id="vTaskSuspend-挂起-amp-vTaskResume-恢复"><a href="#vTaskSuspend-挂起-amp-vTaskResume-恢复" class="headerlink" title="vTaskSuspend 挂起 &amp; vTaskResume 恢复"></a>vTaskSuspend 挂起 &amp; vTaskResume 恢复</h4><p><img src="https://files.shanqianche.cn/202212/1670336283287.png" alt="task被挂起并恢复"></p><h4 id="vTaskSuspendAll-挂起所有-amp-vTaskResumeAll-恢复所有"><a href="#vTaskSuspendAll-挂起所有-amp-vTaskResumeAll-恢复所有" class="headerlink" title="vTaskSuspendAll() 挂起所有 &amp; vTaskResumeAll() 恢复所有"></a>vTaskSuspendAll() 挂起所有 &amp; vTaskResumeAll() 恢复所有</h4><p> 挂起所有task，同时执行完后不能调用FreeRTOS API函数。必须通过vTaskResumeAll（）进行恢复后才能继续直接FreeRTOS API。<strong>保证程序的独立运行</strong></p><p> <img src="https://files.shanqianche.cn/202212/1670336505558.png" alt="指的是系统被挂起后无法被调用"></p><h3 id="vTaskList-任务状态信息"><a href="#vTaskList-任务状态信息" class="headerlink" title="vTaskList() 任务状态信息"></a>vTaskList() 任务状态信息</h3><p> <img src="https://files.shanqianche.cn/202212/1670480943006.png" alt="vTaskList展示task信息"> </p><blockquote><p>运行状态<br>     X: 运行状态<br>    B:阻塞状态<br>    R:准备状态<br>    S:挂起状态<br>    D:删除状态</p></blockquote><h3 id="xTask堆栈"><a href="#xTask堆栈" class="headerlink" title="xTask堆栈"></a>xTask堆栈</h3><p><img src="https://files.shanqianche.cn/202212/1670506507182.png" alt="xTask堆栈的设置"><br><strong>usStackDepth的理解</strong>： 为堆栈的深度。即如果堆栈的宽度为4bytes，深度为100，则堆栈空间为100* 4bytes。</p><p><img src="https://files.shanqianche.cn/202212/1670507145359.png" alt="usStackDepth"></p><h4 id="uxTaskGetStackHighWaterMark-查询剩余堆栈内存"><a href="#uxTaskGetStackHighWaterMark-查询剩余堆栈内存" class="headerlink" title="uxTaskGetStackHighWaterMark() 查询剩余堆栈内存"></a>uxTaskGetStackHighWaterMark() 查询剩余堆栈内存</h4><p>调用vTaskList消耗内存较大，可以通过uxTaskGetStackHighWaterMark来获取内存信息。</p><p><strong>可以用于调试代码</strong></p><h3 id="task-Watchdogs"><a href="#task-Watchdogs" class="headerlink" title="task Watchdogs"></a>task Watchdogs</h3><blockquote><ol><li>中断看门狗300ms、2.task看门狗5s</li></ol></blockquote><h4 id="中断看门狗"><a href="#中断看门狗" class="headerlink" title="中断看门狗"></a>中断看门狗</h4><p><img src="https://files.shanqianche.cn/202212/1670832506559.png" alt="中断看门狗调用的是定时器组一"></p><p><img src="https://files.shanqianche.cn/202212/1670832612635.png" alt="中断看梦的配置"></p><h4 id="task看门狗"><a href="#task看门狗" class="headerlink" title="task看门狗"></a>task看门狗</h4><p><img src="https://files.shanqianche.cn/202212/1670915250123.png" alt="创建时引用"></p><p><img src="https://files.shanqianche.cn/202212/1670937111595.png" alt="将任务加入任务看门狗，并喂狗，如果不喂狗则会报idle错误"></p>]]></content>
    
    
    <summary type="html">description</summary>
    
    
    
    <category term="RTOS" scheme="https://merlynr.github.io/categories/RTOS/"/>
    
    
    <category term="study, graduate student,RTOS" scheme="https://merlynr.github.io/tags/study-graduate-student-RTOS/"/>
    
  </entry>
  
  <entry>
    <title>基于ESP32S3的FreeRTOS之基本运行原理(一)</title>
    <link href="https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E5%9F%BA%E6%9C%AC%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86(%E4%B8%80)/"/>
    <id>https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E5%9F%BA%E6%9C%AC%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86(%E4%B8%80)/</id>
    <published>2022-12-16T03:06:00.000Z</published>
    <updated>2022-12-16T03:06:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="学习任务"><a href="#学习任务" class="headerlink" title="学习任务"></a>学习任务</h1><p><a href="https://freertos.org/RTOS-task-states.html">参考官方文档</a><br><a href="https://www.bilibili.com/video/BV1Nb4y1q7xz/?spm_id_from=333.788&vd_source=6dbca05574a96cc5925db86217ad31cc">参考Michael_ee老师教学</a>  </p><ul><li><input checked="" disabled="" type="checkbox"> 了解Free RTOS的启动机制</li><li><input checked="" disabled="" type="checkbox"> 掌握task</li><li><input checked="" disabled="" type="checkbox"> 掌握list </li><li><input checked="" disabled="" type="checkbox"> 掌握queue</li></ul><h1 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h1><h2 id="基本应用程序启动流程"><a href="#基本应用程序启动流程" class="headerlink" title="基本应用程序启动流程"></a>基本应用程序启动流程</h2><h3 id="框架结构"><a href="#框架结构" class="headerlink" title="框架结构"></a>框架结构</h3><ol><li>First stage bootloader</li></ol><blockquote><p>位于只读的ROM，主动在flash的0x1000偏移地址处加载second stage bootloader 到RAM（IRAM &amp; DRAM）中。</p></blockquote><p>位于components的bootloader文件夹中，都为build文件。</p><ol start="2"><li>Second stage bootloader</li></ol><blockquote><p>从flash中加载分区表和主程序镜像。主程序中包含RAM段和通过flash高速缓存映射的只读段。</p></blockquote><p>位于components/bootloader_support文件夹中，包含了各种芯片的初始化启动代码。</p><ol start="3"><li>Application startup（应用程序入口）</li></ol><blockquote><p>第二个CPU和RTOS的调度器启动。</p></blockquote><p>位于components/freertos文件夹中，核心文件为==list.c #F44336==、==queue.c #FF5722==、==task.c #FF9800==。其中port文件夹中针对不同系统的移植代码。</p><p><img src="https://files.shanqianche.cn/202211/1667286874288.png" alt="针对不同内核的移植代码"></p><h3 id="应用程序的调用顺序"><a href="#应用程序的调用顺序" class="headerlink" title="应用程序的调用顺序"></a>应用程序的调用顺序</h3><blockquote><p>位于 ==freeRTOS #00BCD4==<br>-&gt; app_main()<br>-&gt; 【port_common.c】main_task()<br>-&gt;【port_common.c】xTaskCreatePinnedToCord() 创建了一个main_task<br>-&gt; 在esp_startup_start_app_common()中利用xTaskCreatePinnedToCord创建main_task()<br>-&gt; 【port.c】esp_startup_start_app_common() 此函数主要用于创建对所有task的调度操作，涉及到优先级<br>位于==esp-system #00BCD4==<br>-&gt; 【startup.c】start_cpu0_default() 该函数主要初始化核心组件及服务。<br>-&gt; 【startup.c】弱连接到start_cpu0(), 该函数中涉及到硬件初始化<br>-&gt; 【startup_internal.h】g_startup_fn[] 数组可以调用不同的cpu初始化<br>-&gt; 【stratup_internal.h】#define SYS_STARTUP_FN() 宏定义<br>-&gt; 【cpu_start.c】call_start_cpu0()，由于esp32只有一个内核，所以只会被cpu0调用，这个函数同时调用了esp_mspi_pin_init用于初始化esp，，以及调用了bootloader<br>-&gt; 【sections.id.in链接文件】ENTRY(call_start_cpu0) ; 应用程序入口</p></blockquote><p><img src="https://files.shanqianche.cn/202211/1667287763318.png" alt="task创建了一个main_task"></p><p><img src="https://files.shanqianche.cn/202211/1667288388845.png" alt="start_cpu0()弱连接"></p>]]></content>
    
    
    <summary type="html">description</summary>
    
    
    
    <category term="RTOS" scheme="https://merlynr.github.io/categories/RTOS/"/>
    
    
    <category term="study, graduate student,RTOS" scheme="https://merlynr.github.io/tags/study-graduate-student-RTOS/"/>
    
  </entry>
  
  <entry>
    <title>基于ESP32S3的FreeRTOS之数据流(八)</title>
    <link href="https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E4%BF%A1%E6%81%AF%E6%B5%81(%E5%85%AB)/"/>
    <id>https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E4%BF%A1%E6%81%AF%E6%B5%81(%E5%85%AB)/</id>
    <published>2022-12-16T02:45:00.000Z</published>
    <updated>2022-12-16T02:45:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Message-buffer"><a href="#Message-buffer" class="headerlink" title="Message buffer"></a>Message buffer</h2><blockquote><p>与stream buffer 的不同：<br>（1）：一次只能接收完整一条buffer；<br>（2）：如果数据长度超过接收buffer的大小，stream会继续接收(接收buffer大小的数据量)，而message不接收</p></blockquote><h3 id="基本API"><a href="#基本API" class="headerlink" title="基本API"></a>基本API</h3><p><img src="https://files.shanqianche.cn/202212/1671202136155.png" alt="创建"></p><p><img src="https://files.shanqianche.cn/202212/1671202226329.png" alt="接收function"></p><p><img src="https://files.shanqianche.cn/202212/1671202240298.png" alt="发送function"></p><h3 id="实验一：验证区别一"><a href="#实验一：验证区别一" class="headerlink" title="实验一：验证区别一"></a>实验一：验证区别一</h3><p><img src="https://files.shanqianche.cn/202212/1671202461368.png" alt="全局变量 message buffer句柄"></p><p><img src="https://files.shanqianche.cn/202212/1671202272589.png" alt="调度器"></p><p><img src="https://files.shanqianche.cn/202212/1671202413471.png" alt="创建三天message buffer并发送到buffer中"><br><img src="https://files.shanqianche.cn/202212/1671202330991.png"></p><p><img src="https://files.shanqianche.cn/202212/1671202495555.png" alt="结果：每次只接收一条数据"></p><p><img src="https://files.shanqianche.cn/202212/1671202554503.png" alt="修改为stream buffer继续实验"></p><p><img src="https://files.shanqianche.cn/202212/1671202604076.png" alt="结果：一次接收所有数据"></p><h3 id="实验二：验证区别二"><a href="#实验二：验证区别二" class="headerlink" title="实验二：验证区别二"></a>实验二：验证区别二</h3><h4 id="stream-buffer"><a href="#stream-buffer" class="headerlink" title="stream buffer"></a>stream buffer</h4><p><img src="https://files.shanqianche.cn/202212/1671202816481.png" alt="发送function"></p><p><img src="https://files.shanqianche.cn/202212/1671202862644.png" alt="接收function，buffer小"></p><p><img src="https://files.shanqianche.cn/202212/1671202933240.png" alt="结果：接收"></p><h4 id="message-buffer"><a href="#message-buffer" class="headerlink" title="message buffer"></a>message buffer</h4><p><img src="https://files.shanqianche.cn/202212/1671202988047.png" alt="结果：不在接收"></p>]]></content>
    
    
    <summary type="html">description</summary>
    
    
    
    <category term="RTOS" scheme="https://merlynr.github.io/categories/RTOS/"/>
    
    
    <category term="study, graduate student,RTOS" scheme="https://merlynr.github.io/tags/study-graduate-student-RTOS/"/>
    
  </entry>
  
  <entry>
    <title>基于ESP32S3的FreeRTOS之数据流(七)</title>
    <link href="https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81(%E4%B8%83)/"/>
    <id>https://merlynr.github.io/2022/12/16/%E5%9F%BA%E4%BA%8EESP32S3%E7%9A%84FreeRTOS%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81(%E4%B8%83)/</id>
    <published>2022-12-16T01:44:00.000Z</published>
    <updated>2022-12-16T01:44:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Stream-Buffer"><a href="#Stream-Buffer" class="headerlink" title="Stream Buffer"></a>Stream Buffer</h2><h3 id="基础API"><a href="#基础API" class="headerlink" title="基础API"></a>基础API</h3><blockquote><p>大部分音频以这种形式传输数据。</p></blockquote><p><img src="https://files.shanqianche.cn/202212/1671199467534.png" alt="创建流"></p><p><img src="https://files.shanqianche.cn/202212/1671199642131.png" alt="发送stream buffer"></p><p><img src="https://files.shanqianche.cn/202212/1671199751798.png" alt="接收stream buffer"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><p><img src="https://files.shanqianche.cn/202212/1671199532838.png" alt="全局变量stream buffer句柄"></p><p><img src="https://files.shanqianche.cn/202212/1671199578431.png" alt="创建stream buffer"></p><p><img src="https://files.shanqianche.cn/202212/1671199702196.png" alt="发送handle"></p><p><img src="https://files.shanqianche.cn/202212/1671199794137.png" alt="接收handle"></p><h3 id="实验结果与改进"><a href="#实验结果与改进" class="headerlink" title="实验结果与改进"></a>实验结果与改进</h3><p><img src="https://files.shanqianche.cn/202212/1671199830149.png" alt="结果"></p><p><img src="https://files.shanqianche.cn/202212/1671200002220.png" alt="修改触发量"></p><p><img src="https://files.shanqianche.cn/202212/1671200519994.png" alt="接收"></p><p><img src="https://files.shanqianche.cn/202212/1671200339750.png" alt="基于上修改，将阻塞提前"></p><p><img src="https://files.shanqianche.cn/202212/1671200373451.png" alt="结果，先发送五次后再接收"></p><h2 id="监控stream-buffer-剩余空间"><a href="#监控stream-buffer-剩余空间" class="headerlink" title="监控stream buffer 剩余空间"></a>监控stream buffer 剩余空间</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><img src="https://files.shanqianche.cn/202212/1671200744509.png" alt="task3监控stream buffer剩余空间"></p><h3 id="基本API"><a href="#基本API" class="headerlink" title="基本API"></a>基本API</h3><p><img src="https://files.shanqianche.cn/202212/1671201588256.png" alt="查看剩余空间"></p><h3 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h3><p><img src="https://files.shanqianche.cn/202212/1671201719717.png" alt="监控剩余空间"></p><p><img src="https://files.shanqianche.cn/202212/1671201759665.png" alt="结果"></p>]]></content>
    
    
    <summary type="html">description</summary>
    
    
    
    <category term="RTOS" scheme="https://merlynr.github.io/categories/RTOS/"/>
    
    
    <category term="study, graduate student, RTOS" scheme="https://merlynr.github.io/tags/study-graduate-student-RTOS/"/>
    
  </entry>
  
  <entry>
    <title>STM32F108的学习</title>
    <link href="https://merlynr.github.io/2022/11/11/STM32F108%E7%9A%84%E5%AD%A6%E4%B9%A0/"/>
    <id>https://merlynr.github.io/2022/11/11/STM32F108%E7%9A%84%E5%AD%A6%E4%B9%A0/</id>
    <published>2022-11-11T02:12:00.000Z</published>
    <updated>2022-11-11T02:12:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="f270768e-6a79-4d1d-8fd8-694008c8a311">天天PLAN</a></p><h2 id="第一节"><a href="#第一节" class="headerlink" title="第一节"></a>第一节</h2><h3 id="LED的注意事项"><a href="#LED的注意事项" class="headerlink" title="LED的注意事项"></a>LED的注意事项</h3><ol><li>需要连接电阻，不然可能击穿LED。</li><li><strong>一般LED所需电流为0~10+的毫安【不要超过20mA】，芯片所提供电压为3.3 or 5v，所以接一个1kΩ电阻【限流电阻】即可。</strong></li><li><a href="https://blog.csdn.net/a419116194/article/details/103238872">高电平、低电平</a>。低电平时电路的阻抗低，噪声造成的电平变化小，也就是说，抗干扰能力更强</li></ol><h2 id="第二节"><a href="#第二节" class="headerlink" title="第二节"></a>第二节</h2><h3 id="GPIO的注意事项"><a href="#GPIO的注意事项" class="headerlink" title="GPIO的注意事项"></a>GPIO的注意事项</h3><ol><li>除了GPIO的为通用<del>， 其余皆为复用</del></li></ol><p><img src="./images/1668134920148.png" alt="端口配置"><br>2. <strong>GPIO大部分使用推挽输出，ⅡC使用的开漏输出。</strong><br>3. 地址=基址+偏移地址<br>4. 端口配置寄存器【输入输出配置（模拟、浮空、上拉下拉、保留）（通用推挽、通用开漏、复用推挽、复用开漏），输入输出模式以及频率】–&gt;端口输出数据寄存器（配置端口地址为1的是输出端口）</p><h2 id="第四节"><a href="#第四节" class="headerlink" title="第四节"></a>第四节</h2><ol><li>AHB 高速总线、APB普通外设总结、</li><li><strong>通过APB为GPIO提供RCC时钟源，才可以使外设与CPU连接。</strong></li></ol><p><img src="./images/1668182227309.png" alt="使能GPIO"></p><ol start="3"><li>volatile 避免从缓冲区取值，只能从寄存器中取值。</li></ol><h2 id="第七节"><a href="#第七节" class="headerlink" title="第七节"></a>第七节</h2><ol><li>使用结构体可以定义连续的的相关配置，因为结构体中遍历在内存中是连续的。</li><li>使用STM32官方库需要引入相关文件。</li></ol><p><img src="./images/1668181982839.png" alt="依赖文件"></p><p><img src="./images/1668182057720.png" alt="依赖文件"></p><p><img src="./images/1668182110452.png" alt="库文件"></p><h2 id="第八节-按键控制"><a href="#第八节-按键控制" class="headerlink" title="第八节 按键控制"></a>第八节 按键控制</h2><p><img src="./images/1668242764890.png" alt="低电平输入"></p><ol><li>利用按键进行控制led时，可以利用内部的上拉电位，以此输入高电平，按键后输入低电平，以此控制LED</li></ol><h2 id="第九节呼吸灯"><a href="#第九节呼吸灯" class="headerlink" title="第九节呼吸灯"></a>第九节呼吸灯</h2><ol><li>有两种方式，一种是通过修改平均电平，第二种是通过修改PWM</li></ol><p><img src="./images/1668321362358.png" alt="修改电平，变亮就是增加高电平所占时间"></p><h2 id="第十一节NVIC"><a href="#第十一节NVIC" class="headerlink" title="第十一节NVIC"></a>第十一节NVIC</h2><p><img src="https://files.shanqianche.cn/202211/1668324442560.png" alt="抢占优先级和响应优先级"></p><ol><li>从优先级（响应优先级）,在抢占优先级相同时谁的高谁优先。</li></ol><p><img src="https://files.shanqianche.cn/202211/1668325634113.png" alt="外部中断模式"><br>2. 事件请求：当前事件完成后跳转到中断向量表。中断请求：立刻跳转到中断向量表。</p><p><img src="https://files.shanqianche.cn/202211/1668326173434.png" alt="功能复用"><br>3. 需要考虑到pin复用。时钟和中断。</p><p><img src="https://files.shanqianche.cn/202211/1668326286702.png" alt="外部中断初始化"><br>4. 外部中断EXTI0的配置包括：向量化中断控制器的配置、PA0配置为外部中断、中断工作模式（下降沿检测）</p><h2 id="第十三节SysTick-最后一个异常"><a href="#第十三节SysTick-最后一个异常" class="headerlink" title="第十三节SysTick 最后一个异常"></a>第十三节SysTick 最后一个异常</h2><ol><li>SysTick是系统内部计时器，CorTex-M3内包含。其是一个24位的计时器，基本模式是从一个reload 值递减，为0时触发异常，然后重新计数。</li></ol><p><img src="https://files.shanqianche.cn/202211/1668328540665.png" alt="SysTick CTRL"></p><ol start="2"><li>多久减一可以通过CLKSOURCE来设置，AHB为告诉外部时钟，为72MHZ。频率=1/时钟，所以时钟越大，精度越高。</li></ol><p><img src="https://files.shanqianche.cn/202211/1668329188316.png" alt="时钟为72MHZ，reload value为72，最后1微妙产生一次异常中断"><br>3. 产生一次异常时间的设置。</p><h2 id="第十四节"><a href="#第十四节" class="headerlink" title="第十四节"></a>第十四节</h2><p><img src="https://files.shanqianche.cn/202211/1668346317638.png" alt="时钟配置"></p><p><img src="https://files.shanqianche.cn/202211/1668346298839.png" alt="开启时钟，设置全局变量，使其在异常函数中进行递减"><br><img src="https://files.shanqianche.cn/202211/1668346363077.png" alt="SysTick异常函数"></p><ol><li>延时函数</li></ol><h2 id="第十五节"><a href="#第十五节" class="headerlink" title="第十五节"></a>第十五节</h2><ol><li>stm32 有两个看门狗，一个独立看门狗，一个窗口看门狗</li></ol><p><img src="https://files.shanqianche.cn/202211/1668348073923.png" alt="IWDG"><br><img src="https://files.shanqianche.cn/202211/1668348218257.png" alt="IWDG主要功能"><br>2. 独立看门狗（IWDG）依赖LSI（低速内部时钟），即使晶振，外部时钟不工作，IWDG依旧正常工作</p><p><img src="https://files.shanqianche.cn/202211/1668348022275.png" alt="依赖APB1的外设"><br><img src="https://files.shanqianche.cn/202211/1668348100750.png" alt="WWDG"></p><p><img src="https://files.shanqianche.cn/202211/1668348325069.png" alt="IWDG框图"></p><p><img src="https://files.shanqianche.cn/202211/1668348444032.png" alt="键寄存器"><br><img src="https://files.shanqianche.cn/202211/1668390061020.png" alt="reload value为625，pr预分频因子为4时，IWDG就会每隔一秒钟复位一次"><br>3. WWDG依赖APB1，所以APB1出问题，无法工作。</p><p><img src="https://files.shanqianche.cn/202211/1668390482984.png" alt="设置IWDG，第一步取消写保护，设置PR"></p><p><img src="https://files.shanqianche.cn/202211/1668390706177.png" alt="第一步"><br><img src="https://files.shanqianche.cn/202211/1668390795034.png" alt="第二步PR的设置"></p><p><img src="https://files.shanqianche.cn/202211/1668390820968.png" alt="第三步reload value"><br><img src="https://files.shanqianche.cn/202211/1668390977778.png" alt="第四步装入reload value"></p><p><img src="https://files.shanqianche.cn/202211/1668391016821.png" alt="第五步使能"></p><p><img src="https://files.shanqianche.cn/202211/1668391395544.png" alt="IWDG_initt code"><br>5. 4. IWDG的设置</p><h2 id="第16节课WWDG"><a href="#第16节课WWDG" class="headerlink" title="第16节课WWDG"></a>第16节课WWDG</h2><ol><li><strong>偶尔的两次异常，即正常状态到异常状态、接着异常状态又到正常状态，这属于异常，如果这两步操作在异常恢复时间内，即喂狗时间内，IWDG是无法检测到的。而WWDG可以解决。</strong></li></ol><p><img src="https://files.shanqianche.cn/202211/1668392219848.png" alt="特殊异常"><br>2. WWDG喂狗机制，当WWDG的递减计数器1T[6:0]大于上限窗口W[6:0]时1，如果尝试去喂狗，将立刻产生CPU复位。</p><p><img src="https://files.shanqianche.cn/202211/1668392718745.png" alt="只有在0x60-0x3F内喂狗才有效"><br><img src="https://files.shanqianche.cn/202211/1668393252329.png" alt="WWDG原理"></p><p><img src="https://files.shanqianche.cn/202211/1668393442189.png" alt="enter description here"></p><p><img src="https://files.shanqianche.cn/202211/1668393484988.png" alt="喂狗"></p><ol start="3"><li>WWDG配置code<pre><code>- 因为WWDG是与APB1连接的，所以需要从APB1相关函数使能WWDG- 初始化WWDG，配置计数，计数是有上限的，通过与最大值进行想与来限制。- 设置预分频值（PR）- 设置上窗口值- 使能并且喂狗</code></pre></li></ol><p><img src="https://files.shanqianche.cn/202211/1668495595263.png" alt="WWDG初始化配置"></p><p><img src="https://files.shanqianche.cn/202211/1668495613227.png" alt="中断向量表初始化"></p><p>必须进行喂狗，不然CPU会不断进行重启。<br><img src="https://files.shanqianche.cn/202211/1668495643575.png" alt="喂狗"></p><h2 id="第十七节USART"><a href="#第十七节USART" class="headerlink" title="第十七节USART"></a>第十七节USART</h2><ol><li>主要用于芯片与芯片之间的传输</li><li>芯片与PC之间的通信</li><li>芯片与外围模块之间的通信 </li></ol><p>UART 异步功能<br>USART 同步功能<br>USART_CK具有时钟功能<br>USART_CTS具有流控功能，因为不同设备之间频率不一样，接发速率也不一样，CTS可以控制是否接受或者发送数据【硬件流控】，可以用于判断是否进行发送数据<br>USART_RTS发送请求</p><h3 id="代码-amp-逻辑"><a href="#代码-amp-逻辑" class="headerlink" title="代码&amp;逻辑"></a>代码&amp;逻辑</h3><ol><li>开启对应总线的时钟，同时需要开启GPIO管脚的时钟</li></ol>]]></content>
    
    
    <summary type="html">description</summary>
    
    
    
    <category term="graduate student time" scheme="https://merlynr.github.io/categories/graduate-student-time/"/>
    
    
    <category term="study, graduate student,STM32" scheme="https://merlynr.github.io/tags/study-graduate-student-STM32/"/>
    
  </entry>
  
  <entry>
    <title>JavaScript高级程序设计第四版</title>
    <link href="https://merlynr.github.io/2022/05/16/JavaScript%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AC%AC%E5%9B%9B%E7%89%88/"/>
    <id>https://merlynr.github.io/2022/05/16/JavaScript%E9%AB%98%E7%BA%A7%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AC%AC%E5%9B%9B%E7%89%88/</id>
    <published>2022-05-15T16:00:00.000Z</published>
    <updated>2022-07-17T09:06:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="戴师帮忙选的important-point"><a href="#戴师帮忙选的important-point" class="headerlink" title="戴师帮忙选的important point"></a>戴师帮忙选的important point</h1><p><img src="http://files.shanqianche.cn/20225/1652971169338.png" alt="important point page-1"></p><p><img src="http://files.shanqianche.cn/20225/1652971169333.png" alt="important point page-2"></p><p><img src="http://files.shanqianche.cn/20225/1652971169335.png" alt="important point page-3"></p><p><img src="http://files.shanqianche.cn/20225/1652971169334.png" alt="important point page-4"></p><p><img src="http://files.shanqianche.cn/20225/1652971169337.png" alt="important point page-5"></p><p><img src="http://files.shanqianche.cn/20225/1652971169336.png" alt="important point page-6"></p><p><img src="http://files.shanqianche.cn/20225/1652971169339.png" alt="important point page-7"></p><p><img src="http://files.shanqianche.cn/20225/1652971169340.png" alt="important point page-8"></p><p><img src="http://files.shanqianche.cn/20225/1652971169411.png" alt="important point page-9"></p><p><img src="http://files.shanqianche.cn/20225/1652971169408.png" alt="important point page-X"></p><h1 id="第二章-HTML-中的-JavaScript"><a href="#第二章-HTML-中的-JavaScript" class="headerlink" title="第二章 HTML 中的 JavaScript"></a>第二章 HTML 中的 JavaScript</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol><li>第2.1中提到type中设置为module，代码当成了<font color="#8B0000">ES6模块</font>，需要实践来验证。</li></ol><p><img src="http://files.shanqianche.cn/20225/1652971169412.png" alt="script标签中type属性"></p><ol start="2"><li>第2.1中提到浏览器不会对文件扩展名进行检测，为服务器<font color="#8B0000">动态生成js</font>提供可能性，不是很理解。</li></ol><p><img src="http://files.shanqianche.cn/20225/1652971169413.png" alt="不对文件后缀检测的作用"></p><ol start="3"><li>第2.1不是很懂其中提到的<font color="#8B0000">MIME</font>的作用及判断原理</li></ol><p><img src="http://files.shanqianche.cn/20225/1652971169416.png" alt="返回正确MIME类型"></p><ol start="4"><li>第2.1中提到src中url的请求策略，不理解<font color="#bf242a">同源策略</font></li></ol><p><img src="http://files.shanqianche.cn/20225/1652971169414.png" alt="src的请求策略，以及返回的JavaScript的受限规则"></p><ol start="5"><li>第2.1.2中推迟脚本执行中提到DOMContentLoad事件，没见过，下次学习一下HTML</li></ol><p><img src="http://files.shanqianche.cn/20225/1652971169421.png" alt="delay属性设置后依旧会在DOMContentLoad事件之前执行"></p><ol start="6"><li>第2.1.3async可以保证脚本在<font color="#E9967A">页面load事件之前执行</font></li></ol><p><img src="http://files.shanqianche.cn/20225/1652971169415.png" alt="async在页面中的执行顺序"></p><h2 id="理解与解决"><a href="#理解与解决" class="headerlink" title="理解与解决"></a>理解与解决</h2><h1 id="第三章语言基础"><a href="#第三章语言基础" class="headerlink" title="第三章语言基础"></a>第三章语言基础</h1><h2 id="学习与反思"><a href="#学习与反思" class="headerlink" title="学习与反思"></a>学习与反思</h2><ol><li>声明的提升</li></ol><p><img src="http://files.shanqianche.cn/20225/1652971169420.png" alt="var声明提升"></p><ol start="2"><li><p>补充：<font color="#8B0000"><a href="https://www.javascripttutorial.net/javascript-event-loop/">Event Loop</a></font></p></li><li><p>var，let与const在for中的使用</p></li></ol><p><img src="http://files.shanqianche.cn/20225/1652971169419.png" alt="for循环"></p><p>setTimeOut会在for循环结束后进行输出，var在这里读取的是一个全局变了 $i$ ,所有只会输出五，但是JavaScript引擎会为for循环中的let声明分别创建独立的变量实例，所以有五个 $i$ 进行输出。</p><p><img src="http://files.shanqianche.cn/20225/1652971169494.png" alt="for中的let"></p><p><img src="http://files.shanqianche.cn/20225/1652971169495.png" alt="for中的const"></p><h2 id="问题与解释"><a href="#问题与解释" class="headerlink" title="问题与解释"></a>问题与解释</h2><ol><li>第3.4.5中提到八进制和十六进制在数学操作中都被视为十进制，什么意思？</li></ol><p><img src="http://files.shanqianche.cn/20225/1652971169496.png" alt="八进制和十六进制的运算方式"><br>答：都转化为十进制再进行计算。</p><ol start="2"><li>第3.4.7中为什么要避免Symbol()包装对象？</li></ol><p><img src="http://files.shanqianche.cn/20225/1652971169497.png" alt="Symbol()无法包装对象"><br>解：对象是引用类型，返回的是指针，针对同一类型的对象是同一个指针，这就与Symbol（）矛盾了。<br>3. 第3.4.7.4的使用Symbol实现异步迭代以及后续的内容不是很懂。</p><p><img src="http://files.shanqianche.cn/20225/1652971169498.png" alt="相关异步的问题"><br>解：重构了迭代函数<br>4. 第3.4.7.16中with环境啥意思</p><p>解：</p><p><img src="http://files.shanqianche.cn/20225/1652971169499.png" alt="with的使用"></p><ol start="5"><li>第3.6.7中标签语句进行学习</li></ol><p><img src="http://files.shanqianche.cn/20225/1652971169500.png" alt="标签语句"><br>解：</p><h1 id="第四章变量、作用域与内存"><a href="#第四章变量、作用域与内存" class="headerlink" title="第四章变量、作用域与内存"></a>第四章变量、作用域与内存</h1><h2 id="问题与解"><a href="#问题与解" class="headerlink" title="问题与解"></a>问题与解</h2><ol><li>第4.2.2中let与var的提升的区别，let提升出现暂时性死区啥意思？</li></ol><p><img src="http://files.shanqianche.cn/20226/1655395936285.png" alt="let与var的hoisting"><br>2. 第4.3.4.3中提到内存泄漏的原因，那么如何检测内存泄漏呢？</p><h1 id="第五章基本引用类型"><a href="#第五章基本引用类型" class="headerlink" title="第五章基本引用类型"></a>第五章基本引用类型</h1><ol><li>引用值与传统面向对象编程语言中的类相似，但实现不同？<b>理解</b></li></ol><h1 id="第六章集合引用类型"><a href="#第六章集合引用类型" class="headerlink" title="第六章集合引用类型"></a>第六章集合引用类型</h1><ol><li>第6.2.4中提到会出现无法判断一个对象是否为数组的现象？</li></ol><p><img src="http://files.shanqianche.cn/20226/1655908202281.png" alt="无法判断是否为数组"></p><ol start="2"><li>第6.2.13中迭代器的作用域对象的使用。</li></ol><p><a href="https://blog.csdn.net/qq_35087256/article/details/79658253">相关文档</a></p><p><img src="http://files.shanqianche.cn/20226/1656118450989.png" alt="作用域对象"></p><ol start="3"><li>第6.3.4中提到的上溢和下溢不是很懂。</li></ol><p><img src="http://files.shanqianche.cn/20226/1656257982512.png" alt="解决上溢下溢问题"><br>4. 第6.9中，“每种包装类型都映射到同名的原始类型”什么意思？<br>5. 第6.9中，“在以读模式访问原始值时，后台会实例化一个原始值包装对象，通过这个对象可以操作数据。”？<br>6. 第6.9中，“涉及原始值的语句只要一执行完毕，包装对象就会立即销毁。”？</p><h1 id="第七章迭代器与生成器"><a href="#第七章迭代器与生成器" class="headerlink" title="第七章迭代器与生成器"></a>第七章迭代器与生成器</h1><h2 id="问题与解决"><a href="#问题与解决" class="headerlink" title="问题与解决"></a>问题与解决</h2><ol><li>第7.3.2.4中描述到yied*实现递归，没看懂代码</li></ol><p><img src="http://files.shanqianche.cn/20226/1656581774389.png" alt="enter description here"></p><h1 id="第八章对象、类与面向对象变成"><a href="#第八章对象、类与面向对象变成" class="headerlink" title="第八章对象、类与面向对象变成"></a>第八章对象、类与面向对象变成</h1><ol><li>第8.1.7中对象结构中提到解构在内部使用函数这块不是很懂</li></ol><p><img src="http://files.shanqianche.cn/20227/1656743508866.png" alt="解构在内部使用函数"></p><p><strong><font color="#FF1493">结构这的语法糖还是比较怪的，建议都看</font></strong></p><ol start="2"><li>第8.3.2.2中提到盗用构造函数的缺点不是很理解 <img src="https://files.shanqianche.cn/20227/1657633886960.png" alt="缺点"> <img src="https://files.shanqianche.cn/20227/1657633898659.png" alt="缺点"></li><li>第8.3.4中原型式的继承不是很懂作用</li></ol><p><img src="https://files.shanqianche.cn/20227/1657635961798.png" alt="原型式继承"><br>3. 第8.3.6中寄生组合继承，重新梳理一下继承中的逻辑</p><p><img src="https://files.shanqianche.cn/20227/1657637338484.png" alt="理论"></p><p><img src="https://files.shanqianche.cn/20227/1657637456555.png" alt="为什么说只调用一次"><br>4. 第8.4.4.5中类混入提到利用辅助函数进行展开嵌套，不是很懂原理和作用。</p><p><img src="https://files.shanqianche.cn/20227/1657711373381.png" alt="展开嵌套"></p><h1 id="第九章代理与反射"><a href="#第九章代理与反射" class="headerlink" title="第九章代理与反射"></a>第九章代理与反射</h1><ol><li>第9.1.6.2中提到使用反射不在抛错而是返回false，与实际代码不一样。</li></ol><p><img src="https://files.shanqianche.cn/20227/1657938559413.png" alt="文档"></p><p><img src="https://files.shanqianche.cn/20227/1657938590087.png" alt="实际演示"></p><h1 id="第十章函数"><a href="#第十章函数" class="headerlink" title="第十章函数"></a>第十章函数</h1><ol><li>第10.14闭包的作用和定义依旧不是很懂</li><li>第10.14.1中this对象的作用域不清楚</li></ol><p><img src="http://files.shanqianche.cn/20227/1658048008355.png" alt="this对象的作用域"></p><ol start="3"><li>第10.14.2中</li></ol><p><img src="http://files.shanqianche.cn/20227/1658049551215.png" alt="闭包问题"></p><ol start="4"><li>第10.15，10.16都不是很懂了</li></ol><h1 id="期约和异步函数"><a href="#期约和异步函数" class="headerlink" title="期约和异步函数"></a>期约和异步函数</h1><ol><li>第11.2.4.4中对于reduce的使用不是很懂</li></ol><p><img src="http://files.shanqianche.cn/20227/1658200969767.png" alt="promise+reduce"></p><p><img src="http://files.shanqianche.cn/20227/1658201198623.png" alt="promise reduce"></p><ol start="2"><li>第11.2.5.1中期约的取消，实现这块函数不是很懂</li></ol><p><img src="http://files.shanqianche.cn/20227/code.png" alt="期约取消功能的实现"></p><h1 id="第十四"><a href="#第十四" class="headerlink" title="第十四"></a>第十四</h1><ol><li>第14.3.3 微任务队列及为啥处理两次</li></ol><p><img src="https://files.shanqianche.cn/20227/1658421089137.png" alt="记录队列"></p><h1 id="第二十章"><a href="#第二十章" class="headerlink" title="第二十章"></a>第二十章</h1><ol><li>第20.9.2，3，4看不懂可读流，可写流和转换流</li></ol><h1 id="第二十四章"><a href="#第二十四章" class="headerlink" title="第二十四章"></a>第二十四章</h1><ol><li>第24.6中fetch API的数据绑定不是很懂，尤其stream那块</li></ol>]]></content>
    
    
    <summary type="html">每天学习JS</summary>
    
    
    
    <category term="FE" scheme="https://merlynr.github.io/categories/FE/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
    <category term="daily" scheme="https://merlynr.github.io/tags/daily/"/>
    
    <category term="JavaScript" scheme="https://merlynr.github.io/tags/JavaScript/"/>
    
    <category term="book" scheme="https://merlynr.github.io/tags/book/"/>
    
    <category term="front-end" scheme="https://merlynr.github.io/tags/front-end/"/>
    
  </entry>
  
  <entry>
    <title>业务流程的LSTM精准模型</title>
    <link href="https://merlynr.github.io/2021/07/29/%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E7%9A%84LSTM%E7%B2%BE%E5%87%86%E6%A8%A1%E5%9E%8B/"/>
    <id>https://merlynr.github.io/2021/07/29/%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E7%9A%84LSTM%E7%B2%BE%E5%87%86%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-07-28T16:00:00.000Z</published>
    <updated>2021-08-11T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>使用LSTM模型对事件下一步，时间戳和调用的资源进行预测。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>本文主要是对于前人提出在LSTM中利用近似前缀预测—<a href="https://blog.zuishuailcq.xyz/2021/07/19/%E5%9F%BA%E4%BA%8ELSTM%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%9A%E5%8A%A1%E8%BF%87%E7%A8%8B%E9%A2%84%E6%B5%8B%E7%9B%91%E6%8E%A7/">基于LSTM神经网络的业务过程预测监控 | 吾辈之人，自当自强不息！</a>的缺陷的改进，缺陷：</p><ol><li>无法处理数字变量</li><li>不能生成带有时间戳的时间序列</li><li>后续有文章提出通过one-hot编码来对事件进行分类，而不是使用嵌入维度来实现的，这样<strong>随着事件类型的增加，精度就会极度下降</strong>。</li></ol><blockquote><p>知识补充<br>后处理：在模型训练后，人为的修改模型结果使之预测结果更加符合真实情况。<br><a href="https://blog.csdn.net/xieyan0811/article/details/80549001">数据挖掘之_后处理_谢彦的技术博客-CSDN博客_数据后处理</a></p></blockquote><dl><dt><strong><font color="#006400">解决：</font></strong><br>本文通过提出用于建立新的预处理和后处理方法和架构以及使用LSTM神经网络的事件日志的生成模型来解决上述方法的局限性。</dt><dd>具体地说，本文提出了一种方法去学习模型，该方法可以生成由三组(事件类型、角色、时间戳)组成的轨迹(或从给定前缀开始的轨迹的后缀)。提出的方法结合了Tax等人[13]和Evermann等人[2]的优点，<font color="#483D8B">通过使用嵌入维度，同时支持事件日志中的分类属性和数字属性</font>。本文考虑了神经网络中共享层和特有层的不同组合所对应的三种体系结构。</dd></dl><p><strong><font color="#FF8C00">评估：</font></strong></p><ol><li>第一种方法比较了与不同体系结构、预处理和后处理选择相对应的所提出方法的备选实例。该评估的目的是<font color="#00008B">根据获取到的日志的特征</font>，得出关于哪些设计选择更可取的指南。</li><li>比较提出方法在上面三条约束的表现。</li></ol><h1 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h1><h2 id="LSTM-当前的进展"><a href="#LSTM-当前的进展" class="headerlink" title="LSTM 当前的进展"></a>LSTM 当前的进展</h2><ol><li>使用LSTM对于事件类型较少的序列进行迭代循环预测，使用的是one-hot编码将事件类型，事件戳映射到向量特征向量，并使用事件发生的时间特征对其进行补充。<strong>缺点</strong>是当事件类型较多的时候，该方法效果会变差。</li></ol><ul><li>该模型由共享LSTM层构成，其中包括一个专门用于预测事件的LSTM和一个预测事件的LSTM</li></ul><p><img src="http://files.shanqianche.cn/2021730/1627633758272.png" alt="2017-使用LSTM神经网络进行预测性业务流程监控"></p><ol start="2"><li>使用嵌入维度的LSTMs，可以减少输入长度和增加新的特征。<strong>缺点</strong>是依旧无法处理数值变量，所以也不能预测时间戳。<strong>优点</strong>是可以处理大量事件类型。</li></ol><ul><li>使用两层LSTMs隐藏层。</li></ul><p><img src="http://files.shanqianche.cn/2021730/1627636620079.png" alt="2017-利用深度学习预测过程行为"></p><ol start="3"><li>提出基于RNN的模型MM-Pred来预测下一步事件和流程后续。<strong>缺点</strong>是无法处理数值变量，所以也不能预测时间戳。</li></ol><ul><li>这种方法同时使用控制流信息（事件类型）和案例数据（事件属性）。</li><li>该结构由编码器、调制器和解码器组成。</li><li>编码器和解码器使用LSTM网络将每个事件的属性转换为隐藏表示或从隐藏表示转换为隐藏表示。</li><li>调制器组件求出可变长度序列比对权重向量，其中每个权重表示用于预测未来事件和属性的属性的相关性。</li></ul><p><img src="http://files.shanqianche.cn/202182/1627873762436.png" alt="多属性事件序列的深度预测模型"></p><ol start="4"><li>使用多阶段深度学习的方法来预测下一个事件。<strong>缺点</strong>是无法处理数值变量，所以也不能预测时间戳。</li></ol><ul><li>首先是将每个事件映射到特征向量</li><li>下一步使用transformations降低输入维度，通常有，通过提取n-gram、使用hash、将输入通过两个自动编码层等方法</li><li>将转化后的输入传给负责预测的前馈神经网络</li></ul><p><img src="http://files.shanqianche.cn/202182/1627887742568.png" alt="业务流程事件预测的多阶段深度学习方法"></p><ol start="5"><li>作者提出一种基于GRU的神经网络架构BINet，用于业务流程执行中的实时异常检测。该架构用于预测下一个事件及属性。</li></ol><ul><li>该方法旨在为跟踪中的每个事件分配一个似然分数，然后用于检测异常。这种方法表明，过程行为的生成模型也可用于异常检测。</li></ul><p><img src="http://files.shanqianche.cn/202182/1627907436804.png" alt="基于深度学习的多元业务流程异常检测"></p><ol start="6"><li>作者比较几种真实数据集在MMs，all-k MMs以及基于自动机的模型中预测下一步的准确性和性能。</li></ol><ul><li>结果表明，AKOM模型具有最高的精度（在某些情况下优于RNN体系结构），而基于自动机的模型具有较高的可解释性。</li></ul><p><img src="http://files.shanqianche.cn/202182/1627907971849.png" alt="下一个元素预测序列建模方法的跨学科比较"></p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p><img src="http://files.shanqianche.cn/2021811/1628666548975.png" alt="构建模型的步骤"></p><h2 id="预处理阶段"><a href="#预处理阶段" class="headerlink" title="预处理阶段"></a>预处理阶段</h2><h3 id="Data-transformation"><a href="#Data-transformation" class="headerlink" title="Data transformation"></a>Data transformation</h3><p><strong>根据属性性质（分类或连续）进行特定预处理。</strong></p><p><strong><font color="#294B71">分类：</font></strong></p><p>处理数据：将事件和资源作为类别属性，使用嵌入维度。</p><blockquote><p>向训练网络提供属性之间关联的正面和负面示例，使网络能够识别和定位具有相似特征的近似属性。根据NLP社区4中使用的一项通用建议，嵌入维度的数量被确定为类别数量的第四根，以避免它们之间可能发生冲突。生成的值作为不可训练的参数导出并在所有实验中重用，这样就不会增加模型的复杂性</p></blockquote><p><img src="http://files.shanqianche.cn/2021811/1628683817654.png" alt="用于训练嵌入层的网络结构和生成的4d空间被缩减为3d的空间"></p><p> <strong><font color="#E9967A">连续：</font></strong></p><p><font color="#1E90FF"> 对数据进行归一化，以供预测模型解释</font>。这里处理的事件之间的相对时间，问题在于不同日志，相对时间可能具有很大的可变性。 这种高可变性可以隐藏有关过程行为的有用信息，例如时间瓶颈或异常行为，如果不小心执行属性缩放，则可以隐藏这些信息。</p><p> 寻求一种合适的缩放方法。</p><p> <img src="http://files.shanqianche.cn/2021811/1628685428211.png" alt="最大值和对数归一化两种方法的对比"></p><h3 id="Sequences-creation"><a href="#Sequences-creation" class="headerlink" title="Sequences creation"></a>Sequences creation</h3><p> <strong>提取每个事件日志跟踪的固定大小的n-gram，以创建输入序列和预期事件来训练预测网络。</strong></p><p> <img src="http://files.shanqianche.cn/2021811/1628686156534.png" alt="从BPI 2012事件日志的案例id 174770中提取的五个n-gram"></p><p> <em>role表示的是事件与资源的关联</em></p><h2 id="Model-Structure-Definition-Phase"><a href="#Model-Structure-Definition-Phase" class="headerlink" title="Model Structure Definition Phase"></a>Model Structure Definition Phase</h2><p><img src="http://files.shanqianche.cn/2021811/1628686308297.png" alt="Baseline architecture"></p><p><img src="http://files.shanqianche.cn/2021811/1628687598994.png" alt="Tested architectures"></p><h2 id="Post-processing-Phase"><a href="#Post-processing-Phase" class="headerlink" title="Post-processing Phase"></a>Post-processing Phase</h2><p>从零前缀开始生成业务流程的完整跟踪中，传统使用的是arg max，直接根据下一个事件的最大概率来跟踪，但是这就会所有追踪的事件都倾向于概率最大值，对于低概率发生的事件无法追踪。这里作者使用的是arg max和随机选择的参数作为下一个事件的选择。</p><blockquote><p>我觉得应该使用softmax</p></blockquote><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><blockquote><p>本节描述了两个实验评估。第一个实验比较了三种架构在前处理和后处理选择方面的不同实例。第二个实验将提出的方法与技术背景中其它论文中的下一个事件、后缀和剩余时间预测任务的三条基线进行比较</p></blockquote><h2 id="Data-Set"><a href="#Data-Set" class="headerlink" title="Data Set"></a>Data Set</h2><pre><code>在本实验中，使用了九个来自不同领域、具有不同特征的真实事件日志</code></pre><ul><li>Helpdesk5事件日志包含来自意大利软件公司helpdesk票务管理过程的记录</li><li>BPI 20126 中的两个事件日志与来自德国金融机构的贷款申请流程相关。 这个过程由三个子过程组成，我们从中使用了 W 子过程，以便与”下一个元素预测序列建模方法的跨学科比较”进行比较 。</li><li>BPI 20137中的事件日志与沃尔沃的IT事件和问题管理有关。我们使用完整的案例学习生成模型</li><li>BPI 20158中的五个事件日志包含五个荷兰城市在四年期间提供的建筑许可证申请数据。原始事件日志分为五个部分（每个市政局一个）。所有事件日志都在子流程级别指定，包括345多个活动。因此，按照”Diagnostics of building per-mit application process in dutch municipalities”中所述的步骤对其进行预处理，以便在阶段级别进行管理</li></ul><p><img src="http://files.shanqianche.cn/2021811/1628688257014.png" alt="Event logs description"></p><ol><li><em><font color="#A9A9A9">SF指根据其在记录道数量、事件、活动和序列长度方面的组成分为简单、中等和复杂。</font></em></li><li><em><font color="#A9A9A9">TV指据每个事件日志的平均持续时间和最大持续时间之间的关系，将时间变异性（TV）分为稳定或可变</font></em></li></ol><h2 id="Experimental-setup"><a href="#Experimental-setup" class="headerlink" title="Experimental setup"></a>Experimental setup</h2><p><strong><font color="#8A2BE2">目的：</font></strong><br>    使用 LSTM 模型从大小为0的前缀开始跟踪生成完整的事件日志，然后将生成的跟踪与原始日志中的流程进行比较。</p><p><font color="#1E90FF">方法：使用两个指标来评估生成的事件日志的相似性。</font></p><ol><li>Demerau-Levinstain（DL）算法根据一个字符串与另一个字符串相等所需的版本数测量序列之间的距离。该算法在每次执行插入、删除、替换和转置等操作时都进行惩罚。因此，我们使用其倒数来衡量生成的活动或角色序列与实际事件日志中观察到的序列之间的相似性。然后，较高的值意味着序列之间的相似性较高。</li><li>平均绝对误差（MAE）度量用于测量预测时间戳的误差。通过取观测值和预测值之间距离的绝对值，然后计算这些震级的平均值来计算该测量值。我们使用该度量来评估每对（生成轨迹、地面真值轨迹）的生成相对时间和观测时间之间的距离。</li></ol><blockquote><p>使用交叉验证，将事件日志分为两部分：70%用于培训，30%用于验证。第一个折叠被用作训练2000个模型的输入（每个事件日志大约220个模型）。<br>这平均220个模型配置了不同的预处理技术和体系结构。配置值是从972个组合的完整搜索空间中随机选择的。然后，使用每个经过培训的模型生成完整事件的新事件日志（参见第3节中描述的选择下一个活动的技术）。生成了每个配置的15个日志，并对其结果进行了平均。评估了32000多个生成的事件日志。</p></blockquote><h2 id="Results-and-Interpretation"><a href="#Results-and-Interpretation" class="headerlink" title="Results and Interpretation."></a>Results and Interpretation.</h2><p><img src="http://files.shanqianche.cn/2021811/1628689342024.png" alt="不同配置的事件日志中的相似性结果"></p><ul><li><font color="#A9A9A9">MAE列对应于预测记录道周期时间的平均绝对误差</font> </li></ul><p>结果表明，使用这种方法可以训练学习并可靠地再现原始日志的观察行为模式的模型。此外，研究结果表明，对于LSTM模型来说， <strong><font color="#A52A2A">学习词汇量较大</font></strong> 的序列比学习较长的序列更困难。要了解这些模式，需要更多的示例，如BPI2012和BPI2015的结果所示。这两个日志都有30多个活动，但在跟踪数量上有很大差异（见表2）。BPI2012的高度相似性还表明，使用嵌入维度处理大量事件类型可以改善结果，只要示例数量足以学习底层模式。</p><p>针对本实验中评估的模型结构构件，我们按照预处理、模型结构和超参数选择以及预测等阶段对其进行分析，以构建生成模型。</p><p><img src="http://files.shanqianche.cn/2021811/1628689921162.png" alt="Preprocessing phase components comparison"></p><p><strong><font color="#FF8C00">这里主要比较的是对于相对时间缩放方式，和进行缩放的作用。</font></strong></p><ol><li>a 说明了如何使用最大值作为缩放技术，具有很小时间变化的日志呈现更好的结果。相比之下，具有不规则结构的日志使用对数归一化具有较低的<font color="#0000FF"> MAE</font>【横坐标】。</li><li>b 展示了使用不同大小的 n-gram 时的 DL 相似性结果，与事件日志的结构有关。 我们可以观察到，使用更长的n-grams对于trace更长的日志有更好的结果，呈现出稳定的增长趋势。 相比之下，中、简单结构的事件日志趋势不明显。 因此，应将长 n-gram 的使用保留给具有很长跟踪的日志。**<font color="#006400">这里体现的是n-gram size对于预测结果的作用，只有复杂的日志呈现良好的正相关。</font>**</li></ol><p><img src="http://files.shanqianche.cn/2021811/1628690528922.png" alt="共享层的总体相似性"></p><p>关于模型结构定义阶段，图说明连接结构的总体相似度最低。相比之下，仅在分类属性之间共享信息的模型体系结构具有中等最佳性能。然而，它与专门的体系结构并不遥远，尽管它的分布范围更广。这意味着在不同性质的属性之间共享信息会在网络正在处理的模式中产生噪声，从而阻碍学习过程。</p><p><img src="http://files.shanqianche.cn/2021811/1628690699173.png" alt="下一个事件选择方法的比较"></p><p>关于预测阶段，图显示了随机选择在所有事件日志中如何优于arg max。这种行为在具有较长和复杂跟踪的事件日志中更为明显。结果表明，无论事件日志结构如何，随机选择都是评估学习过程的可取方法。</p><h2 id="Comparison-Against-Baselines"><a href="#Comparison-Against-Baselines" class="headerlink" title="Comparison Against Baselines"></a>Comparison Against Baselines</h2><h3 id="Experimental-setup-1"><a href="#Experimental-setup-1" class="headerlink" title="Experimental setup"></a>Experimental setup</h3><p><strong><font color="#FF00FF">目的：</font></strong><br>    评估我们的方法在预测下一个事件、剩余事件序列（即后缀）和剩余时间（对于不同长度的跟踪前缀）方面的相对性能。</p><ol><li>next event prediction — 为每个模型提供长度增加的跟踪前缀，从 1 到每个跟踪的长度。 对于每个前缀，我们预测下一个事件并测量准确性（正确预测的百分比）。</li><li> suffix and remaining time prediction — 为模型提供了长度增加的前缀，直到案件结束。</li></ol><p><strong><font color="#8A2BE2">baselines:</font></strong></p><ul><li>next event and suffix prediction</li></ul><ol><li> Predictive business process monitoring with LSTM neural networks</li><li> Predicting process behaviour using deep learning</li><li> A deep predictive model for multi-attribute event sequence</li></ol><ul><li>remaining time prediction</li></ul><ol><li>Predictive business process monitoring with LSTM neural networks【Helpdesk, BPI2012W and BPI2012 event logs】</li></ol><h3 id="Results-and-Interpretation-1"><a href="#Results-and-Interpretation-1" class="headerlink" title="Results and Interpretation"></a>Results and Interpretation</h3><p><img src="http://files.shanqianche.cn/2021811/1628691394455.png" alt="下一个事件和后缀预测结果"></p><p><strong>这些结果表明，分类属性的维度控制所采用的措施，使我们的方法即使在长序列中也能获得始终如一的良好性能。</strong></p><p><img src="http://files.shanqianche.cn/2021811/1628691420458.png" alt="剩余循环时间MAE的结果（以天为单位）"></p><p>图10显示了剩余循环时间预测的MAE。尽管我们的技术目标不是预测剩余时间，但与Tax等人相比，它在这项任务中实现了类似的性能——在一个日志中略逊于它，在另一个日志中略逊于它的长前缀。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p><strong><font color="#FF8C00">优</font></strong></p><ol><li>评估表明，使用更长的n-gram可获得更高的精度</li><li>对数归一化是适用于高可变性测井的缩放方法，与总是选择最有可能的下一个事件相比，使用LSTM产生的概率随机选择下一个事件可导致更广泛的记录道和更高的精度。论文还表明，该方法在预测剩余事件序列及其从给定跟踪前缀开始的时间戳方面优于现有的基于LSTM的方法</li></ol><blockquote><p>作者预计，所提出的方法可以作为业务流程模拟的工具。实际上，从本质上讲，流程模拟器是一种通用模型，它生成由事件类型、资源和时间戳组成的跟踪集，并从中计算性能度量，如等待时间、循环时间和资源利用率。虽然流程模拟器依赖于可解释的流程模型（例如BPMN模型），但原则上可以使用能够生成事件跟踪的任何模型来模拟流程，其中每个事件都由事件类型（活动标签）、时间戳和资源组成。使用LSTM网络进行流程模拟的一个关键挑战是如何捕获“假设”场景（例如，删除任务或删除资源的效果）。</p></blockquote><p><strong><font color="#2F4F4F">future</font></strong><br>计划应用技术，使用”An eye into the future: Leveraging a-priori knowledge in predictive business process monitoring.”中的约束，从LSTM模型生成事件序列</p>]]></content>
    
    
    <summary type="html">Learning Accurate LSTM Models of Business Processes</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
    <category term="process mining" scheme="https://merlynr.github.io/tags/process-mining/"/>
    
    <category term="LSTM" scheme="https://merlynr.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>基于LSTM神经网络的业务过程预测监控</title>
    <link href="https://merlynr.github.io/2021/07/19/%E5%9F%BA%E4%BA%8ELSTM%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%9A%E5%8A%A1%E8%BF%87%E7%A8%8B%E9%A2%84%E6%B5%8B%E7%9B%91%E6%8E%A7/"/>
    <id>https://merlynr.github.io/2021/07/19/%E5%9F%BA%E4%BA%8ELSTM%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%9A%E5%8A%A1%E8%BF%87%E7%A8%8B%E9%A2%84%E6%B5%8B%E7%9B%91%E6%8E%A7/</id>
    <published>2021-07-18T16:00:00.000Z</published>
    <updated>2021-07-27T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=VAPwMDAm550&list=RDVAPwMDAm550&start_radio=1" title="我還年輕 我還年輕"><img src="https://res.cloudinary.com/marcomontalbano/image/upload/v1626681958/video_to_markdown/images/youtube--VAPwMDAm550-c05b58ac6eb4c4700831b2b3070cd403.jpg" alt="我還年輕 我還年輕"></a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文研究了长-短期记忆（LSTM）神经网络作为一种预测模型。并证明LSTMs在预测运行案例的下一个事件及其时间戳方面优于现有的技术。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>作者提到他的目的是提出一个可以适用的框架。本文的研究点：</p><ol><li>LSTMs能否被用于广泛的流程预测，以及如何应用？</li><li>如何保障LSTM在不同数据中的准确度始终如一？</li></ol><p>在不同预测内容中，作者适用了4个日志数据集进行验证比较。</p><h1 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h1><p>主要讲的是目前对于三种预测的技术，包括了，时间相关预测、事件结果的预测、正在执行事件的预测</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h2><ul><li>数据集A</li><li>数据集A中所有序列 $A^{*}$</li><li>一个长度为n的序列 σ =&lt;  $a_{1}$ ,  $a_{2}$ ,  $a_{3}$ , …… ,  $a_{n}$  &gt;,空序列为&lt;&gt;</li><li>$σ_ { 1 } \cdot σ _ { 2 }$ 表示序列 $σ_{1}$ 与 $σ_{2}$ 的串联</li><li> $h d ^ { k } ( o ) = ( a _ { 1 } , a _ { 2 } , \cdots , a _ { k } )$ 为<strong>前缀长度</strong>为k（0&lt;k&lt;n） 的序列 σ的<strong>前缀</strong>。 $t l ^ { k } ( o ) = ( a _ { k + 1 } \cdots , a _ { n } )$ 是它的后缀。</li></ul><p>对于前缀后缀的一个<strong>栗子</strong>：<br>序列： $σ _ { 1 } = ( a , b , c , d , e )$<br>前缀长度为二的前缀： $h d ^ { 2 } ( σ _ { 1 } ) = ( a , b )$<br>后缀为： $t l ^ { 2 } ( σ _ { 1 } ) = ( c , d , e )$</p><ul><li> $\varepsilon$ 【伊普西隆】为所有事件集合，T为时域。</li><li> $\pi _{ \tau }\in \varepsilon  \rightarrow T$ 为事件分配事件戳</li><li> $\pi _{ A }\in \varepsilon  \rightarrow A$ 从事件集A种为一个流程分配活动<h2 id="RNN和LSTM"><a href="#RNN和LSTM" class="headerlink" title="RNN和LSTM"></a>RNN和LSTM</h2></li></ul><p><a href="https://blog.zuishuailcq.xyz/2021/07/19/Long%20Short%20Term%20Memory%20Networks/">Long Short Term Memory Networks | 吾辈之人，自当自强不息！</a></p><h1 id="下一个活动和时间戳预测"><a href="#下一个活动和时间戳预测" class="headerlink" title="下一个活动和时间戳预测"></a>下一个活动和时间戳预测</h1><p>介绍评估多种体系结构预测下一个事件和时间戳。</p><p><img src="http://files.shanqianche.cn/2021728/1627442124718.png" alt="事件预测算法"></p><p><img src="http://files.shanqianche.cn/2021728/1627442195597.png" alt="事件预测算法"></p><p>输入一个事件的前缀，然后预测下一个事件。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><blockquote><p><strong><font color="#D2691E">知识补充</font></strong><br>one hot编码是将类别变量转换为机器学习算法易于利用的一种形式的过程。<br>假设“花”的特征可能的取值为daffodil（水仙）、lily（百合）、rose（玫瑰）。one hot编码将其转换为三个特征：is_daffodil、is_lily、is_rose，这些特征都是二进制的。<br><a href="https://zhuanlan.zhihu.com/p/37471802">什么是one hot编码？为什么要使用one hot编码？ - 知乎</a></p></blockquote><ol><li>首先为LSTM构建特征向量矩阵，作为输入。</li></ol><p>事件e=σ（i）的时间特征指的是在trace中的上一个时间和当前时间之间的时间。转换函数：<br><img src="http://files.shanqianche.cn/2021720/1626745464036.png" alt="enter description here"></p><p>三种时间特征：fvt1表示事件的当前时间特征【与上一个时间的时间间隔】，同时也添加了包函一天时间的特征fvt2和包含一周的时间特征fvt3.这样当事件在工作日或者工作周结束的时候预测下一个活动的事件中时间间隔则会更长。</p><ul><li>LSTM可以通过fvt1学到不同节点的事件宇时间差的依赖关系。</li><li>fvt2，fvt3的加入，是为处理有些有些事件超出了工作日的特殊情况，因为传统的日志处理中只记录工作日中的。</li></ul><ol start="2"><li>对时间步长k【第k个事件的时间】的输出 $o _ { a } ^ { k }$ 进行one-hot编码。</li></ol><p><font color="#6495ED">异常情况</font></p><ul><li>当在时间k为事件的结尾，既没有新的事件可以预测。 </li></ul><p><font color="#B22222"> <strong>解决</strong></font></p><ul><li>当在时间k结束时,给输出的one-hot编码向量增加额外标记值1 </li></ul><ol start="3"><li><p>设置第二个输出值 $o_{t}^{k}$ 为下一个时间间隔的 $fv_{t1}$ 值。当知道当前时间戳，就可以计算到下一个事件的时间戳。</p></li><li><p>使用Adam算法【梯度下降算法】进行神经网络权重优化</p></li></ol><p><strong><font color="#FF1493">如何优化：</font></strong></p><ul><li>最小化基础事件one-hot编码和被预测的下一个事件的one-hot编码的<font color="#0000FF">交叉熵</font>。</li><li>最小化事件和预测事件之间时间的<font color="#FF1493">平均误差</font>（MAE）</li></ul><h2 id="模型的构建"><a href="#模型的构建" class="headerlink" title="模型的构建"></a>模型的构建</h2><p><img src="http://files.shanqianche.cn/2021727/1627371341818.png" alt="单任务层神经网络结构"></p><p><img src="http://files.shanqianche.cn/2021727/1627371437071.png" alt="共享多任务层神经网络结构"></p><p><img src="http://files.shanqianche.cn/2021727/1627371484245.png" alt="n层共享，m层单任务混合神经网络结构"></p><ol><li>使用相同的数据特征，分别单独训练两个模型，一个是预测下一步事件，另一个是预测下一个时间戳，如图a</li><li>多任务学习可以在同一个神经网络结构学习到多个模型，例如图b，同一个LSTM神经网络结构学习输出两个模型。</li><li>混合模型</li></ol><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><ul><li>使用的循环神经网络依赖库Keras构建项目</li><li>硬件是NVidia Tesla k80 GPU，每次epoch时间为15-90s。其中预测时间时间戳是以毫秒为单位的。</li></ul><h2 id="实验准备"><a href="#实验准备" class="headerlink" title="实验准备"></a>实验准备</h2><h3 id="评定标准"><a href="#评定标准" class="headerlink" title="评定标准"></a>评定标准</h3><p>本文使用的<font color="#FF8C00">MAE</font>（平均绝对误差）来作为实验结果比较的参考。在实验效果评价这块，作者通过修改 van der Aalst提出的论文中用于预测剩余时间的模型来作为baseline。</p><h3 id="实验准备-1"><a href="#实验准备-1" class="headerlink" title="实验准备"></a>实验准备</h3><p>使用两个数据集进行预测下一个活动和时间戳。其中2/3的数据用于训练模型，1/3的用于预测。</p><p><i class="fas fa-tags" ></i>这里数据中长度为2的序列进行 $2 \leq k \lt | o |$ 预测，长度小于2则不对其预测。</p><p><strong><font color="#00008B">数据集</font></strong></p><ol><li>帮助中心数据集</li></ol><p>来自于意大利软件公司的票务管理系统，主要包括9中事件，一种事务流程。其中流程总共有3804条，事件有13710个。<br>2. BPI12子数据集W</p><p>此事件日志源自Business Process Intelligence Challenge（BPI’12）2，包含来自大型金融机构金融产品应用程序的数据。此流程由三个子流程组成：一个子流程跟踪应用程序的状态，一个子流程跟踪与应用程序关联的工作项的状态，第三个子流程跟踪报价的状态。在预测未来事件及其时间戳的上下文中，我们对自动执行的事件不感兴趣。因此，我们将评估范围缩小到工作项子流程，<font color="#FF8C00">其中包含手动执行的事件</font>。此外，我们过滤日志以<strong>仅保留complete类型的事件</strong>。</p><h2 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h2><p><img src="http://files.shanqianche.cn/2021727/1627377231055.png" alt="result，前缀为all表示所有前缀的平均值"><br>N表示神经元。MAE为当前配置不同前缀长度的性能。</p><p>表1显示了help desk和BPI’12w子流程日志上各种LSTM体系结构在MAE预测时间和预测下一事件准确性方面的性能。由于BPI12中的流程长度较长，所以前缀较长。</p><p>TODO 前缀不是很懂<br><font color="#7FFF00">分析：</font></p><ol><li>ALL LSTM体系结构在所有前缀上都优于baseline，同时分别比较LSTM模型和baseline模型，可以发现 <strong><font color="#FF00FF">短前缀的增益要比长前缀要好。</font></strong></li><li>数据集helpdesk的预测准确度最好为71%；BPI’12 W数据集预测的最佳精度为76%，高于Breuker等人报告的71.9%的精度和Evermann等人报告的62.3%的精度。</li><li>预测精度最高的模型都为混合模型，尝试将每层的神经元数量减少到75个，对于只有一个共享层的架构，将其增加到150个，但发现这会导致两个任务的性能下降。可能有75个神经元导致模型欠拟合，而150个神经元导致模型过拟合。我们还在单层架构上对传统RNN进行了实验，发现它们在时间和活动预测方面都比LSTM差得多。</li></ol><h1 id="后缀预测"><a href="#后缀预测" class="headerlink" title="后缀预测"></a>后缀预测</h1><blockquote><p><strong><font color="#ff7500">本章理解</font></strong><br>区别于上一节，上一章是单个时间步长预测下一步，而本章是预测一个运行案例的整个延续。</p></blockquote><p><img src="http://files.shanqianche.cn/2021728/1627454393652.png" alt="事件预测"></p><p><img src="http://files.shanqianche.cn/2021728/1627454420942.png" alt="时间预测"></p><h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>通过迭代地预测下一个事件和时间戳，然后再次进行预测直至这个案例结束。这里用 $\perp$ 表示案例结尾。</p><p><strong><font color="#006400">迭代预测：</font></strong></p><p><img src="http://files.shanqianche.cn/2021728/1627455885003.png" alt="事件预测算法"></p><p><img src="http://files.shanqianche.cn/2021728/1627456583245.png" alt="时间预测算法"></p><p><font color="#FF1493">当当前事件为END，则不进行预测；否则将预测的结果输入到预测模块迭代预测。</font></p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>对于预测准确度的评价这里采用的是通过计算预测结果与实际结果的编辑距离来衡量。</p><blockquote><p>知识补充：<font color="#2e4e7e">Levenshtein distance</font><br><a href="https://www.cnblogs.com/ivanyb/archive/2011/11/25/2263356.html">字符串相似度算法（编辑距离算法 Levenshtein Distance） - ZYB - 博客园</a></p></blockquote><p><strong>问题：</strong><br>  当处理并发任务的时候，Levenshtein distance并不适用于计算。例如&lt;a,b&gt;为预测的下一个事件，但实际上为&lt;b,a&gt;，这种情况只是因为ab并发顺序导致的，<font color="#DC143C">实际上并无相关</font>。但是Levenshtein distance结果为2,因为将预测序列转换为实际序列需要一次删除和一次插入操作。</p><p><strong>解决</strong><br>Damerau-Levenstein距离是一种更好地反映预测质量的评估度量，它为Levenshtein距离使用的操作集添加了交换操作。Damerau-Levenshtein距离将分配1的转换成本⟨a、 b⟩ 进入⟨b、 a⟩. 为了获得可变长度记录道的可比较结果，我们通过实际案例后缀长度和预测后缀长度的最大值对Damerau-Levenshtein距离进行归一化，归一化的Damerau-Levenshtein距离减1以获得Damerau-Levenshtein相似性（DLS）。</p><p><strong>模型</strong><br>采用的是双层架构，每层100个神经元的LSTM。</p><p><strong>数据集</strong><br>这是荷兰某市政当局环境许可程序的日志每宗个案涉及一份许可证申请。<br>该日志包含937个案例和381种事件类型的38944个事件。几乎每一种情况都遵循一个独特的路径，这使得后缀预测更具挑战性。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="http://files.shanqianche.cn/2021728/1627479266512.png" alt="基于Damerau-Levenshtein相似性的后缀预测结果"><br>Polato为baseline，no duplicates表示去掉案例中的重复事件，但是保留一个【案例结尾即使为重复事件也不去掉】。</p><p>观察可以发现所有数据集中的LSTM预测结果都要好于baseline。在BPI12数据集种很多案例中的事件会重复的出现，这就会导致预测的后缀长于真实案例，因此删除了BPI12中重复的事件并重新进行评估。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>利用LSTM神经网络预测运行案例的下一个活动及其时间戳的技术。表明，这种技术在现实数据集上优于现有的基线。此外，我们发现通过单一模型（多任务学习）预测下一个活动及其时间戳比使用单独的模型进行预测具有更高的准确性。</li><li>提出了一个运行种的案例整个延续的预测和预测剩余的周期时间的解决方案。</li><li>发现了LSTM模型的局限性，**<font color="#9932CC">即在一个案例种某些事件多次重复出现时，导致后缀过长，性能就会很低</font>**。</li></ol>]]></content>
    
    
    <summary type="html">Predictive Business Process Monitoring with LSTM Neural Networks</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>Long Short Term Memory Networks</title>
    <link href="https://merlynr.github.io/2021/07/19/Long%20Short%20Term%20Memory%20Networks/"/>
    <id>https://merlynr.github.io/2021/07/19/Long%20Short%20Term%20Memory%20Networks/</id>
    <published>2021-07-18T16:00:00.000Z</published>
    <updated>2021-07-18T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1842025914&auto=1&height=66"></iframe><p><a href="https://zhuanlan.zhihu.com/p/32085405">人人都能看懂的LSTM - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/37644325">RNN、LSTM、GRU基础原理篇 - 知乎</a><br><a href="https://blog.csdn.net/matrix_space/article/details/53374040">机器学习：深入理解 LSTM 网络 (一)_Matrix-11-CSDN博客_lstm 机器学习</a></p><blockquote><p>中文分词、词性标注、命名实体识别、机器翻译、语音识别都属于序列挖掘的范畴。<font color="#DC143C">序列挖掘</font>的特点就是某一步的输出不仅依赖于这一步的输入，还依赖于其他步的输入或输出。在序列挖掘领域传统的机器学习方法有HMM（Hidden Markov Model，隐马尔可夫模型）和CRF（Conditional Random Field，条件随机场），近年来流行深度学习算法RNN（Recurrent Neural Networks，循环神经网络）。</p></blockquote><h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p><img src="http://files.shanqianche.cn/2021719/1626698381786.png" alt="RNN网络架构图"></p><p>比如一个句子中有5个词，要给这5个词标注词性，那相应的RNN就是个5层的神经网络，每一层的输入是一个词，每一层的输出是这个词的词性。<br><img src="http://files.shanqianche.cn/2021719/1626698589144.png" alt="讲解"></p><h1 id="RNN的变体"><a href="#RNN的变体" class="headerlink" title="RNN的变体"></a>RNN的变体</h1><h2 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h2><p>双向RNN认为otot不仅依赖于序列之前的元素，也跟tt之后的元素有关，这在序列挖掘中也是很常见的事实。</p><p><img src="http://files.shanqianche.cn/2021719/1626698749683.png" alt=" Bidirectional RNNs网络结构"></p><h2 id="深层双向RNN"><a href="#深层双向RNN" class="headerlink" title="深层双向RNN"></a>深层双向RNN</h2><p>在双向RNN的基础上，每一步由原来的一个隐藏层变成了多个隐藏层。</p><p><img src="http://files.shanqianche.cn/2021719/1626698820535.png" alt="Deep Bidirectional RNNs网络结构"></p><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>前文提到，由于<font color="#9932CC"><strong>梯度消失/梯度爆炸</strong></font>的问题传统RNN在实际中很难处理长期依赖，而LSTM（Long Short Term Memory）则绕开了这些问题依然可以从语料中学习到长期依赖关系。</p><p><img src="http://files.shanqianche.cn/2021719/1626699453120.png" alt="传统RNN每一步的隐藏单元只是执行一个简单的tanh或ReLU操作"></p><p><img src="http://files.shanqianche.cn/2021719/1626699501199.png" alt=" LSTM每个循环的模块内又有4层结构:3个sigmoid层，1个tanh层"></p><p><strong><font color="#D2691E">解释LSTM模块：</font></strong></p><p><img src="http://files.shanqianche.cn/2021719/1626699615537.png" alt="图标说明"></p><ul><li>粉色的圆圈表示一个二目运算。</li><li>两个箭头汇合成一个箭头表示2个向量首尾相连拼接在一起。</li><li>一个箭头分叉成2个箭头表示一个数据被复制成2份，分发到不同的地方去。</li></ul><p> LSTM的关键是细胞状态C，一条水平线贯穿于图形的上方，这条线上只有些少量的线性操作，信息在上面流传很容易保持。</p><p><img src="http://files.shanqianche.cn/2021719/1626699717681.png" alt="忘记层"></p><ol><li>忘记层，决定细胞状态中<strong>丢弃什么信息</strong>。把ht−1和xt拼接起来，传给一个sigmoid函数，该函数输出0到1之间的值，这个值乘到细胞状态Ct−1上去。<font color="#FF8C00">sigmoid函数的输出值直接决定了状态信息保留多少</font>。比如当我们要预测下一个词是什么时，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。</li></ol><p><img src="http://files.shanqianche.cn/2021719/1626700419154.png" alt="更新细胞状态"><br>2. 上一步的细胞状态Ct−1已经被忘记了一部分，接下来本步应该把哪些信息新加到细胞状态中呢？这里又包含2层：一个tanh层用来产生更新值的候选项C~t，tanh的输出在[-1,1]上，<strong>说明细胞状态在某些维度上需要加强，在某些维度上需要减弱</strong>；还有一个sigmoid层（输入门层），它的输出值要乘到tanh层的输出上，起到一个<strong>缩放</strong>的作用，<em>极端情况下sigmoid输出0说明相应维度上的细胞状态不需要更新</em>。在那个预测下一个词的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。</p><p><img src="http://files.shanqianche.cn/2021719/1626700649763.png" alt="生成新的细胞状态"><br>3. 现在可以让旧的细胞状态Ct−1与ft（f是forget忘记门的意思）相乘来丢弃一部分信息，然后再加个需要更新的部分it∗C~t（i是input输入门的意思），这就生成了<strong>新的细胞状态Ct</strong></p><p><img src="http://files.shanqianche.cn/2021719/1626700735024.png" alt="循环模块的输出"><br>4. 最后该决定输出什么了。输出值跟细胞状态有关，把Ct输给一个tanh函数得到输出值的候选项。<strong>候选项中的哪些部分最终会被输出由一个sigmoid层来决定</strong>。在那个预测下一个词的例子中，如果细胞状态告诉我们当前代词是第三人称，那我们就可以预测下一词可能是一个第三人称的动词。</p><h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p>GRU（Gated Recurrent Unit）是LSTM最流行的一个变体，比LSTM模型要简单。没有了存储单元</p><p><img src="http://files.shanqianche.cn/2021719/1626700963311.png" alt="GRU"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>RNN 结构的一个吸引人之处在于其可以利用之前的输入信息。但是一个关键的需要解决的问题是当前的信息与之前的信息的关联度有长有短。<br>LSTM的内部结构。通过门控状态来控制传输状态，<strong>记住需要长时间记忆的，忘记不重要的信息</strong>；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。、<br>但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=1842</summary>
      
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="process mining" scheme="https://merlynr.github.io/tags/process-mining/"/>
    
  </entry>
  
  <entry>
    <title>损失函数之交叉熵(一般用于分类问题)</title>
    <link href="https://merlynr.github.io/2021/07/18/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5(%E4%B8%80%E8%88%AC%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)/"/>
    <id>https://merlynr.github.io/2021/07/18/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5(%E4%B8%80%E8%88%AC%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)/</id>
    <published>2021-07-17T16:00:00.000Z</published>
    <updated>2021-07-17T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=i73Lh3h0DpQ" title="ONE HOUR"><img src="https://res.cloudinary.com/marcomontalbano/image/upload/v1626618085/video_to_markdown/images/youtube--i73Lh3h0DpQ-c05b58ac6eb4c4700831b2b3070cd403.jpg" alt="ONE HOUR"></a></p><p><a href="https://blog.csdn.net/u014453898/article/details/81559462">损失函数之交叉熵(一般用于分类问题)_ZJE-CSDN博客</a></p><h2 id="信息量、信息熵、相对熵"><a href="#信息量、信息熵、相对熵" class="headerlink" title="信息量、信息熵、相对熵"></a>信息量、信息熵、相对熵</h2><h3 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h3><p><font color="#FF8C00">一件事发生的概率越大，其蕴含的信息量就越少，反之，若发生的几率越小，则蕴含的信息量就越大</font>。</p><p>例如，“太阳从东方升起”：这件事发生概率极大，大家都习以为常，所以不觉得有什么不妥的地方，因此蕴含信息量很小。但“国足踢入世界杯”：这就蕴含的信息量很大了，因为这件事的发生概率很小。</p><p><img src="http://files.shanqianche.cn/2021718/1626617713541.png" alt="信息量"></p><p>若某事x的发生概率为P(x)，则信息量的计算公式为：<br>$I ( x ) = - \log ( P ( x ) )$</p><p>上式的log的底为2，当然也可以是e、10。在神经网络中，log的底一般是e。当log的底大于1，log的图形就像下图红色线。因为P(x)的取值范围为0<del>1，可以看到，log的图像，在0</del>1的时候是负数，且P(x)越接近0，log越接近负无穷，P(x)越接近1，log越接近0，所以信息量的公式会在log前面加个负号，让log的取值范围为0~∞。当P(x)接近0，log接近无穷，P(x)接近1，log接近0，这符合信息量的概率越小，信息量越大的定义。</p><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>信息熵可以表达<strong>数据的信息量大小</strong>。<br>信息熵也被称为熵，用来表示所有信息量的<strong>期望</strong>。<br>期望是试验中每次可能结果的概率乘以其结果的总和。<br>信息熵的公式如下：<br>$H ( X ) = - \sum _ { i = 1 } ^ { n } P ( x _ { i } ) \log ( P ( x _ { i } )  )$ $( X = x _ { 1 } , x _ { 2 } , x _ { 3 } , x _ { n - 1 } , x _ { n } )$</p><p>使用明天的天气概率来计算其信息熵：</p><p><img src="http://files.shanqianche.cn/2021718/1626618468307.png" alt="天气"><br>$H ( X ) = - ( 0.5 * \log ( 0.5 ) + 0.2 * \log ( 0.2 ) + 0.3 * \log ( 0.3 ) )$</p><h3 id="KL散度（相对熵）—–用于衡量两个概率分布的差异"><a href="#KL散度（相对熵）—–用于衡量两个概率分布的差异" class="headerlink" title="KL散度（相对熵）—–用于衡量两个概率分布的差异"></a>KL散度（相对熵）—–用于衡量两个概率分布的差异</h3><p><strong><font color="#8A2BE2">如何理解 “衡量两个概率分布的差异”？</font></strong></p><p>例如在机器学习中，常常用P(x)表示样本的真实分布，用Q(x)表示模型预测的分布，比如在一个三分类任务中（例如，猫狗马分类器），[x1，x2，x3]分别表示猫，狗，马的概率，输入一张猫的图片，其真实分布为P(x)=[1，0，0]，预测分布为Q(x)=[0.7，0.2，0，1]，那么P(x)和Q(x)就是两个不同的概率分布，可以用KL散度来计算他们的差异。<br>公式为：<br><img src="http://files.shanqianche.cn/2021718/1626618687852.png" alt="enter description here"><br>KL散度越小，表示P(x)和Q(x)越接近，所以可以通过反复训练，来使Q(x)逼近P(x)，但KL散度有个特点，就是不对称，就是用P来你和Q和用Q来你和P的KL散度(相对熵)是不一样的，但是P和Q的距离是不变的。</p><p><strong>那KL散度(相对熵)和交叉熵有什么联系呢</strong>？</p><p>我们通过对相对熵公式进行变形：<br><img src="http://files.shanqianche.cn/2021718/1626618747252.png" alt="enter description here"><br>H(X)为之前的信息熵，后面那一坨其实就是交叉熵了，所以可以看到：**<font color="#FF00FF">KL散度 = 交叉熵 - 信息熵</font>**</p><p>所以交叉熵的公式如下：<br><img src="http://files.shanqianche.cn/2021718/1626618800801.png" alt="enter description here"><br>从信息熵的公式，我们知道，对于同一个数据集，其信息熵是不变的，所以信息熵可以看作一个常数，因此<font color="#9400D3">当KL散度最小时，也即是当交叉熵最小时</font>。在多分类任务中，KL散度(相对熵)和交叉熵是等价的。</p><h2 id="交叉熵的原理"><a href="#交叉熵的原理" class="headerlink" title="交叉熵的原理"></a>交叉熵的原理</h2><p>交叉熵是<font color="#057748">用来衡量两个 概率分布 的距离</font>(也可以叫差别)。[概率分布：即[0.1，0.5，0.2，0.1，0.1]，每个类别的概率都在0~1，且加起来为1]。</p><p>若有两个概率分布p(x)和q(x)，通过q来表示p的交叉熵为：(<strong>注意</strong>，p和q呼唤位置后，交叉熵是不同的)<br>$H ( p , q ) = - \sum p ( x ) \log q ( x )$<br>只要把p作为正确结果(如[0，0，0，1，0，0])，把q作为预测结果(如[0.1，0.1，0.4，0.1，0.2，0.1])，就可以得到两个概率分布的交叉熵了，<strong>交叉熵值越低，表示两个概率分布越靠近</strong>。</p><p><strong>交叉熵计算实例：</strong><br>假设有一个三分类问题，某个样例的正确答案是(1，0，0)，某个模型经过softmax回归之后的预测答案是(0.5，0.4，0.1)，那么他们的交叉熵为：<br><img src="http://files.shanqianche.cn/2021718/1626619013263.png" alt="enter description here"><br>如果另一个模型的预测概率分布为(0.8，0.1，0.1)，则这个预测与真实的交叉熵为：<br><img src="http://files.shanqianche.cn/2021718/1626619026409.png" alt="enter description here"><br>由于0.1小于0.3，所以第二个预测结果要由于第一个。</p><h2 id="使用交叉熵的背景"><a href="#使用交叉熵的背景" class="headerlink" title="使用交叉熵的背景"></a>使用交叉熵的背景</h2><p>通过神经网络解决分类问题时，一般会设置k个输出点，k代表类别的个数，如下图<br><img src="http://files.shanqianche.cn/2021718/1626619104894.png" alt="enter description here"><br>每个输出结点，都会输出该结点对应类别的得分，如[cat，dog，car，pedestrian] 为[44，10，22，5]</p><p>但是输出结点输出的是得分，而不是概率分布，那么就没有办法用交叉熵来衡量预测结果和真确结果了，那怎么办呢，**<font color="#FF00FF">解决方法是在输出结果后接一层 softmax，softmax的作用就是把输出得分换算为概率分布</font>**。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=i73Lh3h0DpQ&quot; title=&quot;ONE HOUR&quot;&gt;&lt;img src=&quot;https://res.cloudinary.com/marcomontalbano/image/upload/</summary>
      
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="process mining" scheme="https://merlynr.github.io/tags/process-mining/"/>
    
  </entry>
  
  <entry>
    <title>基于注意力机制的神经网络业务过程预测分析</title>
    <link href="https://merlynr.github.io/2021/07/17/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%9A%E5%8A%A1%E8%BF%87%E7%A8%8B%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/"/>
    <id>https://merlynr.github.io/2021/07/17/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%9A%E5%8A%A1%E8%BF%87%E7%A8%8B%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/</id>
    <published>2021-07-16T16:00:00.000Z</published>
    <updated>2021-07-17T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1842025914&auto=1&height=66"></iframe><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>提出了一种具有注意力机制的神经网络，它是使用公开的事件日志（如BPI Challenge 2013）进行训练。<br>同时使用n-gram模型对比结果和LSTM（长-短期记忆结构的神经网络）对比训练时间。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>提到使用以前较小的数据进行与之前的研究进行对比，同时也使用到了较大过程的日志进行评估。<br>本文的亮点，作者首次提出结合基于自我关注的transformer模型【NLP中常用】进行流程预测。</p><blockquote><p>Transformer：<br><a href="https://zhuanlan.zhihu.com/p/48508221">详解Transformer （Attention Is All You Need） - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/171875438">李宏毅-Attention，Self-Attention，Transformer - 知乎</a><br>Attention is All You Need<br><a href="https://finance.sina.com.cn/tech/2021-01-26/doc-ikftpnny1935086.shtml">堪比当年的LSTM，Transformer引燃机器学习圈：它是万能的|LSTM|机器学习_新浪科技_新浪网</a></p></blockquote><blockquote><p>残差网络<br>残差网络是为了解决深度神经网络（DNN）隐藏层过多时的网络退化问题而提出。退化（degradation）问题是指：当网络隐藏层变多时，网络的准确度达到饱和然后急剧退化，而且这个退化不是由于过拟合引起的。<br><a href="https://www.jiqizhixin.com/graph/technologies/738e788b-0e3b-4a8f-bd04-e407c7137694">深度残差网络 | 机器之心</a></p></blockquote><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><ol><li><p>Attention</p></li><li><p>self-attention</p></li></ol><p><a href="https://blog.csdn.net/At_a_lost/article/details/108469516">Attention机制与Self-Attention机制的区别_At_a_lost的博客-CSDN博客_attention和self attention的区别</a></p><ol start="2"><li><p>Transformer<br>Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，Transformer只是一个功能更强大的词袋模型而已。<br>为了解决这个问题，论文中在编码词向量时引入了位置编码（Position Embedding）的特征。—自己设计编码规则</p></li><li><p>前馈神经网络也经常称为多层感知器（Multi-Layer Perceptron，MLP）</p></li></ol><h3 id="事件日志"><a href="#事件日志" class="headerlink" title="事件日志"></a>事件日志</h3><p><strong><font color="#FF8C00">简述日志结构</font></strong></p><p><img src="http://files.shanqianche.cn/2021717/1626528110183.png" alt="流程结构--UML"></p><p>一个事件日志由多个案例组成，但一个案例总是分配给一个事件日志。案件与事件的关系也是如此；事件的典型属性是<strong>活动、持续时间、优先级或成本</strong>。</p><p>事件日志与事件案例：一对多<br>事件案例与事件：一对多</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p><strong>这块需要提前学习Transformer</strong></p><ol><li>通常，序列中的每个位置可以关注序列中的任何其他位置。作者提到为了不让Softmax函数计算时不考虑位置特征，将当前事件之后的位置的值设置为无穷大【忽略位置特征】</li></ol><ul><li>我的理解是位置特征无法通过分类来实现，这也是Transformer无法捕获序列顺序的原因</li></ul><ol start="2"><li>这里为了梯度的稳定，Transformer使用了score归一化，即除以 $\sqrt{d_{k}}$</li></ol><blockquote><p>**<font color="#7FFF00">知识补充</font>**：softmax<br><a href="https://zhuanlan.zhihu.com/p/105722023">一文详解Softmax函数 - 知乎</a><br>多分类、求大</p></blockquote><p><img src="http://files.shanqianche.cn/2021717/1626531018697.png" alt="Attention函数"></p><p>$W _ { i } ^ { Q } , W _ { i } ^ { K } , W _ { i } ^ { V } \in R^{d_{model}*d_{k}}$ ， $W^{O}\in R^{hd_{k}*d_{model}}$ 【 $d_{model}$ 表示嵌入的长度（可以理解为词嵌入）】</p><p><img src="http://files.shanqianche.cn/2021717/1626531384298.png" alt="self-attention函数"></p><p><img src="http://files.shanqianche.cn/2021717/1626531047214.png" alt="相当于h 个不同的self-attention的集成，全连接层"></p><p>通过学习线性变换将Q向量、K向量和V向量投影到h个不同的子空间中，在每个子空间上<strong>并行</strong>计算Attention值。结果被连接并投射到特征空间，使得模型能够联合处理来自不同位置的不同表示子空间的特征。</p><h2 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h2><ol><li>2016年之前主要是使用MMs（生成模型）和聚类算法（KNN或k-means）相结合；剩下的就是一些类似MMs，例如，基于贝叶斯概念的概率有限自动机（概率模型），它使用期望的极大似然估计。 数据来源2012，2013公开的BPI比赛。</li><li>2016至今，几乎都是长短期记忆结构（LSTM）然后与其它模型相结合的方法来预测，比如结合词嵌入的神经网络，使用一个热编码转换事件特征，并将其与生成的时间特征连接到单个特征向量。数据来源2012，2013公开的BPI比赛</li><li>另一种方法将事件预测视为经典的多类分类问题，并使用堆叠的自动编码器提取特征，然后使用深度前馈网络对特征进行分类。然而，这种方法只适用于简单的数据集，因为不同表示的数量随着唯一事件类的数量呈多项式增长。</li></ol><blockquote><p><font color="#FF8C00">知识补充</font> 独热编码<br><a href="https://www.cnblogs.com/zongfa/p/9305657.html">数据预处理：独热编码（One-Hot Encoding）和 LabelEncoder标签编码 - 理想几岁 - 博客园</a></p></blockquote><blockquote><p><font color="#9400D3">知识补充</font> 多项式增长<br>也就是对于变量n，5n^2+2n+1这种就叫做多项式。</p></blockquote><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><strong>本实验数据集：</strong></p><ol><li>BPI Challenge 2013</li><li>一家德国软件公司提供的额外数据集。后一个数据集的事件日志包括律师、会计师和审计员使用一种特殊软件工具进行财务核算、管理付款交易和编制年度财务报表的情况。</li></ol><p>数据集由大约2.08亿个事件组成，由会话id标识，该会话id指示它们所属的情况、事件类型和时间戳。每个用户交互（通常是点击按钮）都被视为一个事件，一个案例从应用程序启动一直持续到关闭。</p><p>难点：<br>作者通过流程挖掘，许多独特的事件类增加了预测的难度。在<strong>较小的数据集中</strong>，大多数情况下都非常短。大约四分之一的病例由五个或更少的事件组成，只有百分之一的流程长度为500或更多。<br>同时数据集中大部分数据为重复的，数据类型不均匀，最常见的五个事件类型几乎占了<strong>整个数据集</strong>的一半。</p><h2 id="建模方法"><a href="#建模方法" class="headerlink" title="建模方法"></a>建模方法</h2><p>模型是在tensorflow上实现的，使用tf.data-API作为输入管道，tf.keras-API构建我们的模型。【不懂，反正tensorflow上的组件哇】</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol><li>整理事件类型，并用整数标记</li><li>将一个流程的所有事件放到一个张量（tensor，多维数组，能够创造更高维度的矩阵、向量。）</li><li>为每个流程添加一个结束标记，并且通过左移一个位置来生成训练标签【没懂为啥要左移生成标签，但是目的是为了生成训练标签，感觉是<strong>通过监控位置标签判断是否为训练数据集</strong>】</li><li>在上一部分中提到数据集长短不一，作者提出通过按照长度将流程分到不同区域中。这里为了确保每个区域中的流程相似，就没有设置固定的长度，而是通过制定上限来控制长度。</li></ol><p><img src="http://files.shanqianche.cn/2021717/1626535517054.png" alt="上限控制"></p><p>$l_{1}&lt;9$ </p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p><img src="http://files.shanqianche.cn/2021717/1626535685586.png" alt="a为模型，b为attention模块"></p><p>N表示Transformer的数量、 $d_{model}$ 表示嵌入长度、h表示Attention的数量， $d_{l}$ 表示Q，K，V的向量维度。<br>M表示词汇的数量，这里可以理解为事件的数量，n为单个流程的长度，bsz表示流程的数量，单次样本数量。<br>Pos.Encoding  位置编码标记</p><blockquote><p><strong><font color="#DC143C">知识补充</font></strong><br><a href="https://blog.csdn.net/program_developer/article/details/78597738">神经网络中Epoch、Iteration、Batchsize相关理解和说明_Microstrong-CSDN博客_epoch</a><br><strong>epoch</strong>：中文翻译为时期。<br>一个时期=所有训练样本的一个正向传递和一个反向传递。<br>举个例子，训练集有1000个样本，batchsize=10，那么：<br>训练完整个样本集需要：<br>100次iteration，1次epoch。</p></blockquote><blockquote><p><strong>理解辅助：</strong><br>这块主要和Transformer论文中的Shared-Weight Embeddings and Softmax这一部分一样<br>与其他序列转导模型类似，使用可学习的 Embeddings 将 input tokens and output tokens 转换为维度  的向量【序列转序列转为d（model）维度的向量】。通过线性变换和 softmax 函数将解码器的输出向量转换为预测的 token 概率。在 Transformer 模型中，两个嵌入层和 pre-softmax 线性变换之间共享相同的权重矩阵，在 Embeddings 层中，将权重乘以 . 这些都是当前主流的操作。</p></blockquote><ol><li>通过被一个正态分布初始权值的可训练查找矩阵构成的<strong>嵌入层</strong>将输入映射到一个 $d_{model}$ 维度的特征空间【①使用embedding将输入转化为 $d_{model}$ 维度的向量】，同时pre-softmax函数在transformer中使用相同的查找矩阵。</li><li>由于Transformer不对顺序有预测效果，所以将位置编码加入到嵌入向量中【与第一个方法类似结合sin，cos】</li><li>通过交叉熵和标签滑动来进行拟合结果和消除过拟合</li><li>最后作者提到注意计算结果在内部缓存并复用的问题，主要在担心内存溢出，但是实验缓冲所需的空间可忽略不计。</li></ol><blockquote><p><strong><font color="#0000FF">知识补充</font></strong><br>label smoothing(标签平滑)：正则化策略，为了防止过拟合，加入噪声<br><a href="https://zhuanlan.zhihu.com/p/116466239">label smoothing(标签平滑)学习笔记 - 知乎</a><br>交叉熵即预测值与真实值之间的差值，越少越精准。<br><a href="https://blog.zuishuailcq.xyz/2021/07/18/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5(%E4%B8%80%E8%88%AC%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)/">损失函数之交叉熵(一般用于分类问题) | 吾辈之人，自当自强不息！</a></p></blockquote><blockquote><p><strong><font color="#00CED1">知识补充：</font></strong> 模型理解<br><a href="https://zhuanlan.zhihu.com/p/60821628">碎碎念：Transformer的细枝末节 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/106867810">Transformer理论源码细节详解 - 知乎</a><br><a href="https://www.codenong.com/cs106837783/">Transformer论文详解，论文完整翻译（七） | 码农家园</a><br><a href="https://congchan.github.io/NLP-attention-03-self-attention/">Transformer &amp; Self-Attention (多头) 自注意力编码 | Fly Me to the Moon</a></p></blockquote><h2 id="结果验证"><a href="#结果验证" class="headerlink" title="结果验证"></a>结果验证</h2><p>作者使用了前面提到的BPI2013和DATSET，每个数据集分为三个部分：培训，验证和测试。对于BPI2013的数据集，作者选的的训练、验证、测试集之间比例80%，10%，10%。DATSET则是96%，2%，2%。<br>BPI2013用了30000个训练模型，DATSET使用了一百万个，一个epoch为1000个。</p><ol><li>BPI2013</li></ol><p><img src="http://files.shanqianche.cn/2021718/1626620621539.png" alt="BPI2013结果"><br>展示了四种超参数的配置 $d_{model}$ 为嵌入长度，h为Attention数量， $d_{ff}$ 规定了点式前馈神经网络的内部第一层输出节点， 为前反馈网络中的参数。</p><p>分析，在最小超参数的配置中，当结合四层Transformer后精确度达到最高。同时随着配置的增加，训练的效果却下降了，这里可能是设置超参数过大，欠拟合。</p><p><img src="http://files.shanqianche.cn/2021718/1626622123211.png" alt="近几年比较"><br>【4】”Comprehensible predictive models for business processes” 2016<br>【11】“Predicting process behaviour using deep learning”2016<br>【13】“A multi-stage deep learning approach for business process event prediction”2017</p><p>实验表明，我们的注意力竞争方法通常适用于过程事件预测的任务，并且可以与现有技术相当执行。</p><ol start="2"><li>DATSET</li></ol><p>由于DATSET的数据集较大，这里直接使用较大的超参数，同时结合4和6层的transformer</p><p><img src="http://files.shanqianche.cn/2021718/1626622668082.png" alt="超参数"></p><p><img src="http://files.shanqianche.cn/2021718/1626622687231.png" alt="DATSET"></p><pre><code>    训练的时候将超过500的流程筛选出来，这些只占了数据集1%</code></pre><p>分析，最高为0.6218。超参数较小的模型显着更糟糕，这表明它们无法完全模拟数据的复杂性。但是没有以前的数据进行对比，作者使用LSTM基本模型使用相同的超参数【256，N=4】进行对比。</p><p><img src="http://files.shanqianche.cn/2021718/1626622951569.png" alt="LSTM VS  Model"></p><p>证明了所提出的基于关注力的模型比基于LSTM的模型更好。此外，根据LSTM训练时间是我们的两倍。 <strong>TODO</strong> <font color="#FF00FF">这显示了注意机制对于长跟踪长度的优势，它能够一次处理整个跟踪，而不是一次处理一个元素。</font></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文，作者提出了基于注意力机制的业务流程预测模型，通过数据集验证，不仅在较小的数据集上（BPI2013）可以接近现有技术的精确度，同时在预测复杂的的数据集也可以达到良好的效果，同时训练时间更少。</p><p>提到对于复杂的模型处理的新的思路，<strong>对于流程轨迹进行分割，对于重复的事件进行预处理，缩短流程提升预测精度</strong>或者是<strong>仅部分学习预测</strong>。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=1842</summary>
      
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制</title>
    <link href="https://merlynr.github.io/2021/07/16/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <id>https://merlynr.github.io/2021/07/16/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</id>
    <published>2021-07-15T16:00:00.000Z</published>
    <updated>2021-07-15T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/105335191">注意力机制到底是什么——基于常识的基本结构介绍 - 知乎</a><br><a href="https://www.zhihu.com/question/304499365">(96 封私信 / 80 条消息) 「注意力机制」是什么意思？ - 知乎</a><br><a href="https://my.oschina.net/u/876354/blog/3061863">大话注意力机制（Attention Mechanism） - 雪饼的个人空间 - OSCHINA - 中文开源技术交流社区</a><br><a href="https://zhuanlan.zhihu.com/p/148737297">attention机制中的query,key,value的概念解释 - 知乎</a></p><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>注意力机制(Attention Mechanism)是人们在机器学习模型中嵌入的一种特殊结构，用来自动学习和计算输入数据对输出数据的<font color="#1E90FF"><strong>贡献</strong></font>大小。</p><blockquote><p>目前，注意力机制已经成为深度学习领域，尤其是自然语言处理领域，应用最广泛的“组件”之一。这两年曝光度极高的BERT、GPT、Transformer等等模型或结构，都采用了注意力机制。</p></blockquote><h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>来自于认知工程领域提出的，类似人对于信息采集的机制—特征工程。</p><h3 id="人身上的注意力机制"><a href="#人身上的注意力机制" class="headerlink" title="人身上的注意力机制"></a>人身上的注意力机制</h3><p>去超市购物，和朋友去购物，作为提东西的工具人，不仅要体力跟的上，那么我们还需要的是跟的上朋友的步伐，人山人海中要跟上步伐确实比较困难，当我们用眼睛去一个一个寻找，把路人的信息特征衣服，脸，发色，发型~都传入到脑海中一个一个比对寻找朋友的时候，这时候不仅效率极其低而且大脑表示也遭不住，那么我们只需要记到部分明显特征发型身高等，然后扩大视野，这样效率会明显提高。<br>像这种情形，**<font color="#9932CC">有选择的处理信号</font>**，包括人类很多生物在处理外界信号时的策略，这种处理机制就是注意力机制。</p><h3 id="特征工程——模型外部的注意力机制"><a href="#特征工程——模型外部的注意力机制" class="headerlink" title="特征工程——模型外部的注意力机制"></a>特征工程——模型外部的注意力机制</h3><p>严格来说，「注意力机制」更像是一种方法论。没有严格的数学定义，而是根据具体任务目标，对关注的方向和加权模型进行调整。<br>简单的理解就是，在神经网络的隐藏层，增加「注意力机制」的加权。<br>使不符合注意力模型的内容弱化或者遗忘。</p><p><img src="http://files.shanqianche.cn/2021717/1626502007361.png" alt="特征工程"></p><h3 id="“key-query-value”理论"><a href="#“key-query-value”理论" class="headerlink" title="“key-query-value”理论"></a>“key-query-value”理论</h3><p><img src="http://files.shanqianche.cn/2021717/1626502458858.png" alt="Attention机制"></p><p>将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：<br>$Attention( Q u e r y , Source ) = \sum _ { i = 1 } ^ { L _ { x } }Similarity(Query,Key_{i})*Value_{i}$</p><blockquote><p><strong><font color="#7FFF00">个人理解</font></strong><br>可以把attention机制看作一种软寻址：Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，**<font color="#7FFF00">取出内容的重要性根据Query和Key的相似性来决定</font>**，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。</p></blockquote><p><strong><font color="#008B8B">Attention机制的具体计算过程：</font></strong></p><p><img src="http://files.shanqianche.cn/2021717/1626508762885.png" alt="计算过程"></p><ol><li>根据Query和Key计算两者的相似性或者相关性【学习】</li><li>对第一阶段的原始分值进行归一化处理【获取权重系数】</li><li>根据权重系数对Value进行加权求和</li></ol><blockquote><p><strong><font color="#00008B">知识补充</font></strong><br>点乘又叫向量的内积、数量积，是一个向量和它在另一个向量上的投影的长度的乘积；是标量。 <strong>点乘反映着两个向量的“相似度”</strong>，两个向量越“相似”，它们的点乘越大。<br>向量叉乘求的是<strong>垂直</strong>于这两个向量<br>Cosine相似性，求余弦<br>多层感知器（Multilayer Perceptron,缩写MLP）一种通用的函数近似方法，可以被用来拟合复杂的函数，或解决分类问题</p></blockquote><p>第一阶段中根据Query和Key求相似度目前常见的方法包括：求两者的向量点积、求两者的向量 Cosine相似性或者通过再引入额外的<br>神经网络来求值如下：<br>点积：<br>$Similarity(Query,Key_{i})=Query*Key_{i}$</p><p>Cosine相似性：<br>$Similarity(Query,Key_{i})=\frac { Query<em>Key_{i} } { ||Query||</em>||Key_{i}||}$</p><p>MLP网络：<br>$Similarity(Query,Key_{i})=MLP（Query*Key_{i}$</p><p>第二阶段引入类似SoftMax的计算方式对第一阶段的相似度得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。<br>$a_{i}=Softmax(Sim_{i})= \frac{e^{Sim_{i}}}{ \textstyle \sum_{j=1}^{L_{x}}e^{Sim_{j}} }$</p><p>最后一阶段是加权求和求Attention数值：<br>$Attention(Query,Source)={\textstyle \sum_{i=1}^{L_{x}}a_{i}\cdot Value_{i}}$</p><h2 id="深度学习领域的注意力机制"><a href="#深度学习领域的注意力机制" class="headerlink" title="深度学习领域的注意力机制"></a>深度学习领域的注意力机制</h2><h3 id="注意力机制的思想和基本框架"><a href="#注意力机制的思想和基本框架" class="headerlink" title="注意力机制的思想和基本框架"></a>注意力机制的思想和基本框架</h3><blockquote><p>一些学者尝试让<font color="#9400D3">模型自己学习如何分配自己的注意力</font>，即为输入信号加权。<em>他们用注意力机制的直接目的，就是为输入的各个维度打分，然后按照得分对特征加权，以突出重要特征对下游模型或模块的影响。这也是注意力机制的基本思想。</em></p></blockquote><p><strong>一般会采用”key-query-value”理论来描述注意力机制的机理。</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/105335191&quot;&gt;注意力机制到底是什么——基于常识的基本结构介绍 - 知乎&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.zhihu.com/question/304499365&quot;&gt;(</summary>
      
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="process mining" scheme="https://merlynr.github.io/tags/process-mining/"/>
    
  </entry>
  
  <entry>
    <title>强化学习</title>
    <link href="https://merlynr.github.io/2021/07/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <id>https://merlynr.github.io/2021/07/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</id>
    <published>2021-07-12T16:00:00.000Z</published>
    <updated>2021-07-12T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/steven-yang/p/6481772.html">强化学习读书笔记 - 00 - 术语和数学符号 - SNYang - 博客园</a><br><a href="https://www.cnblogs.com/wacc/p/5391209.html">强化学习笔记1 - Hiroki - 博客园</a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>监督学习在机器学习中取得了重大的成功，然而在<strong>顺序决策制定</strong>和<strong>控制问题</strong>中，比如无人直升机、无人汽车等，难以给出显式的监督信息，因此这类问题中<strong>监督模型无法学习</strong>。<br>强化学习就是为了解决这类问题而产生的。在强化学习框架中，学习算法被称为一个agent，假设这个agent处于一个环境中，两者之间存在交互。<font color="#9400D3">agent通过与环境交互不断增强对环境的适应力，故得名强化学习。</font></p><p><img src="http://files.shanqianche.cn/2021713/1626145210759.png" alt="强化学习"></p><p>在每个时间步 $t$ ，agent：</p><ul><li>接受状态 $s _ { t }$</li><li>接受标量回报 $r _ { t }$</li><li>执行行动 $a _ { t }$</li></ul><p>环境：</p><ul><li>接受动作 $a _ { t }$</li><li>产生状态 $s _ { t }$</li><li>产生标量回报 $r _ { t }$</li></ul><h2 id="MDP-马尔科夫决策过程"><a href="#MDP-马尔科夫决策过程" class="headerlink" title="MDP(马尔科夫决策过程)"></a>MDP(马尔科夫决策过程)</h2><p>通常我们都是从MDP（马尔科夫决策过程）来了解强化学习的。MDP问题中，我们有一个五元组： $( S , A , P , \gamma , P )$</p><ul><li>$S$ :状态集，由agent所有可能的状态组成</li><li>$A$ :动作集，由agent所有可能的行动构成</li><li>$P ( s , a , s ^ { \prime } )$ :转移概率分布，表示状态s下执行动作a后下个时刻状态的概率分布</li><li>$\gamma$ :折扣因子，0≤ $\gamma$ ≤1，表示未来回报相对于当前回报的重要程度。如果 $\gamma$ =0，表示只重视当前立即回报； $\gamma$ =1表示将未来回报视为与当前回报同等重要。【<font color="#FF8C00">这块不懂，可以看后面下围棋的栗子</font>】</li><li>$R ( s , a , s ^ { \prime } )$ :标量立即回报函数。执行动作a，导致状态s转移到s′产生的回报。可以是关于状态-动作的函数 $S \times A \rightarrow R$ ，也可以是只关于状态的函数 $S \rightarrow R$ 。记t时刻的回报为 $r _ { t }$ ，为了后续表述方便，假设我们感兴趣的问题中回报函数只取决于状态，而状态-动作函数可以很容易地推广，这里暂不涉及。</li></ul><p><strong><font color="#8B008B">注：</font></strong> 这里阐述的MDP称为discounted MDP，即<font color="#00008B">带折扣因子的MDP</font>。有些MDP也可以定义为四元组： $( S , A , P , R )$ ，这是<em>因为这类MDP中使用的值函数不考虑折扣因子</em>。</p><blockquote><p>**<font color="#9932CC">马尔可夫性质</font>*<em>：当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布<font color="#006400">仅依赖于当前状态</font>；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是<font color="#1E90FF">条件独立</font>的，那么此随机过程即具有马尔可夫性质。<br>例如：</em>明天的天气（是否下大雨）仅与今天的天气（是否刮大风）有关，而与前天及以前的天气无关*。</p></blockquote><p>MDP过程具有马尔科夫性质，即给定当前状态，未来的状态与过去的状态无关。但与马尔科夫链不同的是，MDP还考虑了<strong>动作</strong>，也就是说MDP中状态的转移不仅和状态有关，还依赖于agent采取的动作。</p><p>我们可以通过下面表格了解各种马尔科夫模型的区别：</p><table><thead><tr><th></th><th>不考虑动作</th><th>考虑动作</th></tr></thead><tbody><tr><td>状态可观测</td><td>马尔科夫链（MC）</td><td>马尔科夫决策过程（MDP）</td></tr><tr><td>状态不完全可观测</td><td>隐马尔科夫模型（HMM）</td><td>不完全可观察马尔可夫决策过程（POMDP）</td></tr></tbody></table><p><strong><font color="#FF8C00">MDP的运行过程：</font></strong><br><img src="http://files.shanqianche.cn/2021713/1626158954807.png" alt="MDP"></p><p>我们从初始状态 $s _ { 0 }$ 出发，执行某个动作 $a _ { 0 }$ ，根据转移概率分布确定下一个状态 $s _ { 1 }$ ∼ $P _ { s0a0 }$ ，接着执行动作 $a _ { 1 }$ ，再根据 $P _ { s1a1 }$ 确定 $s _ { 2 }$ …。</p><p>一个discounted MDP中，我们的目标最大化一个<font color="#1E90FF">累积未来折扣回报</font>:<br>$R _ { t } = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } r _ { t + k + 1 }$</p><p>具体地，我们希望学得一个<strong>策略</strong>（policy），通过执行这个策略使上式最大化。策略一般可以表示为一个函数，它以<font color="#9932CC">状态</font>为输入，输出<font color="#9932CC">对应的动作</font>。策略函数可以是确定的 $\pi ( s ) = a$ ，也可以是不确定的  $\pi ( s , a ) = p ( a | s )$ （这时策略函数是一个<em>条件概率分布</em>，表示给定状态s下执行下一个动作a的概率）。当agent执行一个策略时，每个状态下agent都执行策略指定的动作。</p><p>强化学习通常具有<strong>延迟回报</strong>的特点，以下围棋为例，只有在最终决定胜负的那个时刻才有回报（赢棋为1，输棋为-1），而之前的时刻立即回报均为0。这种情况下， $R _ { t }$ 等于1或-1，这将导致我们很难衡量策略的优劣，因为即使赢了一盘棋，未必能说明策略中每一步都是好棋；同样输了一盘棋也未必能说明每一步都是坏棋。因此我们需要一个目标函数来刻画策略的长期效用。<br>为此，我们可以为策略定义一个<strong>值函数</strong>（value function）来综合评估某个策略的好坏。这个函数既可以是只关于状态的值函数 $V ^ { \pi } ( s )$ ，也可以状态-动作值函数 $Q ^ { \pi } ( s , a )$ 。<font color="#2F4F4F">状态值函数评估agent处于某个状态下的长期收益</font>， 动作值函数评估agent在某个状态下执行某个动作的长期收益。</p><p>本文后续都将以 <strong><font color="#FF8C00">状态值函</font></strong> 数为例，进行阐述。一般常用的有三种形式：</p><ol><li>$V ^ { \pi } ( s ) = E _ { \pi } [ \sum _ { k = 0 } ^ { \infty } r _ { t + k + 1 } | s _ { t } = s ]$</li><li>$V ^ { \pi } ( s ) = E _ { \pi } [ \lim _ { k \rightarrow \infty } \frac { 1 } { k } \sum _ { i = 0 } ^ { k } T _ { t + i + 1 } | s _ { t } = s ]$</li><li>$V ^ { \pi } ( s ) = E _ { \pi } [ \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } r _ { t + k + 1 } | s _ { t } = s ]$</li></ol><p>其中 $E _ { \pi } [ \cdot | s _ { t } = s ]$ 表示从状态s开始，通过执行策略 $π$  得到的累积回报的期望。有些情况下，agent和环境的交互是无止境的，比如一些控制问题，这样的问题称为 <strong><font color="#9400D3">continuing task</font></strong> 。还有一种情况是我们可以把交互过程打散成一个个 **<font color="#9400D3">片段式任务</font>**（episodic task），每个片段有一个起始态和一个终止态（或称为吸收态，absorbing state），比如下棋。当每个episode结束时，我们对整个过程重启随机设置一个起始态或者从某个随机起始分布采样决定一个起始态。<br>上面三种值函数中，我们一般常用第三种形式，我把它叫做 **<font color="#9400D3">折扣值函数</font>**（discounted value function）。</p>]]></content>
    
    
    <summary type="html">【可選】文章描述</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
    <category term="machine leaning" scheme="https://merlynr.github.io/tags/machine-leaning/"/>
    
  </entry>
  
  <entry>
    <title>Markov Model（马尔可夫模型）</title>
    <link href="https://merlynr.github.io/2021/07/05/Markov%20Model%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%89/"/>
    <id>https://merlynr.github.io/2021/07/05/Markov%20Model%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%89/</id>
    <published>2021-07-04T16:00:00.000Z</published>
    <updated>2021-07-06T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/mantch/p/11203748.html">一次性弄懂马尔可夫模型、隐马尔可夫模型、马尔可夫网络和条件随机场！(词性标注代码实现) - mantch - 博客园</a></p><h2 id="马尔可夫网络、马尔可夫模型、马尔可夫过程、贝叶斯网络的区别"><a href="#马尔可夫网络、马尔可夫模型、马尔可夫过程、贝叶斯网络的区别" class="headerlink" title="马尔可夫网络、马尔可夫模型、马尔可夫过程、贝叶斯网络的区别"></a>马尔可夫网络、马尔可夫模型、马尔可夫过程、贝叶斯网络的区别</h2><p>共分六点说明这些概念【<font color="#DC143C">这6点是依次递进的，不要跳跃着看</font>】：</p><ol><li>将随机变量作为结点，若两个随机变量相关或者不独立，则将二者连接一条边；若给定若干随机变量，则形成一个有向图，即构成一个<strong>网络</strong>。</li><li>如果该网络是有向无环图，则这个网络称为<strong>贝叶斯网络</strong>。</li><li>如果这个图退化成线性链的方式，则得到<strong>马尔可夫模型</strong>；因为每个结点都是随机变量，将其看成各个时刻(或空间)的相关变化，以随机过程的视角，则可以看成是<strong>马尔可夫过程</strong>。</li><li>若上述网络是无向的，则是无向图模型，又称<strong>马尔可夫随机场</strong>或者<strong>马尔可夫网络</strong>。</li><li>如果在给定某些条件的前提下，研究这个马尔可夫随机场，则得到<strong>条件随机场</strong>。</li><li>如果使用条件随机场解决标注问题，并且进一步将条件随机场中的网络拓扑变成线性的，则得到<strong>线性链条件随机场</strong>。</li></ol><h2 id="马尔可夫模型"><a href="#马尔可夫模型" class="headerlink" title="马尔可夫模型"></a>马尔可夫模型</h2><h3 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h3><p>马尔可夫过程（Markov process）是一类<font color="#00FFFF">随机</font>过程。它的原始模型是马尔可夫链。<br>该过程具有如下特性：在已知目前状态（现在）的条件下，它未来的演变（将来）<font color="#0000FF">不依赖</font>于它以往的演变 (过去 )。</p><p>每个状态的转移只依赖于之前的n个状态，这个过程被称为1个n阶的模型，其中n是影响转移状态的数目。最简单的马尔可夫过程就是一阶过程，<font color="#006400">每一个状态的转移只依赖于其之前的那一个状态</font>，这个也叫作<strong>马尔可夫性质</strong>。</p><p>假设这个模型的每个状态都只依赖于之前的状态，这个假设被称为<font color="#1E90FF">马尔科夫假设</font>，这个假设可以大大的简化这个问题。显然，这个假设可能是一个非常糟糕的假设，导致很多重要的信息都丢失了。<br><img src="http://files.shanqianche.cn/202176/1625557564343.png"></p><p>假设天气服从<strong>马尔可夫链</strong>：</p><p><img src="http://files.shanqianche.cn/202176/1625557700847.png" alt="天气"></p><p>从上面这幅图可以看出：</p><ul><li>假如今天是晴天，明天变成阴天的概率是0.1</li><li>假如今天是晴天，明天任然是晴天的概率是0.9，和上一条概率之和为1，这也符合真实生活的情况。</li></ul><p><img src="http://files.shanqianche.cn/202178/1625728052403.png" alt="表格"></p><p>由上表我们可以得到马尔可夫链的<strong>状态转移矩阵</strong>：<br><img src="http://files.shanqianche.cn/202176/1625557951076.png" alt="状态转移矩阵"></p><p>因此，一阶马尔可夫过程定义了以下三个部分：</p><ul><li>状态：晴天和阴天</li><li>初始向量：定义系统在时间为0的时候的状态的概率</li><li>状态转移矩阵：每种天气转换的概率</li></ul><p>马尔可夫模型（Markov Model）是一种<font color="#DC143C">统计模型</font>，广泛应用在语音识别，词性自动标注，音字转换，概率文法等各个自然语言处理等应用领域。经过长期发展，尤其是在语音识别中的成功应用，使它成为一种通用的统计工具。到目前为止，它一直被认为是实现快速精确的语音识别系统的最成功的方法。</p><h2 id="隐马尔可夫模型（HMM）"><a href="#隐马尔可夫模型（HMM）" class="headerlink" title="隐马尔可夫模型（HMM）"></a>隐马尔可夫模型（HMM）</h2><blockquote><p> 在某些情况下马尔科夫过程不足以描述我们希望发现的模式。回到之前那个天气的例子，一个隐居的人可能不能直观的观察到天气的情况，但是有一些海藻。民间的传说告诉我们海藻的状态在某种概率上是和天气的情况相关的。在这种情况下我们有两个状态集合，一个可以观察到的状态集合（海藻的状态）和一个隐藏的状态（天气的状况）。我们希望能找到一个算法可以根据海藻的状况和马尔科夫假设来预测天气的状况。</p></blockquote><p>而这个算法就叫做**隐马尔可夫模型(HMM)**。</p><p><img src="http://files.shanqianche.cn/202176/1625559690747.png" alt="HMM"></p><p>隐马尔可夫模型 (Hidden Markov Model) 是一种<strong>统计模型</strong>，用来描述一个含有隐含未知参数的马尔可夫过程。<strong>它是结构最简单的动态贝叶斯网，这是一种著名的有向图模型，</strong> 主要用于<font color="#FF00FF">时序</font>数据建模，在语音识别、自然语言处理等领域有广泛应用。</p><h3 id="隐马尔可夫三大问题"><a href="#隐马尔可夫三大问题" class="headerlink" title="隐马尔可夫三大问题"></a>隐马尔可夫三大问题</h3><p><font color="#9400D3">注意</font></p><ol><li>给定模型，如何有效计算产生观测序列的概率？换言之，如何评估模型与观测序列之间的<font color="#FF1493">匹配程度</font>？</li><li>给定模型和观测序列，如何找到与此观测序列最匹配的状态序列？换言之，如何根据观测序列推断出隐藏的<font color="#B22222">模型状态</font>？</li><li>给定观测序列，如何调整模型参数使得该序列出现的概率最大？换言之，如何训练模型使其能最好地<font color="#B22222">描述</font>观测数据？</li></ol><p>前两个问题是模式识别的问题：1) 根据隐马尔科夫模型得到一个可观察状态序列的概率(<strong>评价</strong>)；2) 找到一个隐藏状态的序列使得这个序列产生一个可观察状态序列的概率最大(<strong>解码</strong>)。第三个问题就是根据一个可以观察到的状态序列集产生一个隐马尔科夫模型（<strong>学习</strong>）。</p><p>对应的三大问题解法：</p><ol><li>向前算法(Forward Algorithm)、向后算法(Backward Algorithm)</li><li>维特比算法(Viterbi Algorithm)</li><li>鲍姆-韦尔奇算法(Baum-Welch Algorithm) (约等于EM算法)</li></ol><blockquote><p>小明现在有三天的假期，他为了打发时间，可以在每一天中选择三件事情来做，这三件事情分别是散步、购物、打扫卫生(<strong>对应着可观测序列</strong>)，可是在生活中我们所做的决定一般都受到天气的影响，可能晴天的时候想要去购物或者散步，可能下雨天的时候不想出门，留在家里打扫卫生。而天气(晴天、下雨天)就属于<strong>隐藏状态</strong>，用一幅概率图来表示这一马尔可夫过程：</p></blockquote><p><img src="http://files.shanqianche.cn/202176/1625563725550.png" alt="场景"></p><p>那么，我们提出三个问题，分别对应马尔可夫的<font color="#B22222">三大</font>问题：</p><ol><li>已知整个模型，我观测到连续三天做的事情是：散步，购物，收拾。那么，根据模型，计算产生这些行为的概率是多少。</li><li>同样知晓这个模型，同样是这三件事，我想猜，这三天的天气是怎么样的。</li><li>最复杂的，我只知道这三天做了这三件事儿，而其他什么信息都没有。我得建立一个模型，晴雨转换概率，第一天天气情况的概率分布，根据天气情况选择做某事的概率分布。</li></ol><h3 id="第一个问题解法"><a href="#第一个问题解法" class="headerlink" title="第一个问题解法"></a>第一个问题解法</h3><ol><li><strong>遍历算法</strong></li></ol><p>假设第一天(T=1 时刻)是晴天，想要购物，那么就把图上的对应概率相乘就能够得到了。<br>第二天(T=2 时刻)要做的事情，在第一天的概率基础上乘上第二天的概率，依次类推，最终得到这三天(T=3 时刻)所要做的事情的概率值，这就是遍历算法，简单而又粗暴。但问题是<font color="#2F4F4F">用遍历算法的复杂度会随着观测序列和隐藏状态的增加而成指数级增长。</font></p><p><font color="#B22222">复杂度为</font>：<math xmlns="http://www.w3.org/1998/Math/MathML"><mn>2</mn><mi>T</mi><msup><mi>N</mi><mi>T</mi></msup></math></p><p><font color="#8A2BE2">理解：</font>每次计算行为发生概率都从最开始遍历计算</p><ol start="2"><li><p><strong>前向算法</strong></p><ol><li>假设第一天要购物，那么就计算出第一天购物的概率(包括晴天和雨天)；假设第一天要散步，那么也计算出来，依次枚举。</li><li>假设前两天是购物和散步，也同样计算出这一种的概率；假设前两天是散步和打扫卫生，同样计算，枚举出前两天行为的概率。</li><li>第三步就是计算出前三天行为的概率。</li></ol></li></ol><p>第二步中要求的概率可以在第一步的基础上进行，同样的，第三步也会<font color="#0000FF">依赖</font>于第二步的计算结果。那么这样做就能够<strong>节省很多计算环节，类似于动态规划</strong>。</p><p><font color="#B22222">复杂度为</font>：<math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>N</mi><mn>2</mn></msup><mi>T</mi></math></p><ol start="3"><li>后向算法</li></ol><p>跟前向算法相反，我们知道总的概率肯定是1，那么B_t=1，也就是最后一个时刻的概率合为1，先计算前三天的各种可能的概率，在计算前两天、前一天的数据，<font color="#696969">跟前向算法相反</font>的计算路径。</p><h3 id="第二个问题解法"><a href="#第二个问题解法" class="headerlink" title="第二个问题解法"></a>第二个问题解法</h3><ol><li>维特比算法（Viterbi）</li></ol><blockquote><p>维特比算法是一个特殊但应用最广的<strong>动态规划算法</strong>。利用动态规划，可以解决任何一个图中的<strong>最短</strong>路径问题。而维特比算法是针对一个特殊的图—篱笆网络（Lattice）的有向图最短路径问题而提出的。它之所以重要，是因为凡是使用<font color="#057748">隐含马尔可夫模型</font>描述的问题都可以用它来解码，包括今天的数字通信、语音识别、机器翻译、拼音转汉字、分词等。</p></blockquote><p>维特比算法一般用于模式识别，通过观测数据来<font color="#FF1493">反推出隐藏状态</font>。</p><p>因为是要根据观测数据来反推，所以这里要进行一个假设，<strong>假设这三天所做的行为分别是：散步、购物、打扫卫生</strong>，那么我们要求的是这三天的天气(路径)分别是什么。</p><ol><li>初始计算第一天下雨和第一天晴天去散步的概率值：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>1</mn></msub><mo>(</mo><mi>R</mi><mo>)</mo></math>表示第一天下雨的概率<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x3C0;</mi><mi>R</mi></msub></math>表示中间的状态(下雨)概率<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>b</mi><mi>R</mi></msub><mo>(</mo><msub><mi>O</mi><mn>1</mn></msub><mo>=</mo><mi>w</mi><mo>)</mo></math>表示下雨并且散步的概率<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>a</mi><mrow><mi>R</mi><mo>-</mo><mi>R</mi></mrow></msub></math>表示下雨天到下雨天的概率</li></ol><p><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>1</mn></msub><mo>(</mo><mi>R</mi><mo>)</mo></math>=<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x3C0;</mi><mi>R</mi></msub><mo>*</mo><msub><mi>b</mi><mi>R</mi></msub><mo>(</mo><msub><mi>O</mi><mn>1</mn></msub><mo>=</mo><mi>w</mi><mo>)</mo></math>=0.6 * 0.1 = 0.06</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>1</mn></msub><mo>(</mo><mi>S</mi><mo>)</mo></math>=<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x3C0;</mi><mi>S</mi></msub><mo>*</mo><msub><mi>b</mi><mi>S</mi></msub><mo>(</mo><msub><mi>O</mi><mn>1</mn></msub><mo>=</mo><mi>w</mi><mo>)</mo></math>=0.4 * 0.6 = 0.24</p><p><font color="#006400">初始路径</font>为：<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x3D5;</mi><mn>1</mn></msub><mo>(</mo><mi>R</mi><mo>)</mo></math>=Rainy<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>&#x3D5;</mi><mn>1</mn></msub><mo>(</mo><mi>S</mi><mo>)</mo></math>=Sunny</p><ol start="2"><li>计算第二天下雨和第二天晴天去购物的概率值:</li></ol><p><img src="http://files.shanqianche.cn/202177/1625641072263.png" alt="行为概率"></p><p><font color="#00FFFF">对应路径为：</font></p><p><img src="http://files.shanqianche.cn/202177/1625642052293.png" alt="对应路径"></p><ol start="3"><li>计算第三天下雨和第三天晴天去打扫卫生的概率值：</li></ol><p><img src="http://files.shanqianche.cn/202177/1625642832307.png" alt="第三天概率"></p><p><font color="#228B22">对应路径为：</font></p><p><img src="http://files.shanqianche.cn/202177/1625642928241.png" alt="行为路径"></p><ol start="4"><li><p>比较每一步中△的概率大小，选取最大值并找到对应的路径，依次类推就能找到最有可能的隐藏状态路径。</p><ol><li>第一天的概率最大值为 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>1</mn></msub><mi>S</mi></math>，对应路径为Sunny，</li><li>第二天的概率最大值为 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>2</mn></msub><mi>S</mi></math>，对应路径为Sunny，</li><li>第三天的概率最大值为 <math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>△</mi><mn>3</mn></msub><mi>S</mi></math>，对应路径为Rainy。</li></ol></li><li><p>合起来的路径就是Sunny-&gt;Sunny-&gt;Rainy，这就是我们所求。</p></li></ol><h3 id="第三个问题解法"><a href="#第三个问题解法" class="headerlink" title="第三个问题解法"></a>第三个问题解法</h3><p>鲍姆-韦尔奇算法(Baum-Welch Algorithm) (约等于<strong>EM</strong>算法)</p><p>如果训练数据只有观测序列而没有状态序列，即{<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>O</mi><mn>1</mn></msub><mo>,</mo><msub><mi>O</mi><mn>2</mn></msub><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msub><mi>O</mi><mi>S</mi></msub></math>}此时HMM的学习就得使用EM算法了，这是<font color="#FF1493">非监督</font>学习。</p><p>通常，如果给定数据和已经模型，那么求模型参数我们会用<font color="#8A2BE2">极大似然估计法</font>，但是<font color="#8B008B">如果变量中含有隐变量，无法用极大似然求解</font>（对数式子里面有求和，难以求出解析解），此时就可以使用EM算法。考虑HMM，观测序列 O是显变量，而状态变量I  则是隐变量，所以HMM实际上是<font color="#556B2F">含有隐变量的概率模型</font></p><p><img src="http://files.shanqianche.cn/202177/1625646003916.png" alt="HMM的概率模型 | λ为模型参数"></p><blockquote><p><font color="#00FFFF">知识补充</font><br>极大似然估计<br>利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！<br>换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“<font color="#FF8C00">模型已定，参数未知</font>”。</p></blockquote><p>可以使用EM算法来求得模型参数。</p><p>关于EM算法流程，有多个版本，但是仔细学习可以发现是大同小异的，以下使用《统计学习方法》上介绍的EM算法流程。</p><p><a href="https://blog.csdn.net/qq_37334135/article/details/86302735">HMM学习笔记（二）：监督学习方法与Baum-Welch算法_成都往右的博客-CSDN博客</a></p><h2 id="马尔可夫网络"><a href="#马尔可夫网络" class="headerlink" title="马尔可夫网络"></a>马尔可夫网络</h2><h3 id="因子图"><a href="#因子图" class="headerlink" title="因子图"></a>因子图</h3><p>WiKIpedia：将一个具有多变量的全局函数因子分解，得到几个局部函数的乘积，以此为基础得到的一个双向图叫做<font color="#00CED1">因子图</font>（Factor Graph）。</p><p>通俗来讲，所谓因子图就是对函数进行因子分解得到的一种<strong>概率图</strong>。一般内含两种节点：变量节点和函数节点。我们知道，一<font color="#FF1493">个全局函数通过因式分解能够分解为多个局部函数的乘积</font>，这些局部函数和对应的变量关系就体现在因子图上。</p><p><img src="http://files.shanqianche.cn/202177/1625649785149.png" alt="栗子"></p><p>其中fA,fB,fC,fD,fE为各函数，表示变量之间的关系，可以是条件概率也可以是其他关系。其对应的因子图为：</p><p><img src="http://files.shanqianche.cn/202177/1625659847778.png" alt="变量-函数之间因子图"></p><p><img src="http://files.shanqianche.cn/202177/1625659895044.png" alt="变量-函数之间因子图"></p><h3 id="马尔可夫网络-1"><a href="#马尔可夫网络-1" class="headerlink" title="马尔可夫网络"></a>马尔可夫网络</h3><blockquote><p>我们已经知道，<strong>有向</strong>图模型，又称作<font color="#bf242a">贝叶斯网络</font>，但在有些情况下，强制对某些结点之间的边增加方向是不合适的。<strong>使用没有方向的无向边，形成了无向图模型</strong>（Undirected Graphical Model,UGM）, 又被称为<strong>马尔可夫随机场或者马尔可夫网络</strong>（Markov Random Field, MRF or Markov network）。</p></blockquote><p><img src="http://files.shanqianche.cn/202177/1625660204643.png" alt="MRF"></p><p>设X=(X1,X2…Xn)和Y=(Y1,Y2…Ym)都是<font color="#006400">联合随机变量</font>，若随机变量Y构成一个无向图 G=(V,E)表示的马尔可夫随机场（MRF），则条件概率分布P(Y|X)称为<strong>条件随机场</strong>（Conditional Random Field, 简称CRF，后续新的博客中可能会阐述CRF）。如下图所示，便是一个线性链条件随机场的无向图模型：</p><p><img src="http://files.shanqianche.cn/202177/1625664493380.png" alt="CRF"></p><p>在概率图中，求某个变量的边缘分布是常见的问题。这问题有很多求解方法，其中之一就是<font color="#A52A2A">把贝叶斯网络或马尔可夫随机场转换成因子图，然后用sum-product算法求解</font>。换言之，基于因子图可以用<strong>sum-product 算法</strong>高效的求各个变量的边缘分布。</p><p>详细的sum-product算法过程，请查看博文：<a href="https://blog.csdn.net/v_july_v/article/details/40984699">从贝叶斯方法谈到贝叶斯网络_结构之法 算法之道-CSDN博客_贝叶斯</a></p><h2 id="条件随机场-CRF"><a href="#条件随机场-CRF" class="headerlink" title="条件随机场(CRF)"></a>条件随机场(CRF)</h2><p><strong>一个通俗的例子</strong></p><p>假设你有许多小明同学一天内不同时段的照片，从小明提裤子起床到脱裤子睡觉各个时间段都有（小明是照片控！）。现在的任务是对这些照片进行分类。比如有的照片是吃饭，那就给它打上吃饭的标签；有的照片是跑步时拍的，那就打上跑步的标签；有的照片是开会时拍的，那就打上开会的标签。问题来了，你准备怎么干？</p><p>一个简单直观的办法就是，不管这些照片之间的时间顺序，想办法训练出一个多元分类器。就是用一些打好标签的照片作为训练数据，训练出一个模型，直接根据照片的特征来分类。例如，如果照片是早上6:00拍的，且画面是黑暗的，那就给它打上睡觉的标签;如果照片上有车，那就给它打上开车的标签。</p><p>乍一看可以！但实际上，由于我们忽略了这些照片之间的时间顺序这一重要信息，我们的分类器会有缺陷的。举个例子，假如有一张小明闭着嘴的照片，怎么分类？显然难以直接判断，需要参考闭嘴之前的照片，如果之前的照片显示小明在吃饭，那这个闭嘴的照片很可能是小明在咀嚼食物准备下咽，可以给它打上吃饭的标签；如果之前的照片显示小明在唱歌，那这个闭嘴的照片很可能是小明唱歌瞬间的抓拍，可以给它打上唱歌的标签。</p><p>所以，为了让我们的分类器能够有更好的表现，<strong>在为一张照片分类时，我们必须将与它相邻的照片的标签信息考虑进来</strong>。这——就是条件随机场(CRF)大显身手的地方！这就有点类似于词性标注了，只不过把照片换成了句子而已，本质上是一样的。</p><p>如同马尔可夫随机场，条件随机场为具有<strong>无向</strong>的图模型，图中的顶点代表随机变量，顶点间的连线代表随机变量间的相依关系，在条件随机场中，随机变量Y 的分布为条件机率，给定的观察值则为随机变量 X。下图就是一个线性连条件随机场。</p><p><img src="http://files.shanqianche.cn/202177/1625665022911.png" alt="线性连条件随机场"></p><p>条件概率分布P(Y|X)称为条件随机场。</p><h2 id="EM算法、HMM、CRF的比较"><a href="#EM算法、HMM、CRF的比较" class="headerlink" title="EM算法、HMM、CRF的比较"></a>EM算法、HMM、CRF的比较</h2><ol><li><strong>EM算法</strong>是<font color="#8FBC8F">用于含有隐变量模型</font>的极大似然估计或者极大后验估计，有两步组成：E步，求期望（expectation）；M步，求极大（maxmization）。本质上EM算法还是一个迭代算法，通过不断用上一代参数对隐变量的估计来对当前变量进行计算，直到收敛。注意：EM算法是对初值敏感的，而且EM是不断求解下界的极大化逼近求解对数似然函数的极大化的算法，也就是说<font color="#B22222">EM算法不能保证找到全局最优值</font>。对于EM的导出方法也应该掌握。</li><li><strong>隐马尔可夫模型</strong>是<font color="#8FBC8F">用于标注问题的生成模型</font>。有几个参数（π，A，B）：初始状态概率向量π，状态转移矩阵A，观测概率矩阵B。称为马尔科夫模型的三要素。马尔科夫三个基本问题：<br>  <strong>概率计算问题</strong>：给定模型和观测序列，计算模型下观测序列输出的概率。–》前向后向算法<br> <strong>学习问题</strong>：已知观测序列，估计模型参数，即用极大似然估计来估计参数。–》Baum-Welch(也就是EM算法)和极大似然估计。<br> <strong>预测问题</strong>：已知模型和观测序列，求解对应的状态序列。–》近似算法（贪心算法）和维比特算法（动态规划求最优路径）</li><li><strong>条件随机场CRF</strong>，给定一组输入随机变量的条件下另一组输出随机变量的条件概率分布密度。条件随机场假设输出变量构成马尔科夫随机场，而我们平时看到的大多是线性链条随机场，也就是由输入对输出进行预测的判别模型。求解方法为<font color="#B22222">极大似然估计或正则化的极大似然估计。</font></li><li>之所以总把HMM和CRF进行比较，主要是因为CRF和HMM都利用了图的知识，但是CRF利用的是马尔可夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。</li><li>HMM和CRF对比：其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/mantch/p/11203748.html&quot;&gt;一次性弄懂马尔可夫模型、隐马尔可夫模型、马尔可夫网络和条件随机场！(词性标注代码实现) - mantch - 博客园&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;马尔可夫网</summary>
      
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
    <category term="machine leaning" scheme="https://merlynr.github.io/tags/machine-leaning/"/>
    
  </entry>
  
  <entry>
    <title>KNN（K近邻法 K Nearest Neighbors）</title>
    <link href="https://merlynr.github.io/2021/07/05/KNN%EF%BC%88K%E8%BF%91%E9%82%BB%E6%B3%95%20K%20Nearest%20Neighbors%EF%BC%89/"/>
    <id>https://merlynr.github.io/2021/07/05/KNN%EF%BC%88K%E8%BF%91%E9%82%BB%E6%B3%95%20K%20Nearest%20Neighbors%EF%BC%89/</id>
    <published>2021-07-04T16:00:00.000Z</published>
    <updated>2021-07-04T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/25994179">一文搞懂k近邻（k-NN）算法（一） - 知乎</a></p><p><a href="https://blog.csdn.net/qq_20412595/article/details/82013677">机器学习算法（2）之K近邻算法_不曾走远的博客-CSDN博客</a></p><p><a href="https://blog.csdn.net/pxhdky/article/details/85080980">【机器学习】K近邻法（KNN）与kd树原理详解_齐在的专栏-CSDN博客</a></p><p>TODO 序列KNN</p><h2 id="KNN概述"><a href="#KNN概述" class="headerlink" title="KNN概述"></a>KNN概述</h2><ul><li>常用有监督学习方法</li><li>常用分类方法</li><li>同时也是回归方法</li><li>是懒惰学习</li></ul><blockquote><p><font color="#ff7500">扩展学习</font><br>懒惰学习是一种训练集处理方法，其会<font color="#012C54">在收到测试样本的同时进行训练</font>，与之相对的是急切学习，其会<font color="#8A2BE2">在训练阶段开始对样本进行学习</font>处理。</p></blockquote><p><font color="#FF8C00">基本思路：</font><br>如果一个待分类样本在特征空间中的k个最相似(即特征空间中K近邻)的样本中的大多数属于某一个类别，则该样本也属于这个类别，即近朱者赤，近墨者黑。</p><h2 id="KNN算法介绍"><a href="#KNN算法介绍" class="headerlink" title="KNN算法介绍"></a>KNN算法介绍</h2><h3 id="KNN模型"><a href="#KNN模型" class="headerlink" title="KNN模型"></a>KNN模型</h3><p><strong>kNN使用的模型实际上对应于对特征空间的划分。</strong></p><p><font color="#006400">由三个及基本要素组成：</font></p><ul><li>距离度量</li><li>k值的选择</li><li>决策规划</li></ul><ol><li>距离度量</li></ol><p>KNN中使用的距离度量可以是欧式距离、曼哈顿距离、切比雪夫距离或者一般的闵可夫斯基距离。</p><blockquote><p><font color="#9932CC">知识补充</font><br>设特征空间 $X$ 是 $n$ 维实数向量空间<math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mi>n</mi></msup></math>，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub></math>∈ $X$ ，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mo>(</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msubsup><mo>)</mo><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msubsup><msup><mo>)</mo><mi>T</mi></msup></math>，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>j</mi></msub><mo>=</mo><mo>(</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msubsup><mo>)</mo><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msubsup><mi>x</mi><mi>j</mi><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msubsup><msup><mo>)</mo><mi>T</mi></msup></math></p><ol><li><p>闵可夫斯基距离（Minkowski distance,<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离）<br><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mi>j</mi></msub></math>的<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离定义为：<br><img src="http://files.shanqianche.cn/202175/1625491452207.png"><br>其中，p ≥ 1 。 </p></li><li><p>曼哈顿距离（Manhattan distance）<br>当p = 1 时，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离就变成了曼哈顿距离：<br><img src="http://files.shanqianche.cn/202175/1625491491032.png"></p></li><li><p>欧式距离（Euclidean distance）<br>当p = 2时，<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mi>p</mi></msub></math>距离就变成了欧式距离：<br><img src="http://files.shanqianche.cn/202175/1625491674920.png"></p></li><li><p>切比雪夫距离（Chebyshev distance）<br>当<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>p</mi><mo>=</mo><mo>&#x221E;</mo><mo>,</mo><msub><mi>L</mi><mi>p</mi></msub></math>距离就变成了切比雪夫距离，它是各个坐标距离的最大值：<br><img src="http://files.shanqianche.cn/202175/1625491693716.png"></p></li></ol></blockquote><ol start="2"><li>k值选择（借鉴李航–统计学习方法）</li></ol><p>如果k值较小，则训练误差减少，只有与输入实例相似的训练实例才会对于预测结果起作用,“学习”<font color="#D2691E">近似误差会减小</font>，但泛化误差提高了，预测结果会对近邻实例点非常敏感。k值较小意味着模型变得复杂，容易发生<font color="#0000FF">过拟合</font>。</p><p>如果k值较大，可以减少泛化误差，其优点是可以<font color="#D2691E">减少学习的估计误差</font>，但训练误差会增加，这时与输入实例相差较远的训练实例也会对预测结果起作用。k值较大意味着模型变得简单，容易发生<font color="#0000FF">欠拟合</font>。</p><p>通常情况下，我们需要对 k 经过多种尝试，来决定到底使用多大的 k 来作为最终参数。k通常会在3～10直接取值，或者是k等于训练数据的<font color="#DC143C">平方根</font>。比如15个数据，可能会取k=4。<br>第二种方法，选择能使测试集达到最优的k kk，即能够使得如MAPE等衡量预测准确度的统计量达到最小；<br>第三种方法，同时训练多个函数不同参数k kk的模型，然后取所有模型的预测值的平均值作为最终的预测值。</p><p>当k = 1时，k近邻算法就是最近邻算法。k值一般<font color="#FF1493">采用交叉验证法选取最优值</font>。</p><ol start="3"><li>决策规划</li></ol><p>通常，在分类任务中使用投票法计算最终预测结果，在回归任务中使用平均法，还可基于距离远近进行加权平均或加权投票。</p><h3 id="KNN算法描述"><a href="#KNN算法描述" class="headerlink" title="KNN算法描述"></a>KNN算法描述</h3><p>下面以<font color="#008B8B">分类</font>任务为例，介绍KNN算法，回归任务与此类似，区别不大。</p><p>输入：训练数据集<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>D</mi><mo>=</mo><mo>{</mo><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)</mo><msubsup><mo>}</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></msubsup></math>    ，其中，<img src="http://files.shanqianche.cn/202175/1625492364666.png"> 是实例的类别。<br>过程：</p><ul><li>根据给定的距离度量，在训练集D中找出与x最邻近的k个点，涵盖着k 个点的领域记为<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></math>；</li><li>在<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>N</mi><mi>k</mi></msub><mo>(</mo><mi>x</mi><mo>)</mo></math>中根据分类决策规则决定x的类别y： <img src="http://files.shanqianche.cn/202175/1625492543638.png" alt="所属类别"><br>输出：测试样本x xx所属的类别y yy。</li></ul><h2 id="KNN算法实现"><a href="#KNN算法实现" class="headerlink" title="KNN算法实现"></a>KNN算法实现</h2><h3 id="KNN算法蛮力实现"><a href="#KNN算法蛮力实现" class="headerlink" title="KNN算法蛮力实现"></a>KNN算法蛮力实现</h3><p> 首先我们看看最想当然的方式。</p><pre><code> 既然我们要找到k个最近的邻居来做预测，那么我们只需要计算预测样本和所有训练集中的样本的距离，然后计算出最小的k个距离即可，接着多数表决，很容易做出预测。这个方法的确简单直接，在样本量少，样本特征少的时候有效。但是在实际运用中很多时候用不上，为什么呢？因为我们经常碰到样本的特征数有上千以上，样本量有几十万以上，如果我们这要去预测少量的测试集样本，算法的时间效率很成问题。因此，这个方法我们一般称之为蛮力实现。&lt;font color=&quot;#1E90FF&quot;&gt;比较适合于少量样本的简单模型的时候用&lt;/font&gt;。</code></pre><h3 id="KD树实现原理"><a href="#KD树实现原理" class="headerlink" title="KD树实现原理"></a>KD树实现原理</h3><pre><code>KD树算法没有一开始就尝试对测试样本分类，而是先对训练集建模，建立的模型就是KD树，建好了模型再对测试集做预测。所谓的KD树就是K个特征维度的树，注意这里的K和KNN中的K的意思不同。KNN中的K代表特征输出类别，KD树中的K代表样本特征的维数。为了防止混淆，后面我们称特征维数为n。</code></pre><p>KD树算法包括三步，第一步是建树，第二部是搜索最近邻，最后一步是预测。</p><h4 id="KD树的建立"><a href="#KD树的建立" class="headerlink" title="KD树的建立"></a>KD树的建立</h4><p>我们首先来看建树的方法。KD树建树采用的是从m个样本的n维特征中，分别计算n个特征的取值的方差，用<font color="#DC143C">方差最大</font>的第k维特征<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>k</mi></msub></math>来作为<font color="#B22222">根节点</font>。对于这个特征，我们选择特征<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>k</mi></msub></math>的取值的中位数<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>kv</mi></msub></math>对应的样本作为划分点，对于所有第k维特征的取值小于<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>kv</mi></msub></math>的样本，我们划入左子树，对于第k维特征的取值大于等于<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>n</mi><mi>kv</mi></msub></math>的样本，我们划入右子树，对于左子树和右子树，我们采用和刚才同样的办法来找方差最大的特征来做<font color="#B22222">根节点，递归</font>的生成KD树。</p><p><img src="http://files.shanqianche.cn/202175/1625494718364.png" alt="构建KD树"></p><p>比如我们有二维样本6个，{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，构建kd树的具体步骤为：</p><ol><li>找到划分的特征。6个数据点在x，y维度上的数据方差分别为6.97，5.37，所以在x轴上方差更大，用第1维特征建树。</li><li>确定划分点（7,2）。根据x维上的值将数据排序，6个数据的中值(所谓中值，即中间大小的值)为7，所以划分点的数据是（7,2）。这样，该节点的分割超平面就是通过（7,2）并垂直于：划分点维度的直线x=7；（很显然，中位数为6 ，这里选择（5,4）或者(7,2)都是可以的。这种情况任选一个即可）</li><li>确定左子空间和右子空间。 分割超平面x=7将整个空间分为两部分：x&lt;=7的部分为左子空间，包含3个节点={(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节点={(9,6)，(8,1)}。</li><li>用同样的办法划分左子树的节点{(2,3),(5,4),(4,7)}和右子树的节点{(9,6)，(8,1)}。最终得到KD树。</li><li>后续步骤反复上面的，<font color="#8FBC8F">直到两个子区域没有实例存在时停止（这意味着最后所有训练实例都对应一个叶结点或内部结点），从而形成kd树的区域划分</font>。</li></ol><p><img src="http://files.shanqianche.cn/202175/1625495422262.png" alt="KD树"></p><p><font color="#DC143C">标准kNN算法的切分特征选择是按顺序的，后来对kd树的一个重大改进是选择方差最大的特征，方差越大，不同实例点区分越明显，更方便进行划分。</font></p><h4 id="KD树搜索最近邻"><a href="#KD树搜索最近邻" class="headerlink" title="KD树搜索最近邻"></a>KD树搜索最近邻</h4><p>当我们生成KD树以后，就可以去预测测试集里面的样本目标点了。对于一个目标点，我们<font color="#9932CC">首先在KD树里面找到包含目标点的叶子节点</font>。<font color="#0000FF">以目标点为圆心，以目标点到叶子节点样本实例的距离为半径，得到一个超球体</font>，<font color="#B22222">最近邻的点一定在这个超球体内部</font>。然后返回叶子节点的父节点，检查另一个子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点寻找是否有更加近的近邻,有的话就更新最近邻。如果不相交那就简单了，我们直接返回父节点的父节点，在另一个子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。</p><p><img src="http://files.shanqianche.cn/202175/1625496230689.png" alt="目标点为（2，4.5）"></p><p>从上面的描述可以看出，KD树划分后可以大大减少无效的最近邻搜索，很多<font color="#8A2BE2">样本点由于所在的超矩形体和超球体不相交，根本不需要计算距离。大大节省了计算时间。</font></p><p>先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间查找到（4,7），形成搜索路径&lt;(7,2)，(5,4)，(4,7)&gt;，但 （4,7）与目标查找点的距离为3.202，而（5,4）与查找点之间的距离为3.041，所以（5,4）为查询点的最近点； 以（2，4.5）为圆心，以3.041为半径作圆，如下图所示。可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间进行查找，也就是将（2,3）节点加入搜索路径中得&lt;(7,2)，(2,3)&gt;；于是接着搜索至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5；回溯查找至（5,4），直到最后回溯到根结点（7,2）的时候，以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如下图所示。至此，搜索路径回溯完，返回最近邻点（2,3），最近距离1.5。</p><h3 id="球树实现原理"><a href="#球树实现原理" class="headerlink" title="球树实现原理"></a>球树实现原理</h3><p>KD树算法虽然提高了KNN搜索的效率，但是在某些时候效率并不高，比如当处理不均匀分布的数据集时,不管是近似方形，还是矩形，甚至正方形，都不是最好的使用形状，因为他们都有角。一个例子如下图：<br><img src="http://files.shanqianche.cn/202175/1625496491462.png" alt="enter description here"></p><p>　　如果黑色的实例点离目标点星点再远一点，那么虚线圆会如红线所示那样扩大，导致与左上方矩形的右下角相交，既然相 交了，那么就要检查这个左上方矩形，而实际上，最近的点离星点的距离很近，检查左上方矩形区域已是多余。于此我们看见，KD树把二维平面划分成一个一个矩形，但矩形区域的角却是个难以处理的问题。</p><p>　　为了优化超矩形体导致的搜索效率的问题，有人引入了球树，这种结构可以优化上面的这种问题。</p><p><strong><font color="#7FFF00">球树的建立</font></strong></p><p><img src="http://files.shanqianche.cn/202175/1625496601030.png" alt="球树"></p><ol><li>先构建一个超球体，这个超球体是可以包含所有样本的最小球体。</li><li>从球中选择一个离球的中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个上，然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径。这样我们得到了两个子超球体，和KD树里面的左右子树对应。（PS:<font color="#B22222">这里选择两个点后，就以这两个点来聚类，所以先确定的是以这两个点为中心来计算其他点到该中心的距离。当所有点都确定自己的中心后，再重新计算一次该超球体的半径和球心</font>。）</li><li>对于这两个子超球体，递归执行步骤2，最终得到了一个球树。</li></ol><p>　　可以看出KD树和球树类似，主要区别在于球树得到的是节点样本组成的最小超球体，而KD得到的是节点样本组成的超矩形体，这个超球体要与对应的KD树的超矩形体小，这样在做最近邻搜索的时候，可以避免一些无谓的搜索。</p><h2 id="KNN优缺点"><a href="#KNN优缺点" class="headerlink" title="KNN优缺点"></a>KNN优缺点</h2><p>优点：</p><ol><li>结构简单；</li><li>无数据输入假定，准确度高，对异常点不敏感。</li><li> 训练时间复杂度比支持向量机之类的算法低，仅为O(n)</li><li> 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说，KNN方法较其他方法更为适合</li></ol><p>缺点：</p><ol><li>计算复杂度高、空间复杂度高；</li><li>样本不平衡时，对稀有类别预测准确度低；</li><li>使用懒惰学习，预测速度慢。</li><li>KD树，球树之类的模型建立需要大量的内存</li><li>相比决策树模型，KNN模型可解释性不强<h2 id="什么情况下选择KNN"><a href="#什么情况下选择KNN" class="headerlink" title="什么情况下选择KNN"></a>什么情况下选择KNN</h2></li></ol><p><img src="http://files.shanqianche.cn/202177/1625667539256.png" alt="choose"></p><h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><p><a href="https://www.cnblogs.com/ybjourney/p/4702562.html">机器学习（一）——K-近邻（KNN）算法 - Yabea - 博客园</a></p><h2 id="序列KNN"><a href="#序列KNN" class="headerlink" title="序列KNN"></a>序列KNN</h2><p><a href="https://antkillerfarm.github.io/ml/2017/10/19/Machine_Learning_28.html">机器学习（二十八）——KNN, AutoML, 数据不平衡问题</a></p><p><a href="https://blog.csdn.net/qq_41196612/article/details/107265167">R语言实战——基于KNN聚类的时间序列分析预测_三只佩奇不结义的博客-CSDN博客_r语言knn回归及预测</a></p><p><a href="https://www.coder.work/article/383913">python - 如何使用 KNN/K-means 对数据帧中的时间序列进行聚类 - IT工具网</a></p><p><a href="https://github.com/iwuqing/Time-Series-Classification-based-on-KNN">iwuqing/Time-Series-Classification-based-on-KNN: 基于KNN聚类算法结合Dynamic Time Warping（动态时间调整）的时间序列分类</a></p><p><a href="https://github.com/vvanggeng/TSC-KNN">vvanggeng/TSC-KNN: 基于KNN和DTW的时间序列分类</a></p>]]></content>
    
    
    <summary type="html">【可選】文章描述</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
    <category term="machine leaning" scheme="https://merlynr.github.io/tags/machine-leaning/"/>
    
  </entry>
  
  <entry>
    <title>用于业务流程事件和结果预测的混合模型</title>
    <link href="https://merlynr.github.io/2021/07/05/%E7%94%A8%E4%BA%8E%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E4%BA%8B%E4%BB%B6%E5%92%8C%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B%E7%9A%84%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/"/>
    <id>https://merlynr.github.io/2021/07/05/%E7%94%A8%E4%BA%8E%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E4%BA%8B%E4%BB%B6%E5%92%8C%E7%BB%93%E6%9E%9C%E9%A2%84%E6%B5%8B%E7%9A%84%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-07-04T16:00:00.000Z</published>
    <updated>2021-07-14T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h3><p>对于多样性流程进行异常预测</p><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><ol><li>序列k近邻法（KNN）</li><li>基于序列比对的马尔科夫模型扩展法</li></ol><p><font color="#1E90FF">思路：</font><br>利用数据的时间分类特征，利用高阶马尔可夫模型预测过程的下一步，并利用序列对比技术预测过程的结果。通过考虑基于k个最近邻的相似过程序列的子集，增加了数据的多样性方面。</p><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>已经证明，通过一组实验，序列k最近邻提法比原始提供更好的结果;我们的扩展马尔可夫模型优于随机猜测、马尔可夫模型和隐马尔可夫模型。</p><blockquote><p><font color="#2e4e7e">知识补充</font><br><a href="https://blog.zuishuailcq.xyz/2021/07/05/KNN%EF%BC%88K%E8%BF%91%E9%82%BB%E6%B3%95%20K%20Nearest%20Neighbors%EF%BC%89/">KNN（K近邻法 K Nearest Neighbors） | 吾辈之人，自当自强不息！</a><br><a href="https://blog.zuishuailcq.xyz/2021/07/05/Markov%20Model%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%89/">Markov Model（马尔可夫模型） | 吾辈之人，自当自强不息！</a></p></blockquote><h2 id="阐述"><a href="#阐述" class="headerlink" title="阐述"></a>阐述</h2><p>在进行流程预测的前，我们需要从日志中挖掘流程。通过分析数据，可以得知数据为带有<strong>时间序列</strong>的数据。1999年已经有人证明MMs适用于研究用户网上浏览行为。同时事件序列也可用于训练已经<strong>编码后续事件之间的转换概率</strong>的马尔可夫模型，<br>类似其它机器学习模型，越是高阶的模型越是拟合数据，预测结果也更加准确。</p><blockquote><p><font color="#FF1493">知识补充</font><br>在马尔可夫链的每一步，系统根据概率分布，可以从一个状态变到另一个状态,也可以保持当前状态。状态的改变叫做过渡，与不同的状态改变相关的概率叫做<strong>过渡概率</strong><br><font color="#006400">TODO </font>默认预测的提出没有仔细研究<br>当给定数据集很少时，导致无法机器学习和准确预测，所以为了解决这个问题，提出了几个为了解决特征和默认事件之间的非线性依赖关系的模型，通过补充特征默认值来补充数据，进行预测。<strong>默认预测</strong></p></blockquote><p>但是当数据多样化同时不聚集的时候，会导致高阶模型弱覆盖，对于未被覆盖的序列，就需要默认预测。但是默认预测会降低模型的准确度。<br><em>在马尔可夫模型中</em>，为了平衡覆盖与准确性，一般解决思想是合并多个不同阶的MMs的转换状态，然后在预测的时候遵循“冗余”状态。例如可选择马尔可夫模型（selective Markov model）</p><blockquote><p>TODO<br>遵循“冗余”状态的实际意义是什么？它的实际表现是什么<br>an extension of All Kth order Markov models (Deshpande &amp; Karypis, 2004)</p></blockquote><blockquote><p><font color="#2F4F4F">知识补充</font><br>个人理解<strong>弱覆盖</strong>是由于特征广而弱，无法高效学习，导致有一些特征被丢弃没有学习到。</p></blockquote><p>第二部部分讲述的是采用kNN算法来预测过程结果，即通过比对给定领域内的序列，找到最相似的序列。06年有研究者发表“预测使用电信公司的数据流失”，用欧几里德距离来计算给定序列与样本之间的距离。</p><blockquote><p>TODO 生物顺序结合没懂</p></blockquote><p>在本文中作者将KNN与生物学的序列对齐组合形成顺序kNN。</p><blockquote><p><font color="#bf242a">知识补充</font><br><strong>编辑距离</strong>是针对二个字符串（例如英文字）的差异程度的量化量测，量测方式是看至少需要多少次的处理才能将一个字符串变成另一个字符串。</p></blockquote><h3 id="creation"><a href="#creation" class="headerlink" title="creation"></a>creation</h3><blockquote><p>TODO 不理解下面的匹配机制</p></blockquote><p>首先是为了解决高阶MMs弱覆盖，提出了MMs和序列对准融合的技术。当预测模型无法找到预测实例对应的序列时，<strong>应用匹配过程</strong>以便从与给定序列中最相似的转换矩阵中提取那些序列（图案）。</p><p>其次是提出一种预测结果的序列kNN方法，即通过比对序列局部结果，然后比对相似程度，获得预测结果</p><h2 id="序列比对"><a href="#序列比对" class="headerlink" title="序列比对"></a>序列比对</h2><p><font color="#FF1493">为了确定序列相似性</font><br>序列比对主要类似于生物学中的新的DNA序列与DNA数据库进行比较。将DNA序列的事务与蛋白质数据库进行比较，以验证它们之间的关系是否可能发生在发生概率之间。</p><blockquote><p>序列比对<br>包括全局比对和局部比对<br>全局比对是提供了全局优化解决方案，遍历所有查询序列的整个长度。<br>局部比对旨在从两个查询序列中找到最相似的段</p></blockquote><p>我们在MMs中并不是使用当前的序列比对算法，而是使用序列比对的思想；同时作者提出<strong>局部序列比对与kNNs的结合</strong>使用方法：【替换矩阵】</p><blockquote><p><font color="#006400">知识补充</font><br>在序列比对算法中的<strong>替换矩阵</strong>又称为打分矩阵，其数学本质是统计权重。<br><a href="https://zhuanlan.zhihu.com/p/150582377">替换矩阵（计分矩阵）| 原理和作用 - 知乎</a><br>在序列比对中，我们一般需要给出一个定量的数值来描述两者的一致性和相似性。在此过程中，替换矩阵用来评价碱基或残基之间的相似性，在长期实践中，人们发现一些特定的碱基替换或者残基替换的频率是要高于另一些替换的，因此人们可以通过统计方法或者基于进化的突变模型来给每一种替换定义不同的分值，来体现出不同碱基或残基之间发生替换的可能性。其可以分成核酸序列替换矩阵和蛋白质序列替换矩阵。</p></blockquote><p> <strong><font color="#9932CC">替换矩阵：</font></strong><br> <em>生物学中用于描述单位时间内，一个氨基酸转换为另一个氨基酸的速率</em>。本文中替换矩阵作为单位矩阵，主对角线的元素是1，其他元素都是0。为了呈现突变，使用了更复杂形式的替代矩阵。上边<font color="#1E90FF">知识补充</font>中提到不同的矩阵不同值可以更好表现序列单元之间转换的可能性与频率。<font color="#FF00FF">我的理解是序列对比的权重表</font>。</p><p> <strong><font color="#9932CC">打分矩阵</font></strong><br> 即模型打分矩阵函数：</p><p> <img src="http://files.shanqianche.cn/2021710/1625901529848.png" alt="初始化"></p><p> <img src="http://files.shanqianche.cn/2021710/1625901755301.png" alt="计算公式"><br><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>y</mi><mi>j</mi></msub><mo>)</mo></math>是替换矩阵中Xi与Yj的替换分数。<br><font color="#483D8B">TODO</font> 公式中有个别参数读不懂</p><p><img src="http://files.shanqianche.cn/2021710/1625902300497.png" alt="栗子"></p><p><strong><font color="#FF8C00">案例：</font></strong><br>序列ABCDE与序列EBCAD对应的打分矩阵，根据分数高低来寻找最佳片段，然后沿着对角线从最高点到左上角，直至分数为0，来确定匹配序列，如图栗子中为BC。</p><h2 id="预测模型"><a href="#预测模型" class="headerlink" title="预测模型"></a>预测模型</h2><p><strong><font color="#9932CC">前提：</font></strong><br>本模型目的是在流程实例中预测<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>s</mi><mrow><mi>i</mi></mrow><mrow><mo>(</mo><mi>N</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup></math>下一事件<math xmlns="http://www.w3.org/1998/Math/MathML"><msubsup><mi>s</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>(</mo><mi>N</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msubsup></math>。其中<math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>S</mi><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow></msub></math>业务流程中的事件的预测是基于<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>S</mi><mo>=</mo><mo>{</mo><msub><mi>S</mi><mn>1</mn></msub><mo>,</mo><msub><mi>S</mi><mn>2</mn></msub><mo>&#x22EF;</mo><msub><mi>S</mi><mi>N</mi></msub><mo>}</mo></math></p><p>其中一个业务流程实例（Sj）是按时间顺序排列的离散事件(或任务)的组合Sj=<img src="http://files.shanqianche.cn/2021710/1625906179169.png" alt="enter description here">，而单个事件是来自于事件类型的有限集合<math xmlns="http://www.w3.org/1998/Math/MathML"><mi>E</mi><mo>=</mo><mo>{</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><mo>&#x22EF;</mo><mo>,</mo><msub><mi>e</mi><mi>L</mi></msub><mo>}</mo></math></p><p><img src="http://files.shanqianche.cn/2021710/1625907751234.png" alt="predict model"></p><h3 id="MMs"><a href="#MMs" class="headerlink" title="MMs"></a>MMs</h3><p><a href="https://blog.zuishuailcq.xyz/2021/07/05/Markov%20Model%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%89/">Markov Model（马尔可夫模型） | 吾辈之人，自当自强不息！</a></p><p>同时文中为了保障预测模型准确性，提出构建 <strong><font color="#006400">动态MMs</font></strong> ，通过存储单个事件在数据集中的次数、紧跟事件发生的下一个事件在数据集中的次数（事件A的下一个事件为B，这里指B的次数）和转换矩阵。还可以通过将折扣因子与事件数量结合，这样就可以在加入新数据时更新折扣因子来提供更多权重。</p><blockquote><p><strong><font color="#9400D3">知识补充</font></strong> 折扣因子<br><a href="https://www.zhihu.com/question/61389929">(95 封私信 / 79 条消息) 马尔可夫决策过程中为什么需要discount factor ，也就是问为啥时间近的状态影响越大？ - 知乎</a><br><a href="https://www.jianshu.com/p/678f57342d0b">#David Silver Reinforcement Learning # 笔记2-MDP - 简书</a><br><a href="https://www.cnblogs.com/wacc/p/5391209.html">强化学习笔记1 - Hiroki - 博客园</a></p></blockquote><h3 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h3><p><a href="https://blog.zuishuailcq.xyz/2021/07/05/Markov%20Model%EF%BC%88%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%EF%BC%89/">Markov Model（马尔可夫模型） | 吾辈之人，自当自强不息！</a></p><h3 id="混合模型"><a href="#混合模型" class="headerlink" title="混合模型"></a>混合模型</h3><blockquote><p>主要是平衡准确性和覆盖</p></blockquote><p>结合高阶MMs与序列比对技术（Waterman 1994），以保持高阶马尔可夫模型的高精度，同时弥补缺乏覆盖面积【个人感觉是通过通过增加区别序列对比消除过拟合】，提出了MSA（Markov sequence alignment 马尔可夫序列比对）【一种基于相似序列可能产生相同结果的假设，<font color="#E9967A">指处理没有遇到过的新的序列，然后在转换矩阵中找最类似的</font>】</p><p>通过构建矩阵比较两个序列的重量，最小则最为接近。<br>规则：<br>节点事件相同，则重量为0<br>如果事件不同则为1或者 $δ$</p><p><img src="http://files.shanqianche.cn/2021713/1626182437859.png" alt="栗子"></p><ol><li>从两个序列中删除的第一事件A和E导致重量为1</li><li>两个序列中的第二个事件是B并且具有重量0</li><li>第四个事件A从EBCAD中删除，以便将两个D事件匹配在两个序列中的第四和第五位置，重量加 $δ$</li><li>最后，我们从ABCDE序列中删除最后一个事件E，增加重量 $δ$</li><li>匹配两个序列的总重量是 $w = 1 + 0 + 0 + δ + δ = 1 + 2δ$</li></ol><p><font color="#8B0000">以下示例将说明我们的方法</font>，通过一个三阶MMs的转换矩阵和序列BCC。此序列尚未发生在矩阵之前并未存储在矩阵中。<font color="#FF8C00">目的是从矩阵找到最相似的序列，并使用它们的预测来为给定序列BCC生成预测。</font></p><p><img src="http://files.shanqianche.cn/2021713/1626183826410.png" alt="替换矩阵"></p><ol><li>我们首先将BCC与ABC相匹配</li></ol><p><img src="http://files.shanqianche.cn/2021713/1626183855711.png" alt="BCC to ABC"></p><p>weight： $w=δ+0+δ=2δ$</p><ol start="2"><li>类似地，匹配CBC和BCC的得到的重量</li></ol><p>weight: $w=2δ$</p><ol start="3"><li>BAA和BCC</li></ol><p>weight: $w=2$</p><p>这一块理解的是当无法通过删除操作达到序列匹配的时候则加重为1，否则通过删除和插入惩罚达 $δ$ 到序列匹配，文中提到的是0.4。<br>通过比重，ABC与CBC一样小，所以<font color="#9400D3">ABC与CBC都和BCC最相似</font>，但是它们的预测载体分别是（0.1,0.2,0.7）和（0.1,0.5,0.4），基于这些向量，就是预测{B，C}的下一步。<br>这两个事件的发生的权重和频率是相等的;因此，由于两个序列中较高的转换概率分别是 $m _ { 13 }=0.7$ 和  $m _ { 22 }=0.5$ ，因此给定的序列预测的下一步是具有更高的转移概率C。</p><p><img src="http://files.shanqianche.cn/2021713/1626185939137.png" alt="算法一"></p><ol><li>比对序列，识别通过删除/插入符号匹配两个序列的最佳编辑过程</li></ol><p><img src="http://files.shanqianche.cn/2021713/1626185957405.png" alt="算法二"><br>2. 计算匹配两个序列的分数</p><p><font color="#D2691E">算法1和2说明了通过删除和插入惩罚来匹配两个序列的过程</font></p><h3 id="KNN结合序列比对"><a href="#KNN结合序列比对" class="headerlink" title="KNN结合序列比对"></a>KNN结合序列比对</h3><p>KNN基本上是一种非参数方法;因此，其中一个优点是不需要训练模型。序列KNN的核心思想是**<font color="#FF1493">找到类似的序列</font>** ，期望这些序列具有共同的行为和结果。<br>业务流程的比对需要加入时间特征，这里采用生物学的序列对准技术结合KNN，通过与序列对准组合，kNN允许我们顺序地比较符号序列，提出了K最近序列比对（KnsSA）。</p><p>首先根据给的N个序列，构建了距离矩阵 $( d _ { i j } ) N * N$ ，然后使用距离矩阵的元素对序列进行排序。通过使用局部对齐匹配每对序列来获得距离矩阵的元素</p><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><h3 id="过程分析和数据预处理"><a href="#过程分析和数据预处理" class="headerlink" title="过程分析和数据预处理"></a>过程分析和数据预处理</h3><pre><code>确定了MSA（Markov sequence alignment 马尔可夫序列比对）和KnsSA（KNN序列比对）</code></pre><blockquote><p><font color="#00CED1">非对应知识补充</font>：MSA<br><a href="https://blog.csdn.net/weixin_39569389/article/details/111647915">序列两两比对算法_多重序列比对(MSA)分析工具怎么选，看这一篇就够了_weixin_39569389的博客-CSDN博客</a><br><a href="https://www.jianshu.com/p/31fb919f1c91">【陪你学·生信】九、多序列比对-Multiple Sequence Alignment（MSA） - 简书</a></p></blockquote><blockquote><p><font color="#B8860B">非对应知识补充</font>：MSA<br><a href="https://zhuanlan.zhihu.com/p/92254686">替代梯度下降——基于极大值原理的深度学习训练算法 - 知乎</a><br>MSA可以替代梯度下降函数<br>优一：MSA的每次迭代的收敛速度确实比梯度下降方法快一些<br>优二：梯度下降法的一大问题就是如果参数初始化得不好，那么就有可能会遇到一些局部平坦的区域，导致收敛速度变慢，而MSA方法则不会受到这个问题的影响<br>缺点：每轮迭代的时间会比梯度下降慢得多，这是可以理解的，毕竟MSA的每轮迭代都需要去找到一个最大值，而梯度下降只需要计算一次梯度就行了。这就导致了<strong>虽然MSA每次迭代收敛地更快，但是从时间上来看却反而更慢了</strong><br><img src="http://files.shanqianche.cn/2021714/1626226968387.png" alt="总结"></p></blockquote><p>第一个数据集（DS1）由电信线路故障修复记录为9个月。第二（DS2）涵盖1个月的时间，并表示固定宽带断层的过程。第三数据集DS3来自不同的故障修复过程。数据集DS1和DS2用于我们扩展马尔可夫模型的实验中。数据集DS3和DS4用于KNSSA实验。</p><p>数据复杂且长短不一。【文中没有提到整理数据方式，只是进行可视化】</p><p><img src="http://files.shanqianche.cn/2021713/1626187757351.png" alt="通过工具Aperture挖掘10%的DS1的数据形成的可视化"></p><h3 id="扩展马尔可夫模型实验"><a href="#扩展马尔可夫模型实验" class="headerlink" title="扩展马尔可夫模型实验"></a>扩展马尔可夫模型实验</h3><blockquote><p>扩展马尔可夫模型实验</p></blockquote><ol><li>RM - 随机模型</li></ol><p>为了找到当前之后的下一个任务，我们随机从潜在的下一个任务中选择。例如，从历史数据得知事件a来自于序列A属于集合{C，D，E}的任务，我们随机选择从该设置的值作为预测的下一步。【不通，反正就是一个baseline，在有限的范围内进行下一步】</p><ol start="2"><li>高阶MMs</li></ol><p>从一阶到kth阶生成许多不同的秩马尔可夫模型。给定序列，我们从最高阶MMs开始。如果在转换矩阵中找不到给定序列，我们通过删除序列中的第一个事件来创建新的较短序列。然后，我们将使用下一个下阶MMs继续过程，直到我们在转换矩阵中找到一个匹配项，或者在尝试第一阶Markov模型之后，需要默认预测。【只通过降阶来预测 <strong><font color="#FF8C00">所有</font></strong> 序列】</p><ol start="3"><li>HMM</li></ol><p>我们测试了几个具有不同长度的多个序列HMM，用于输入序列和不同数量的隐藏状态，并选择了最佳状态。</p><p>**<font color="#9932CC">结果分析对比</font>**：<br>90%用于训练，10%做预测。使用十折交叉验证预测结果</p><blockquote><p><font color="#006400">知识补充</font>交叉验证<br><a href="https://zhuanlan.zhihu.com/p/267910629">8. Sklearn — 交叉验证(Cross-Validation) - 知乎</a></p></blockquote><p>首先是构建一阶到七阶MSAs，然后通过通过MSAs选取数据集的最适合 $l$ ,其中DS1取值为5，DS2取值3。</p><blockquote><p>$l$ 表示序列选取长度</p></blockquote><p>下图一，二别说明了DS1和DS2这两个数据集的MSAa准确性。结果表明，通过引入<font color="#FF00FF">默认预测</font>改进模块，MSA优于其他可比模型，尤其是当马尔可夫模型的阶数增加时。这是因为 <strong><font color="#D2691E">序列之间比较的相关性与其长度成正比。</font></strong></p><blockquote><p>数字表示阶层<br>蓝色表示原生MMs的预测准确度，红加蓝表示加上默认预测模块改进后的预测精确度。</p></blockquote><p><img src="http://files.shanqianche.cn/2021714/1626252630431.png" alt="图一，使用数据集DS1在MMs中应用默认预测模块前后的正确预测百分比"></p><p><img src="http://files.shanqianche.cn/2021714/1626253369803.png" alt="图二，使用数据集DS2在MMs中应用默认预测模块前后的正确预测百分比"></p><p><font color="#9932CC">分析：</font><br>二阶MSAs在一系列不同阶MSAs中表现最好，就DS1而言，正确预测27%；三阶MSAs在DS2中表现最好，可以达到70%的准确率。同时可以分析得到在五阶MSAs预测时，默认预测模块的作用明显增加。</p><p><img src="http://files.shanqianche.cn/2021715/1626314785957.png" alt="四种模型预测比较，第一个为全阶MMs"></p><p>这里选择的是五阶段MSA和全五阶MMs，因为全阶段MMs在高阶时是具有优势的，这里为了验证默认预测的作用所以使用五阶。该图显示了两个数据集下所有模型的性能。可以看出，RM表现最差，DS2的成功率只有10%左右。当应用于更大的数据集DS1时，结果下降到接近2%（MSAs是其14倍）。同时可以发现选择正确的下一个任务的概率随着集合大小的增加而降低。结果突出了处理复杂数据集的难度。当数据不是太多样化(即DS2)时，MSA(五阶)获得最高数量的正确预测，结果为63%。与其他基准相比，MSA结果优于达到57%的五阶马尔可夫模型、达到60%的全Kth (K=5)马尔可夫模型和达到43%的隐马尔可夫模型。</p><h3 id="连续KNN实验"><a href="#连续KNN实验" class="headerlink" title="连续KNN实验"></a>连续KNN实验</h3><h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><p>这块作者提出的预测流程结果，所以需要给数据做标记，用于区分输出结果是成功还是失败。但是不同数据集输出的结果判断标注不同，<font color="#FF8C00">同一个数据集中不同流程的结果判断标注也不同</font>，这就无法统一对预测结果进行判定。</p><p><font color="#9932CC">为了寻找到不同业务流程相统一的结果判断标准，这里采用的是通过设置时间阈值来衡量流程结果的成功与否。</font></p><pre><code>然而，在DS4的情况下，实际交付日期和向客户承诺的交付日期之间的差被用作确定成功和失败的标准。特别是，如果实际交付日期在约定日期之前，则该流程实例被归类为成功，否则被归类为失败。</code></pre><h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><blockquote><p><font color="#7FFF00">通过另外两种baseline来评估KnsSA</font></p></blockquote><ol><li>RM – random model(随机模型)：为了找到过程的结果，我们随机生成一个介于0和1之间的数字；如果生成的数字大于0.5，则结果为成功(1)，反之亦然；如果生成的数字小于0.5，则结果为失败(0)</li><li>Original KNN： 我们选择K个最近的序列是因为它们有共同的独特任务。例如，给定两个序列A、B、D和A、A、C，第一个序列中有一个A、一个B和一个D；第二个中有两个As和一个C。每个独特的任务可以被视为一个类别。每个类别的距离计算为相应任务出现次数的差值。为了获得任意两个给定序列之间的总距离，对所有类别进行求和。例如，前面给出的两个序列由四个类别A、B、C和D组成。类别A的距离分别为dA= 1，类别B、C和D的距离分别为dB=1、dC=1和dD= 1。这两个序列的总距离为d=dA+dB+dC+dD=4【每个流程中每个事件进行求总距离】</li></ol><p>该模型是局部序列比对技术与KNN的结合模型，所以选择KNN的k值，结合实验中标签是0和1，这里作者k值选择为<font color="#FF8C00">奇</font>数；同时考虑数据集的多样性，k值应该<font color="#FF8C00">较小</font>。</p><blockquote><p>**<font color="#B8860B">知识补充</font>**： k值为什么选择奇数<br><img src="http://files.shanqianche.cn/2021715/1626333046633.png" alt="k值为什么选择奇数"></p></blockquote><p><img src="http://files.shanqianche.cn/2021715/1626333229424.png" alt="Local KnsSA模型对DS3，DS4预测结果"></p><p><img src="http://files.shanqianche.cn/2021715/1626333425840.png" alt="Local KnsSA，KNN，RM对于数据集预测结果"></p><p><strong>结果表明</strong>，该模型优于原始KNN和随机猜测的基准模型。这也意味着数据的时间特性对于预测过程结果很重要。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文的设计构思，首先通过MMs和HMM生成数据集的替换矩阵，然后作者提出一种序列比对的马尔可夫扩展模型（MSAs）。为了验证该模型的有效性，与MMs和HMMs进行对比，发现其准确率由于其它至少10%。<br>同时表明了高阶马尔可夫模型预测准确率高于一阶【图五】。从五阶以后加入缺省默认预测模块后可以大幅度提高预测准确度。<br>第二个贡献是提出新的序列比对KNN（KnsSA）。该方法优于基准模型，证明了数据的序列特征在预测过程结果中的重要作用。相将相似序列进行分类，然后进行下一步处理。</p><h2 id="下一步构思"><a href="#下一步构思" class="headerlink" title="下一步构思"></a>下一步构思</h2><p>未来的研究将着眼于使用序列比对和K均值聚类将数据聚类成K个组，然后用合适的方法处理每个组</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;摘要&quot;&gt;&lt;a href=&quot;#摘要&quot; class=&quot;headerlink&quot; title=&quot;摘要&quot;&gt;&lt;/a&gt;摘要&lt;/h2&gt;&lt;h3 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h3&gt;&lt;p&gt;对于多</summary>
      
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="RPA" scheme="https://merlynr.github.io/tags/RPA/"/>
    
    <category term="machine learning" scheme="https://merlynr.github.io/tags/machine-learning/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
</feed>
