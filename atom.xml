<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>吾辈之人，自当自强不息！</title>
  
  <subtitle>博客</subtitle>
  <link href="https://merlynr.github.io/atom.xml" rel="self"/>
  
  <link href="https://merlynr.github.io/"/>
  <updated>2021-11-07T16:00:00.000Z</updated>
  <id>https://merlynr.github.io/</id>
  
  <author>
    <name>Merlynr</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2021-11-8【mission】</title>
    <link href="https://merlynr.github.io/2021/11/08/2021-11-8%E3%80%90mission%E3%80%91/"/>
    <id>https://merlynr.github.io/2021/11/08/2021-11-8%E3%80%90mission%E3%80%91/</id>
    <published>2021-11-07T16:00:00.000Z</published>
    <updated>2021-11-07T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h1><ol><li>20.50-11.20 <a href="https://rl.qiwihui.com/zh_CN/latest/partII/chapter9/on-policy_prediction_with_approximation.html">在策略预测近似方法</a></li><li>在策略控制近似方法</li><li>离策略近似方法</li><li>资格迹</li><li>策略梯度的方法</li><li>周报</li></ol>]]></content>
    
    
    <summary type="html">每天学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="daily" scheme="https://merlynr.github.io/tags/daily/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>2021-11-01【周总结】</title>
    <link href="https://merlynr.github.io/2021/11/01/2021-11-01%E3%80%90%E5%91%A8%E6%80%BB%E7%BB%93%E3%80%91/"/>
    <id>https://merlynr.github.io/2021/11/01/2021-11-01%E3%80%90%E5%91%A8%E6%80%BB%E7%BB%93%E3%80%91/</id>
    <published>2021-10-31T16:00:00.000Z</published>
    <updated>2021-10-31T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>目标<br>1.流程构建辅助决策<br>2.页面元素获取<br>3.运行过程中的自动修正和反馈</p><h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[1]周能. 复杂场景下基于深度增强学习的移动机器人控制方法研究[D].北京邮电大学,2019.<br><a href="https://segmentfault.com/a/1190000014128757">强化学习</a><br><a href="https://blog.csdn.net/weixin_41521681/article/details/108016951">强化学习算法分类</a><br>[1]凌兴宏,李杰,朱斐,刘全,伏玉琛.基于双重注意力机制的异步优势行动者评论家算法[J].计算机学报,2020,43(01):93-106.</p><h2 id="策略模块"><a href="#策略模块" class="headerlink" title="策略模块"></a>策略模块</h2><ol><li>无算法</li><li>组成：影响因子【暂定名称】，决策池、根据因子和决策池重新构建权重最大的可能【简单算法】</li><li>是否考虑不同错误对应不同修改方案</li><li>影响因子不仅是根据页面元素的可能性设置，同时需要考虑动作</li><li>错误的基本原因“自适应”，“版本更替”</li><li>增强模型，决策函数，评估函数，奖励函数和外界反馈</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021102/1633159142456.png" alt="DQN"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021102/1633159979319.png" alt="训练框架"></p><h3 id="idea"><a href="#idea" class="headerlink" title="idea"></a>idea</h3><ol><li>预测模块基于lstm进行微调</li><li>改进增强学习模型actor-critic，加入影响因子，提高决策效果</li><li>为了控制<h2 id="引用-1"><a href="#引用-1" class="headerlink" title="引用"></a>引用</h2>智能自动化技术在汽车安全辅助驾驶系统中的应用 </li></ol><h1 id="二辩准备"><a href="#二辩准备" class="headerlink" title="二辩准备"></a>二辩准备</h1><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul><li><input disabled="" type="checkbox"> <ol><li>ppt新增页面—上次答辩记录和修改</li></ol></li><li><input disabled="" type="checkbox"> <ol start="2"><li>将DOM元素获取放到最后一个研究方案</li></ol></li><li><input disabled="" type="checkbox"> <ol start="3"><li>重新写研究目标，研究内容，研究方案，算法是主要</li></ol></li></ul><h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><ol><li>情节性的任务需要重新额外的<a href="https://rl.qiwihui.com/zh_CN/latest/partI/chapter3/finite_markov_decision_process.html#id2">符号</a></li><li>算法函数<a href="https://rl.qiwihui.com/zh_CN/latest/partI/chapter3/finite_markov_decision_process.html#id13">分析</a></li><li>多个动作集合成一个，减少不必要的搜索</li><li>一个栗子 示例3.19：回收机器人的贝尔曼最优方程 使用（<a href="https://rl.qiwihui.com/zh_CN/latest/partI/chapter3/finite_markov_decision_process.html#id13">3.19</a>）这个解决方案依赖于至少三个假设，在实践中很少是这样的： （1）我们准确地知道环境的动态；（2）我们有足够的计算资源来完成解决方案的计算；（3）马尔可夫性。 </li><li> <a href="https://rl.qiwihui.com/zh_CN/latest/partI/chapter4/dynamic_programming.html#id7">使价值函数的策略贪婪通常会使更改的策略的价值函数不正确，使价值函数与策略一致通常会导致该策略不再贪婪。</a>  策略评估 指的是（通常）迭代计算一个给定策略的价值函数。策略提升 指的是给定一个策略的价值函数计算一个提升的策略。</li><li>  就像在动态规划（DP）的那章所做的，首先我们考虑预测的问题 （计算一个确定的随机策略 π 的价值 vπ 和 qπ ）， 然后是策略提升，以及最后，控制的问题和解决它的广义策略迭代方法。 从动态规划（DP）中得到的这些想法都被推广到蒙特卡洛方法中，不过在这种情况下（指蒙特卡洛），我们只有样本经验。</li><li>  行为策略&amp;目标策略的制定</li><li>  rollout策略生成模拟轨迹</li><li>  MCTS决策时规划取得的巨大成功深刻影响了人工智能，许多研究人员正在研究用于游戏和单一个体应用程序的<a href="https://rl.qiwihui.com/zh_CN/latest/partI/chapter8/planning_and_learning_with_tabular_methods.html#id12">基本程序的修改和扩展</a>。</li><li>  <a href="https://rl.qiwihui.com/zh_CN/latest/partI/chapter8/planning_and_learning_with_tabular_methods.html#id14">总结</a></li><li>  <a href="https://rl.qiwihui.com/zh_CN/latest/partII/index.html#id1">泛化</a></li></ol>]]></content>
    
    
    <summary type="html">周学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="weekly" scheme="https://merlynr.github.io/tags/weekly/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>开题准备</title>
    <link href="https://merlynr.github.io/2021/09/30/%E5%BC%80%E9%A2%98%E5%87%86%E5%A4%87/"/>
    <id>https://merlynr.github.io/2021/09/30/%E5%BC%80%E9%A2%98%E5%87%86%E5%A4%87/</id>
    <published>2021-09-29T16:00:00.000Z</published>
    <updated>2021-09-29T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>目标<br>1.流程构建辅助决策<br>2.页面元素获取<br>3.运行过程中的自动修正和反馈</p><h1 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h1><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>[1]周能. 复杂场景下基于深度增强学习的移动机器人控制方法研究[D].北京邮电大学,2019.<br><a href="https://segmentfault.com/a/1190000014128757">强化学习</a><br><a href="https://blog.csdn.net/weixin_41521681/article/details/108016951">强化学习算法分类</a><br>[1]凌兴宏,李杰,朱斐,刘全,伏玉琛.基于双重注意力机制的异步优势行动者评论家算法[J].计算机学报,2020,43(01):93-106.</p><h2 id="策略模块"><a href="#策略模块" class="headerlink" title="策略模块"></a>策略模块</h2><ol><li>无算法</li><li>组成：影响因子【暂定名称】，决策池、根据因子和决策池重新构建权重最大的可能【简单算法】</li><li>是否考虑不同错误对应不同修改方案</li><li>影响因子不仅是根据页面元素的可能性设置，同时需要考虑动作</li><li>错误的基本原因“自适应”，“版本更替”</li><li>增强模型，决策函数，评估函数，奖励函数和外界反馈</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021102/1633159142456.png" alt="DQN"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021102/1633159979319.png" alt="训练框架"></p><h3 id="idea"><a href="#idea" class="headerlink" title="idea"></a>idea</h3><ol><li>预测模块基于lstm进行微调</li><li>改进增强学习模型actor-critic，加入影响因子，提高决策效果</li><li>为了控制<h2 id="引用-1"><a href="#引用-1" class="headerlink" title="引用"></a>引用</h2>智能自动化技术在汽车安全辅助驾驶系统中的应用 </li></ol><h1 id="二辩准备"><a href="#二辩准备" class="headerlink" title="二辩准备"></a>二辩准备</h1><h2 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h2><ul><li><input disabled="" type="checkbox"> <ol><li>ppt新增页面—上次答辩记录和修改</li></ol></li><li><input disabled="" type="checkbox"> <ol start="2"><li>将DOM元素获取放到最后一个研究方案</li></ol></li><li><input disabled="" type="checkbox"> <ol start="3"><li>重新写研究目标，研究内容，研究方案，算法是主要</li></ol></li></ul><h2 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h2><ol><li>情节性的任务需要重新额外的<a href="https://rl.qiwihui.com/zh_CN/latest/partI/chapter3/finite_markov_decision_process.html#id2">符号</a></li><li>函数<a href="https://rl.qiwihui.com/zh_CN/latest/partI/chapter3/finite_markov_decision_process.html#id13">分析</a></li><li>多个动作集合成一个，减少不必要的搜索</li><li>一个栗子 示例3.19：回收机器人的贝尔曼最优方程 使用（<a href="https://rl.qiwihui.com/zh_CN/latest/partI/chapter3/finite_markov_decision_process.html#id13">3.19</a>）这个解决方案依赖于至少三个假设，在实践中很少是这样的： （1）我们准确地知道环境的动态；（2）我们有足够的计算资源来完成解决方案的计算；（3）马尔可夫性。 </li><li> <a href="https://rl.qiwihui.com/zh_CN/latest/partI/chapter4/dynamic_programming.html#id7">使价值函数的策略贪婪通常会使更改的策略的价值函数不正确，使价值函数与策略一致通常会导致该策略不再贪婪。</a>  策略评估 指的是（通常）迭代计算一个给定策略的价值函数。策略提升 指的是给定一个策略的价值函数计算一个提升的策略。</li><li>  就像在动态规划（DP）的那章所做的，首先我们考虑预测的问题 （计算一个确定的随机策略 π 的价值 vπ 和 qπ ）， 然后是策略提升，以及最后，控制的问题和解决它的广义策略迭代方法。 从动态规划（DP）中得到的这些想法都被推广到蒙特卡洛方法中，不过在这种情况下（指蒙特卡洛），我们只有样本经验。</li><li>  行为策略&amp;目标策略的制定</li></ol>]]></content>
    
    
    <summary type="html">每天</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
    <category term="daily/weekly" scheme="https://merlynr.github.io/tags/daily-weekly/"/>
    
  </entry>
  
  <entry>
    <title>2021国庆前最后一周</title>
    <link href="https://merlynr.github.io/2021/09/23/2021%E5%9B%BD%E5%BA%86%E5%89%8D%E6%9C%80%E5%90%8E%E4%B8%80%E5%91%A8/"/>
    <id>https://merlynr.github.io/2021/09/23/2021%E5%9B%BD%E5%BA%86%E5%89%8D%E6%9C%80%E5%90%8E%E4%B8%80%E5%91%A8/</id>
    <published>2021-09-22T16:00:00.000Z</published>
    <updated>2021-09-22T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li><input disabled="" type="checkbox"> <ol><li>完成ppt中背景部分</li></ol></li><li><input disabled="" type="checkbox"> 2.3. 完成ppt中研究现状</li><li><input disabled="" type="checkbox"> <ol start="4"><li>定义长序列</li></ol></li><li><input disabled="" type="checkbox"> <ol start="5"><li>学习计算机编程技术</li></ol></li><li><input disabled="" type="checkbox"> <ol start="6"><li>总结</li></ol></li></ul>]]></content>
    
    
    <summary type="html">周学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="weekly" scheme="https://merlynr.github.io/tags/weekly/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>2021-9-06【周总结】</title>
    <link href="https://merlynr.github.io/2021/09/06/2021-9-06%E3%80%90%E5%91%A8%E6%80%BB%E7%BB%93%E3%80%91/"/>
    <id>https://merlynr.github.io/2021/09/06/2021-9-06%E3%80%90%E5%91%A8%E6%80%BB%E7%BB%93%E3%80%91/</id>
    <published>2021-09-05T16:00:00.000Z</published>
    <updated>2021-09-05T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h1><ol><li><input checked="" disabled="" type="checkbox"> tensorflow2.0</li><li><input checked="" disabled="" type="checkbox"> 看LSTM源码，了解其对于数据的处理方式</li><li><input checked="" disabled="" type="checkbox"> 继续看源码</li></ol><ul><li><input checked="" disabled="" type="checkbox"> <ol start="7"><li>几篇论文创意点总结</li></ol></li><li><input disabled="" type="checkbox"> <ol start="10"><li>几篇论文创意点总结</li></ol></li><li><input checked="" disabled="" type="checkbox"> <ol start="11"><li>小论文摘要</li></ol></li><li><input checked="" disabled="" type="checkbox"> <ol start="7"><li>几篇论文创意点总结<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1></li></ol></li></ul><ul><li>学的是tensorflow中自动求导机制的函数GradientTape()，其中包函了一阶求导和二阶求导。</li><li>对于一个Tensor和Variable类型，如果你的网络结构使用的是Dense Layers构建的，里面的变量w, b类型就是Variable类型的变量， <font color="#0000FF">也就是不需要watch</font>；如果我们构建的x是一个constant也就是一个Tenosr类型的话，为了更好的跟踪梯度的相关信息，这里需要把它加进tape.watch里面去，这个例子就是。</li><li><font color="#228B22">tf.GradientTape里面默认只会跟踪tf.Variable()类型</font>。如果类型不是这个的话。这里为tf.tensor,tf.Variable是tf.tensor的一种特殊类型。因此简单的包装一下，在tensor类型外面包一个Variable类型。</li></ul><p> “Classifying process instances using recurrent neural networks,”–Springer, 2018, pp. 313–324.</p><ul><li>paper中作者的创意是首次使用LSTM和GRU流程进行分类并进行对比，GRU效果较好。【涉及到超长序列可能有用。作者思想是<font color="#A52A2A">将长序列中不频繁及数量少的序列视为一项活动</font>，可以进一步提高吞吐量时间且不会显著影响分类精度】</li></ul><p>“Learning Effective Neural Nets for Outcome Prediction from Partially Labelled Log Data”–2019 IEEE</p><ul><li><p>paper中作者的创新点是基于默认模型进行无监督学习，主要思路是通过微调来提高预测精准度。比较的是训练数据集大小与预测准确度之间的比例，<font color="#7FFF00">没有可以借鉴的</font>。</p><h1 id="TODO-1"><a href="#TODO-1" class="headerlink" title="TODO"></a>TODO</h1><p>构思开题，与自己研究方向出入较大。</p></li></ul>]]></content>
    
    
    <summary type="html">周学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="weekly" scheme="https://merlynr.github.io/tags/weekly/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>EVERYDAY---2021</title>
    <link href="https://merlynr.github.io/2021/08/30/EVERYDAY---2021/"/>
    <id>https://merlynr.github.io/2021/08/30/EVERYDAY---2021/</id>
    <published>2021-08-29T16:00:00.000Z</published>
    <updated>2021-08-29T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TODO-List"><a href="#TODO-List" class="headerlink" title="TODO List"></a>TODO List</h1><ul><li><input disabled="" type="checkbox"> 9:00~10:00 今日学习学习            <strong><font color="#D2691E">todo-1</font></strong></li><li><input disabled="" type="checkbox"> 10:00~10:20 上午学习总结</li><li><input disabled="" type="checkbox"> 10:20~10:50 breaking news【一三五】</li><li><input disabled="" type="checkbox"> 10:20~10:50 coding【二四六】</li><li><input disabled="" type="checkbox"> 10:50~11:20 coding【一三五】</li><li><input disabled="" type="checkbox"> 10:50~11:20 刘畅【二四六】</li></ul><hr><ul><li><input disabled="" type="checkbox"> 14:00~15:00 今日学习安排            <strong><font color="#ff7500">todo-2</font></strong></li><li><input disabled="" type="checkbox"> 15:00~16:00 今日学习安排            <strong><font color="#01274F">todo-3</font></strong></li><li><input disabled="" type="checkbox"> 16:00~16:40 今日学习安排            <strong><font color="#2e4e7e">todo-4</font></strong></li><li><input disabled="" type="checkbox"> 16:40~17:10 下午学习总结</li></ul><hr><ul><li><input disabled="" type="checkbox"> 18:30~20:00 无字幕版美剧</li><li><input disabled="" type="checkbox"> 20:00~21:30 晚上学习安排            <strong><font color="#8B008B">todo-5</font></strong></li><li><input disabled="" type="checkbox"> 21:30~22:10 晚上学习安排            <strong><font color="#7FFF00">todo-6</font></strong></li><li><input disabled="" type="checkbox"> 22:10~22:30 晚上学习总结</li></ul><p>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</p>]]></content>
    
    
    <summary type="html">每学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="daily" scheme="https://merlynr.github.io/tags/daily/"/>
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>2021-8-24【周总结】</title>
    <link href="https://merlynr.github.io/2021/08/24/2021-8-24%E3%80%90%E5%91%A8%E6%80%BB%E7%BB%93%E3%80%91/"/>
    <id>https://merlynr.github.io/2021/08/24/2021-8-24%E3%80%90%E5%91%A8%E6%80%BB%E7%BB%93%E3%80%91/</id>
    <published>2021-08-23T16:00:00.000Z</published>
    <updated>2021-08-24T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h1><ul><li><input checked="" disabled="" type="checkbox"> 学习吴恩达–机器学习视频</li><li><input checked="" disabled="" type="checkbox"> 搭建本地实验环境</li><li><input checked="" disabled="" type="checkbox"> 查看数据集，查看基本预处理方法</li><li><input disabled="" type="checkbox"> 小论文框架构建</li></ul><h1 id="Done"><a href="#Done" class="headerlink" title="Done"></a>Done</h1><ol><li>前段时间只是看完前半部分中吴老师讲的机器学习中基础算法理论，看完这些的时候基本上论文中涉及到机器学习专业词汇都能理解，就算有些涉及到了模型也可以通过查询很快就能理解。上周做实验的时候经常做不下去，就索性把吴老师后半部分的课程看完了，大概有40节，主要包函的是常见的模型，支持向量机，核函数，以及k-means，还有一部分是对于数据的预处理的方法。【看这个视频的原因一个是需要入门，另一个主要原因是当时听彭老师实验室一个研一学生的分享，他做了一次基本实验的演示分享，觉得很不错，对于模型的理解更加透彻，最后才听到他和彭老师对话说的，就是跟着视频学习，所以觉得自己也应该学一遍】</li><li>主要是为了学习tensorflow，本地搭建了python3.6+tensorflow1.5.0和python3.7+tensorflow2.1.1两个版本，由于tensorflow2是去年年初更新的，而且两个版本差异比较大，所以都进行了对于其CPU版本进行简单的尝试</li><li>适用Prom工具进行了简单流程挖掘，基本的流程可以挖掘出来了，下一步是对数据进行预处理，这一块估计有很大工作量</li><li>小论文框架已经开始构思了，现在在整理之前看到的paper，近两年文章发的比较少哇，所以看的论文一般都是2019年以前的。</li></ol><p><strong><font color="#1E90FF">目前的论文思维导图：</font></strong></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021825/1629874905860.png" alt="Introduce的材料"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021825/1629874977085.png" alt="部分数据集及预处理"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021825/1629875159925.png" alt="预处理"></p><p><font color="#5F9EA0">近几年都是前缀填充和连续编码为主，目前我只看到18年有一篇用到N-gram，还是适用自动编码器的文中用到了，其它LSTM这种热门框架都没用到，这个后续中我也会把序列编码的方法换成连续编码。</font></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021825/1629875199842.png" alt="后续待完善"></p><p>这几年都没看到有与语义结合的，就LSTM，GRU这种RNN网络结构，<font color="#CC391D">个人感觉</font>上下文关系在其中的体现不是很大，因为本身业务流程直接关系没有那么复杂，words bag也没有自然语言中实体处理中的那么大，下周抽空看一篇，记得之前看到过一篇。</p><h1 id="下周安排"><a href="#下周安排" class="headerlink" title="下周安排"></a>下周安排</h1><ul><li><input disabled="" type="checkbox"> 看一遍吴老师对于深度学习的讲解</li><li><input disabled="" type="checkbox"> “TAP: A Transformer based Activity Prediction Exploiting Temporal Relations in Collaborative Tasks” 2021 IEEE</li><li><input disabled="" type="checkbox"> 书写小论文的introduce</li><li><input disabled="" type="checkbox"> 继续学习tensorflow的使用</li></ul>]]></content>
    
    
    <summary type="html">每天/周学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="weekly" scheme="https://merlynr.github.io/tags/weekly/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>实验安排</title>
    <link href="https://merlynr.github.io/2021/08/20/%E5%AE%9E%E9%AA%8C%E5%AE%89%E6%8E%92/"/>
    <id>https://merlynr.github.io/2021/08/20/%E5%AE%9E%E9%AA%8C%E5%AE%89%E6%8E%92/</id>
    <published>2021-08-19T16:00:00.000Z</published>
    <updated>2021-08-19T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://gitee.com/merlynr/img-store/raw/master/2021820/1629431056745.png" alt="实验安排"></p>]]></content>
    
    
    <summary type="html">每天/周学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
    <category term="daily/weekly" scheme="https://merlynr.github.io/tags/daily-weekly/"/>
    
  </entry>
  
  <entry>
    <title>2021-8-15【周总结】</title>
    <link href="https://merlynr.github.io/2021/08/15/2021-8-17%E3%80%90%E5%91%A8%E6%80%BB%E7%BB%93%E3%80%91/"/>
    <id>https://merlynr.github.io/2021/08/15/2021-8-17%E3%80%90%E5%91%A8%E6%80%BB%E7%BB%93%E3%80%91/</id>
    <published>2021-08-14T16:00:00.000Z</published>
    <updated>2021-08-14T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="本周安排"><a href="#本周安排" class="headerlink" title="本周安排"></a>本周安排</h1><ul><li><input checked="" disabled="" type="checkbox"> 思考小论文的可研究点</li><li><input disabled="" type="checkbox"> 构建论文结构~</li><li><input disabled="" type="checkbox"> 查找相关算法</li><li><input checked="" disabled="" type="checkbox"> 搭建实验室论坛</li></ul><h1 id="完成情况"><a href="#完成情况" class="headerlink" title="完成情况"></a>完成情况</h1><h2 id="研究点"><a href="#研究点" class="headerlink" title="研究点"></a>研究点</h2><ol><li><p>在不损失正常长度流程的预测精度下，提高长度较长的预测精度</p><ul><li>方法一：该想法建立的基础是在RNN神经网络上，由于对于长序列的处理效率低，所以当时想的可以通过预处理，<strong>通过聚类或者其它方式划分长序列相似片段，再进行局部匹配</strong></li><li>方法二：同样基于RNN，预处理，为长度设置不同给的阈值，然后分别进行预测，但是这样会导致长度长的数据集效率低下，依旧是根本性。</li></ul></li><li><p>加入时间戳或者其它属性来提高预测精度</p><ul><li>主要正对的是attention，本身attention机制对于长序列有较好的预测效果，考虑通过加入时间戳或其它属性来提高预测精度，这块由于比较新，目前我看的论文中没有提到加入其它属性。</li><li>虽然attention在长序列的预测中效率较高，但是由于其机制的根本原因，他不对顺序进行记录，最近看的论文中提到的位置记录模块是通过将位置编码加入到嵌入向量中，考虑是否有其它方法来处理解决顺序预测。</li></ul></li></ol><p><strong>老师，如果对这块有其它好的建议，请给我说哈</strong></p><h2 id="构建论文使用技术框架"><a href="#构建论文使用技术框架" class="headerlink" title="构建论文使用技术框架"></a>构建论文使用技术框架</h2><ol><li>Input data</li></ol><p>这里涉及的主要是日志处理，去除无用信息，根据需求对日志进行分组。<i class="fas fa-tags"></i>这一步很重要，公用数据集给的数据集包函的属性较多也包含一些错误的事件日志，需要在预处理时去除掉，这样才能获得准确的比对权重。<br>2. Predictions type</p><p> 现在还是主要<strong>预测下一个活动</strong>，但是很多论文中会加入其它预测任务，其他预测任务作为辅助任务，可能有助于提高预测性能。<br>唯一的例外是剩余的时间预测问题，因为它可以作为一个直接的预测。</p><ol start="3"><li>Neural Network Type</li></ol><p>LSTM ，Bidirectional LSTM ，包括最近的Transformers都会考虑，尤其LSTM会作为baseline作为对比对象。</p><ol start="4"><li>Sequence encoding</li></ol><p>神经网络都是以固定大小的张量编码方式输入，但是流程序列明显长短不一，所以需要对于输入数据进行规范化。</p><pre><code> 最常用的编码是连续编码和前缀填充编码，因为它们非常通用，可以同时用于cnn和rnn。连续编码和前缀填充编码都是通过填0的方式，使不同长度的序列达到相同的长度。其中又比较了N-gram编码，类似于hash算法将不同长度的序列转化为相同长度的hash值，但是目前只用于自动编码器中，LSTM和Transformer还没有用过，不知道适不适合。TODO：检查N-gram的适用性。</code></pre><ol start="5"><li>Event encoding</li></ol><p>这里主要比较了one-hot和嵌入编码，考虑到日中中流程种类，事件种类较多，而且很多paper验证了嵌入编码是目前预测任务中表现最优的事件编码方式，所以选择了嵌入编码。<br>  6. 数据集</p><p>TODO<br>  7. 验证指标</p><p>TODO<br>  8. 实验</p><p>TODO</p><h2 id="相关算法"><a href="#相关算法" class="headerlink" title="相关算法"></a>相关算法</h2><p>刚开始准备查序列比对的算法，但是目前深度学习中几乎不会用到这种算法，一般只有机器学习中才会用到，当时看的是一个KMP算法；后面考虑到是预处理时，对于长序列进行分段，或者将长序列相似部分进行标记，不会学习预测重复序列，那么这里涉及的是序列查找相似片段相关的算法。所以下一步准备找这些算法。</p><p>如果老师有了解或者看见过相关的算法，请给我推荐一下，我现在也是盲看，这块在预测中没人提出来，所以没有相关算法的介绍，我不知道他们没有提出这种思路是因为这种思路对于精确度的提升没有效果还是有其它原因，但是我看到有的paper中确实对长序列精确度进行了比较，所以感觉可以研究。</p><h2 id="搭建实验室论坛"><a href="#搭建实验室论坛" class="headerlink" title="搭建实验室论坛"></a>搭建实验室论坛</h2><p>局域网内依旧搭建完成了，只需要在公司后台进行nginx配发就可以在公共网络访问了，周一进行尝试后发现失败，然后他们运维在忙，就没弄域名了。</p><h1 id="下周安排"><a href="#下周安排" class="headerlink" title="下周安排"></a>下周安排</h1><ol><li>对于长序列预测的论文进行总结，比较他人的处理方式和优缺点</li><li>对数据集进行初步挖掘，查看长的日志是否与自己理解相似，即长的原因为重复事件较多</li><li>继续查看相关算法</li><li>了解N-gram是否适合于RNN模式</li></ol>]]></content>
    
    
    <summary type="html">周学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="weekly" scheme="https://merlynr.github.io/tags/weekly/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>2021-8-1【周总结】</title>
    <link href="https://merlynr.github.io/2021/08/03/2021-8-1%E3%80%90%E5%91%A8%E6%80%BB%E7%BB%93%E3%80%91/"/>
    <id>https://merlynr.github.io/2021/08/03/2021-8-1%E3%80%90%E5%91%A8%E6%80%BB%E7%BB%93%E3%80%91/</id>
    <published>2021-08-02T16:00:00.000Z</published>
    <updated>2021-08-02T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="本周安排"><a href="#本周安排" class="headerlink" title="本周安排"></a>本周安排</h1><ul><li><input checked="" disabled="" type="checkbox"> 分析理解LSTM框架</li><li><input checked="" disabled="" type="checkbox"> 阅读的相关论文</li><li><input disabled="" type="checkbox"> 构建环境，复现一个论文中的代码</li><li><input disabled="" type="checkbox"> 了解项目中神经网络的应用</li></ul><h1 id="完成情况"><a href="#完成情况" class="headerlink" title="完成情况"></a>完成情况</h1><h2 id="对于LSTM的框架进行了简单的理解"><a href="#对于LSTM的框架进行了简单的理解" class="headerlink" title="对于LSTM的框架进行了简单的理解"></a>对于LSTM的框架进行了简单的理解</h2><p><img src="./attachments/Long_Short_Term_Memory_Networks.pdf" alt="Long Short Term Memory Networks"></p><h2 id="精度《基于LSTM神经网络的业务过程预测》"><a href="#精度《基于LSTM神经网络的业务过程预测》" class="headerlink" title="精度《基于LSTM神经网络的业务过程预测》"></a>精度《基于LSTM神经网络的业务过程预测》</h2><p> 这一篇主要是对于事件和时间戳的预测 <img src="./attachments/%E5%9F%BA%E4%BA%8ELSTM%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%9A%E5%8A%A1%E8%BF%87%E7%A8%8B%E9%A2%84%E6%B5%8B%E7%9B%91%E6%8E%A7.pdf" alt="基于LSTM神经网络的业务过程预测监控"> 基本上理解了作者意图和操作，但是我没法理解LSTM层如何学习数据</p><h2 id="略读了几篇论文，试图理解LSTM的运行逻辑"><a href="#略读了几篇论文，试图理解LSTM的运行逻辑" class="headerlink" title="略读了几篇论文，试图理解LSTM的运行逻辑"></a>略读了几篇论文，试图理解LSTM的运行逻辑</h2><ol><li>使用LSTM对于事件类型较少的序列进行迭代循环预测，使用的是one-hot编码将事件类型，事件戳映射到向量特征向量，并使用事件发生的时间特征对其进行补充。<strong>缺点</strong>是当事件类型较多的时候，该方法效果会变差。</li></ol><ul><li>该模型由共享LSTM层构成，其中包括一个专门用于预测事件的LSTM和一个预测事件的LSTM</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021730/1627633758272.png" alt="2017-使用LSTM神经网络进行预测性业务流程监控"></p><ol start="2"><li>使用嵌入维度的LSTMs，可以减少输入长度和增加新的特征。<strong>缺点</strong>是依旧无法处理数值变量，所以也不能预测时间戳。<strong>优点</strong>是可以处理大量事件类型。</li></ol><ul><li>使用两层LSTMs隐藏层。</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021730/1627636620079.png" alt="2017-利用深度学习预测过程行为"></p><ol start="3"><li>使用多阶段深度学习的方法来预测下一个事件。<strong>缺点</strong>是无法处理数值变量，所以也不能预测时间戳。</li></ol><ul><li>首先是将每个事件映射到特征向量</li><li>下一步使用transformations降低输入维度，通常有，通过提取n-gram、使用hash、将输入通过两个自动编码层等方法</li><li>将转化后的输入传给负责预测的前馈神经网络</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/202182/1627887742568.png" alt="业务流程事件预测的多阶段深度学习方法"></p><p><strong>总结</strong><br>在阅读了几篇类似的paper后，对于LSTM在其中的作用理解加深，但是每一篇文章的重点或者内容侧重点更多是在数据处理上，而LSTM只是作为个较为优秀的神经网络结构。通过略看论文，感觉这可能就是发论文的方式哇。最后我在理解了LSTM网络层的基本原理后不在纠结于它在实际中的运行原理，也把它理解成一个模块了。</p><h2 id="复现精度论文的代码"><a href="#复现精度论文的代码" class="headerlink" title="复现精度论文的代码"></a>复现精度论文的代码</h2><p>失败了，可能是第一次复现深度学习的项目的原因吧，没有经验，每一步都有错误，整整三天晚上都在调试，错误是解决了不少，但是还是效率太低了，所以决定找个简单的入门，先使用一下LSTM再说。</p><p><strong>一个简单的情感分析</strong></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202183/1627997675435.png" alt="数据"><br>只是简单的使用LSTM所以用最简单矩阵数据</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/202183/1627997947288.png" alt="结果"><br>每个epoch为10，进行语义训练，这次训练数据极度拟合，所以结果没有参考价值，主要是熟悉LSTM的使用。</p><h1 id="下周安排"><a href="#下周安排" class="headerlink" title="下周安排"></a>下周安排</h1><ul><li><input disabled="" type="checkbox"> 再挑一篇新的顶刊论文进行精读</li><li><input disabled="" type="checkbox"> 略读几篇LSTM，Attention</li><li><input disabled="" type="checkbox"> 跑一个简单的Attention框架的代码，考虑一下最后研究是使用那个网络结构</li></ul>]]></content>
    
    
    <summary type="html">每天/周学习计划</summary>
    
    
    
    <category term="plan" scheme="https://merlynr.github.io/categories/plan/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="weekly" scheme="https://merlynr.github.io/tags/weekly/"/>
    
    <category term="plan" scheme="https://merlynr.github.io/tags/plan/"/>
    
  </entry>
  
  <entry>
    <title>业务流程的LSTM精准模型</title>
    <link href="https://merlynr.github.io/2021/07/29/%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E7%9A%84LSTM%E7%B2%BE%E5%87%86%E6%A8%A1%E5%9E%8B/"/>
    <id>https://merlynr.github.io/2021/07/29/%E4%B8%9A%E5%8A%A1%E6%B5%81%E7%A8%8B%E7%9A%84LSTM%E7%B2%BE%E5%87%86%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-07-28T16:00:00.000Z</published>
    <updated>2021-08-11T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>使用LSTM模型对事件下一步，时间戳和调用的资源进行预测。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>本文主要是对于前人提出在LSTM中利用近似前缀预测—<a href="https://blog.zuishuailcq.xyz/2021/07/19/%E5%9F%BA%E4%BA%8ELSTM%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%9A%E5%8A%A1%E8%BF%87%E7%A8%8B%E9%A2%84%E6%B5%8B%E7%9B%91%E6%8E%A7/">基于LSTM神经网络的业务过程预测监控 | 吾辈之人，自当自强不息！</a>的缺陷的改进，缺陷：</p><ol><li>无法处理数字变量</li><li>不能生成带有时间戳的时间序列</li><li>后续有文章提出通过one-hot编码来对事件进行分类，而不是使用嵌入维度来实现的，这样<strong>随着事件类型的增加，精度就会极度下降</strong>。</li></ol><blockquote><p>知识补充<br>后处理：在模型训练后，人为的修改模型结果使之预测结果更加符合真实情况。<br><a href="https://blog.csdn.net/xieyan0811/article/details/80549001">数据挖掘之_后处理_谢彦的技术博客-CSDN博客_数据后处理</a></p></blockquote><dl><dt><strong><font color="#006400">解决：</font></strong><br>本文通过提出用于建立新的预处理和后处理方法和架构以及使用LSTM神经网络的事件日志的生成模型来解决上述方法的局限性。</dt><dd>具体地说，本文提出了一种方法去学习模型，该方法可以生成由三组(事件类型、角色、时间戳)组成的轨迹(或从给定前缀开始的轨迹的后缀)。提出的方法结合了Tax等人[13]和Evermann等人[2]的优点，<font color="#483D8B">通过使用嵌入维度，同时支持事件日志中的分类属性和数字属性</font>。本文考虑了神经网络中共享层和特有层的不同组合所对应的三种体系结构。</dd></dl><p><strong><font color="#FF8C00">评估：</font></strong></p><ol><li>第一种方法比较了与不同体系结构、预处理和后处理选择相对应的所提出方法的备选实例。该评估的目的是<font color="#00008B">根据获取到的日志的特征</font>，得出关于哪些设计选择更可取的指南。</li><li>比较提出方法在上面三条约束的表现。</li></ol><h1 id="技术背景"><a href="#技术背景" class="headerlink" title="技术背景"></a>技术背景</h1><h2 id="LSTM-当前的进展"><a href="#LSTM-当前的进展" class="headerlink" title="LSTM 当前的进展"></a>LSTM 当前的进展</h2><ol><li>使用LSTM对于事件类型较少的序列进行迭代循环预测，使用的是one-hot编码将事件类型，事件戳映射到向量特征向量，并使用事件发生的时间特征对其进行补充。<strong>缺点</strong>是当事件类型较多的时候，该方法效果会变差。</li></ol><ul><li>该模型由共享LSTM层构成，其中包括一个专门用于预测事件的LSTM和一个预测事件的LSTM</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021730/1627633758272.png" alt="2017-使用LSTM神经网络进行预测性业务流程监控"></p><ol start="2"><li>使用嵌入维度的LSTMs，可以减少输入长度和增加新的特征。<strong>缺点</strong>是依旧无法处理数值变量，所以也不能预测时间戳。<strong>优点</strong>是可以处理大量事件类型。</li></ol><ul><li>使用两层LSTMs隐藏层。</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021730/1627636620079.png" alt="2017-利用深度学习预测过程行为"></p><ol start="3"><li>提出基于RNN的模型MM-Pred来预测下一步事件和流程后续。<strong>缺点</strong>是无法处理数值变量，所以也不能预测时间戳。</li></ol><ul><li>这种方法同时使用控制流信息（事件类型）和案例数据（事件属性）。</li><li>该结构由编码器、调制器和解码器组成。</li><li>编码器和解码器使用LSTM网络将每个事件的属性转换为隐藏表示或从隐藏表示转换为隐藏表示。</li><li>调制器组件求出可变长度序列比对权重向量，其中每个权重表示用于预测未来事件和属性的属性的相关性。</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/202182/1627873762436.png" alt="多属性事件序列的深度预测模型"></p><ol start="4"><li>使用多阶段深度学习的方法来预测下一个事件。<strong>缺点</strong>是无法处理数值变量，所以也不能预测时间戳。</li></ol><ul><li>首先是将每个事件映射到特征向量</li><li>下一步使用transformations降低输入维度，通常有，通过提取n-gram、使用hash、将输入通过两个自动编码层等方法</li><li>将转化后的输入传给负责预测的前馈神经网络</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/202182/1627887742568.png" alt="业务流程事件预测的多阶段深度学习方法"></p><ol start="5"><li>作者提出一种基于GRU的神经网络架构BINet，用于业务流程执行中的实时异常检测。该架构用于预测下一个事件及属性。</li></ol><ul><li>该方法旨在为跟踪中的每个事件分配一个似然分数，然后用于检测异常。这种方法表明，过程行为的生成模型也可用于异常检测。</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/202182/1627907436804.png" alt="基于深度学习的多元业务流程异常检测"></p><ol start="6"><li>作者比较几种真实数据集在MMs，all-k MMs以及基于自动机的模型中预测下一步的准确性和性能。</li></ol><ul><li>结果表明，AKOM模型具有最高的精度（在某些情况下优于RNN体系结构），而基于自动机的模型具有较高的可解释性。</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/202182/1627907971849.png" alt="下一个元素预测序列建模方法的跨学科比较"></p><h1 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h1><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628666548975.png" alt="构建模型的步骤"></p><h2 id="预处理阶段"><a href="#预处理阶段" class="headerlink" title="预处理阶段"></a>预处理阶段</h2><h3 id="Data-transformation"><a href="#Data-transformation" class="headerlink" title="Data transformation"></a>Data transformation</h3><p><strong>根据属性性质（分类或连续）进行特定预处理。</strong></p><p><strong><font color="#294B71">分类：</font></strong></p><p>处理数据：将事件和资源作为类别属性，使用嵌入维度。</p><blockquote><p>向训练网络提供属性之间关联的正面和负面示例，使网络能够识别和定位具有相似特征的近似属性。根据NLP社区4中使用的一项通用建议，嵌入维度的数量被确定为类别数量的第四根，以避免它们之间可能发生冲突。生成的值作为不可训练的参数导出并在所有实验中重用，这样就不会增加模型的复杂性</p></blockquote><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628683817654.png" alt="用于训练嵌入层的网络结构和生成的4d空间被缩减为3d的空间"></p><p> <strong><font color="#E9967A">连续：</font></strong></p><p><font color="#1E90FF"> 对数据进行归一化，以供预测模型解释</font>。这里处理的事件之间的相对时间，问题在于不同日志，相对时间可能具有很大的可变性。 这种高可变性可以隐藏有关过程行为的有用信息，例如时间瓶颈或异常行为，如果不小心执行属性缩放，则可以隐藏这些信息。</p><p> 寻求一种合适的缩放方法。</p><p> <img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628685428211.png" alt="最大值和对数归一化两种方法的对比"></p><h3 id="Sequences-creation"><a href="#Sequences-creation" class="headerlink" title="Sequences creation"></a>Sequences creation</h3><p> <strong>提取每个事件日志跟踪的固定大小的n-gram，以创建输入序列和预期事件来训练预测网络。</strong></p><p> <img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628686156534.png" alt="从BPI 2012事件日志的案例id 174770中提取的五个n-gram"></p><p> <em>role表示的是事件与资源的关联</em></p><h2 id="Model-Structure-Definition-Phase"><a href="#Model-Structure-Definition-Phase" class="headerlink" title="Model Structure Definition Phase"></a>Model Structure Definition Phase</h2><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628686308297.png" alt="Baseline architecture"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628687598994.png" alt="Tested architectures"></p><h2 id="Post-processing-Phase"><a href="#Post-processing-Phase" class="headerlink" title="Post-processing Phase"></a>Post-processing Phase</h2><p>从零前缀开始生成业务流程的完整跟踪中，传统使用的是arg max，直接根据下一个事件的最大概率来跟踪，但是这就会所有追踪的事件都倾向于概率最大值，对于低概率发生的事件无法追踪。这里作者使用的是arg max和随机选择的参数作为下一个事件的选择。</p><blockquote><p>我觉得应该使用softmax</p></blockquote><h1 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h1><blockquote><p>本节描述了两个实验评估。第一个实验比较了三种架构在前处理和后处理选择方面的不同实例。第二个实验将提出的方法与技术背景中其它论文中的下一个事件、后缀和剩余时间预测任务的三条基线进行比较</p></blockquote><h2 id="Data-Set"><a href="#Data-Set" class="headerlink" title="Data Set"></a>Data Set</h2><pre><code>在本实验中，使用了九个来自不同领域、具有不同特征的真实事件日志</code></pre><ul><li>Helpdesk5事件日志包含来自意大利软件公司helpdesk票务管理过程的记录</li><li>BPI 20126 中的两个事件日志与来自德国金融机构的贷款申请流程相关。 这个过程由三个子过程组成，我们从中使用了 W 子过程，以便与”下一个元素预测序列建模方法的跨学科比较”进行比较 。</li><li>BPI 20137中的事件日志与沃尔沃的IT事件和问题管理有关。我们使用完整的案例学习生成模型</li><li>BPI 20158中的五个事件日志包含五个荷兰城市在四年期间提供的建筑许可证申请数据。原始事件日志分为五个部分（每个市政局一个）。所有事件日志都在子流程级别指定，包括345多个活动。因此，按照”Diagnostics of building per-mit application process in dutch municipalities”中所述的步骤对其进行预处理，以便在阶段级别进行管理</li></ul><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628688257014.png" alt="Event logs description"></p><ol><li><em><font color="#A9A9A9">SF指根据其在记录道数量、事件、活动和序列长度方面的组成分为简单、中等和复杂。</font></em></li><li><em><font color="#A9A9A9">TV指据每个事件日志的平均持续时间和最大持续时间之间的关系，将时间变异性（TV）分为稳定或可变</font></em></li></ol><h2 id="Experimental-setup"><a href="#Experimental-setup" class="headerlink" title="Experimental setup"></a>Experimental setup</h2><p><strong><font color="#8A2BE2">目的：</font></strong><br>    使用 LSTM 模型从大小为0的前缀开始跟踪生成完整的事件日志，然后将生成的跟踪与原始日志中的流程进行比较。</p><p><font color="#1E90FF">方法：使用两个指标来评估生成的事件日志的相似性。</font></p><ol><li>Demerau-Levinstain（DL）算法根据一个字符串与另一个字符串相等所需的版本数测量序列之间的距离。该算法在每次执行插入、删除、替换和转置等操作时都进行惩罚。因此，我们使用其倒数来衡量生成的活动或角色序列与实际事件日志中观察到的序列之间的相似性。然后，较高的值意味着序列之间的相似性较高。</li><li>平均绝对误差（MAE）度量用于测量预测时间戳的误差。通过取观测值和预测值之间距离的绝对值，然后计算这些震级的平均值来计算该测量值。我们使用该度量来评估每对（生成轨迹、地面真值轨迹）的生成相对时间和观测时间之间的距离。</li></ol><blockquote><p>使用交叉验证，将事件日志分为两部分：70%用于培训，30%用于验证。第一个折叠被用作训练2000个模型的输入（每个事件日志大约220个模型）。<br>这平均220个模型配置了不同的预处理技术和体系结构。配置值是从972个组合的完整搜索空间中随机选择的。然后，使用每个经过培训的模型生成完整事件的新事件日志（参见第3节中描述的选择下一个活动的技术）。生成了每个配置的15个日志，并对其结果进行了平均。评估了32000多个生成的事件日志。</p></blockquote><h2 id="Results-and-Interpretation"><a href="#Results-and-Interpretation" class="headerlink" title="Results and Interpretation."></a>Results and Interpretation.</h2><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628689342024.png" alt="不同配置的事件日志中的相似性结果"></p><ul><li><font color="#A9A9A9">MAE列对应于预测记录道周期时间的平均绝对误差</font> </li></ul><p>结果表明，使用这种方法可以训练学习并可靠地再现原始日志的观察行为模式的模型。此外，研究结果表明，对于LSTM模型来说， <strong><font color="#A52A2A">学习词汇量较大</font></strong> 的序列比学习较长的序列更困难。要了解这些模式，需要更多的示例，如BPI2012和BPI2015的结果所示。这两个日志都有30多个活动，但在跟踪数量上有很大差异（见表2）。BPI2012的高度相似性还表明，使用嵌入维度处理大量事件类型可以改善结果，只要示例数量足以学习底层模式。</p><p>针对本实验中评估的模型结构构件，我们按照预处理、模型结构和超参数选择以及预测等阶段对其进行分析，以构建生成模型。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628689921162.png" alt="Preprocessing phase components comparison"></p><p><strong><font color="#FF8C00">这里主要比较的是对于相对时间缩放方式，和进行缩放的作用。</font></strong></p><ol><li>a 说明了如何使用最大值作为缩放技术，具有很小时间变化的日志呈现更好的结果。相比之下，具有不规则结构的日志使用对数归一化具有较低的<font color="#0000FF"> MAE</font>【横坐标】。</li><li>b 展示了使用不同大小的 n-gram 时的 DL 相似性结果，与事件日志的结构有关。 我们可以观察到，使用更长的n-grams对于trace更长的日志有更好的结果，呈现出稳定的增长趋势。 相比之下，中、简单结构的事件日志趋势不明显。 因此，应将长 n-gram 的使用保留给具有很长跟踪的日志。**<font color="#006400">这里体现的是n-gram size对于预测结果的作用，只有复杂的日志呈现良好的正相关。</font>**</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628690528922.png" alt="共享层的总体相似性"></p><p>关于模型结构定义阶段，图说明连接结构的总体相似度最低。相比之下，仅在分类属性之间共享信息的模型体系结构具有中等最佳性能。然而，它与专门的体系结构并不遥远，尽管它的分布范围更广。这意味着在不同性质的属性之间共享信息会在网络正在处理的模式中产生噪声，从而阻碍学习过程。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628690699173.png" alt="下一个事件选择方法的比较"></p><p>关于预测阶段，图显示了随机选择在所有事件日志中如何优于arg max。这种行为在具有较长和复杂跟踪的事件日志中更为明显。结果表明，无论事件日志结构如何，随机选择都是评估学习过程的可取方法。</p><h2 id="Comparison-Against-Baselines"><a href="#Comparison-Against-Baselines" class="headerlink" title="Comparison Against Baselines"></a>Comparison Against Baselines</h2><h3 id="Experimental-setup-1"><a href="#Experimental-setup-1" class="headerlink" title="Experimental setup"></a>Experimental setup</h3><p><strong><font color="#FF00FF">目的：</font></strong><br>    评估我们的方法在预测下一个事件、剩余事件序列（即后缀）和剩余时间（对于不同长度的跟踪前缀）方面的相对性能。</p><ol><li>next event prediction — 为每个模型提供长度增加的跟踪前缀，从 1 到每个跟踪的长度。 对于每个前缀，我们预测下一个事件并测量准确性（正确预测的百分比）。</li><li> suffix and remaining time prediction — 为模型提供了长度增加的前缀，直到案件结束。</li></ol><p><strong><font color="#8A2BE2">baselines:</font></strong></p><ul><li>next event and suffix prediction</li></ul><ol><li> Predictive business process monitoring with LSTM neural networks</li><li> Predicting process behaviour using deep learning</li><li> A deep predictive model for multi-attribute event sequence</li></ol><ul><li>remaining time prediction</li></ul><ol><li>Predictive business process monitoring with LSTM neural networks【Helpdesk, BPI2012W and BPI2012 event logs】</li></ol><h3 id="Results-and-Interpretation-1"><a href="#Results-and-Interpretation-1" class="headerlink" title="Results and Interpretation"></a>Results and Interpretation</h3><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628691394455.png" alt="下一个事件和后缀预测结果"></p><p><strong>这些结果表明，分类属性的维度控制所采用的措施，使我们的方法即使在长序列中也能获得始终如一的良好性能。</strong></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021811/1628691420458.png" alt="剩余循环时间MAE的结果（以天为单位）"></p><p>图10显示了剩余循环时间预测的MAE。尽管我们的技术目标不是预测剩余时间，但与Tax等人相比，它在这项任务中实现了类似的性能——在一个日志中略逊于它，在另一个日志中略逊于它的长前缀。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p><strong><font color="#FF8C00">优</font></strong></p><ol><li>评估表明，使用更长的n-gram可获得更高的精度</li><li>对数归一化是适用于高可变性测井的缩放方法，与总是选择最有可能的下一个事件相比，使用LSTM产生的概率随机选择下一个事件可导致更广泛的记录道和更高的精度。论文还表明，该方法在预测剩余事件序列及其从给定跟踪前缀开始的时间戳方面优于现有的基于LSTM的方法</li></ol><blockquote><p>作者预计，所提出的方法可以作为业务流程模拟的工具。实际上，从本质上讲，流程模拟器是一种通用模型，它生成由事件类型、资源和时间戳组成的跟踪集，并从中计算性能度量，如等待时间、循环时间和资源利用率。虽然流程模拟器依赖于可解释的流程模型（例如BPMN模型），但原则上可以使用能够生成事件跟踪的任何模型来模拟流程，其中每个事件都由事件类型（活动标签）、时间戳和资源组成。使用LSTM网络进行流程模拟的一个关键挑战是如何捕获“假设”场景（例如，删除任务或删除资源的效果）。</p></blockquote><p><strong><font color="#2F4F4F">future</font></strong><br>计划应用技术，使用”An eye into the future: Leveraging a-priori knowledge in predictive business process monitoring.”中的约束，从LSTM模型生成事件序列</p>]]></content>
    
    
    <summary type="html">Learning Accurate LSTM Models of Business Processes</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="process mining" scheme="https://merlynr.github.io/tags/process-mining/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
    <category term="LSTM" scheme="https://merlynr.github.io/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>little paper</title>
    <link href="https://merlynr.github.io/2021/07/21/little%20paper/"/>
    <id>https://merlynr.github.io/2021/07/21/little%20paper/</id>
    <published>2021-07-20T16:00:00.000Z</published>
    <updated>2021-07-20T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h1><p>流程挖掘作为企业</p><p>业务流程预测的基本思路：</p><ul><li>流程发现—从日中中推断流程模型</li><li>流程增强—学习流程日志加以改进</li><li>流程比较/预测—预测结果与流程日志进行比较，检查其一致性的程度。</li></ul><p>在预测业务流程中的下一个事件研究方法中，目前主要的模型和方法有RNN（循环神经网络）、MMs（马尔可夫模型）、语法归纳技术、LSTM（长短期记忆模型）等等，其中精度最高的是信任决策树。    </p>]]></content>
    
    
    <summary type="html">小论文</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Long Short Term Memory Networks</title>
    <link href="https://merlynr.github.io/2021/07/19/Long%20Short%20Term%20Memory%20Networks/"/>
    <id>https://merlynr.github.io/2021/07/19/Long%20Short%20Term%20Memory%20Networks/</id>
    <published>2021-07-18T16:00:00.000Z</published>
    <updated>2021-07-18T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1842025914&auto=1&height=66"></iframe><p><a href="https://zhuanlan.zhihu.com/p/32085405">人人都能看懂的LSTM - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/37644325">RNN、LSTM、GRU基础原理篇 - 知乎</a><br><a href="https://blog.csdn.net/matrix_space/article/details/53374040">机器学习：深入理解 LSTM 网络 (一)_Matrix-11-CSDN博客_lstm 机器学习</a></p><blockquote><p>中文分词、词性标注、命名实体识别、机器翻译、语音识别都属于序列挖掘的范畴。<font color="#DC143C">序列挖掘</font>的特点就是某一步的输出不仅依赖于这一步的输入，还依赖于其他步的输入或输出。在序列挖掘领域传统的机器学习方法有HMM（Hidden Markov Model，隐马尔可夫模型）和CRF（Conditional Random Field，条件随机场），近年来流行深度学习算法RNN（Recurrent Neural Networks，循环神经网络）。</p></blockquote><h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626698381786.png" alt="RNN网络架构图"></p><p>比如一个句子中有5个词，要给这5个词标注词性，那相应的RNN就是个5层的神经网络，每一层的输入是一个词，每一层的输出是这个词的词性。<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626698589144.png" alt="讲解"></p><h1 id="RNN的变体"><a href="#RNN的变体" class="headerlink" title="RNN的变体"></a>RNN的变体</h1><h2 id="双向RNN"><a href="#双向RNN" class="headerlink" title="双向RNN"></a>双向RNN</h2><p>双向RNN认为otot不仅依赖于序列之前的元素，也跟tt之后的元素有关，这在序列挖掘中也是很常见的事实。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626698749683.png" alt=" Bidirectional RNNs网络结构"></p><h2 id="深层双向RNN"><a href="#深层双向RNN" class="headerlink" title="深层双向RNN"></a>深层双向RNN</h2><p>在双向RNN的基础上，每一步由原来的一个隐藏层变成了多个隐藏层。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626698820535.png" alt="Deep Bidirectional RNNs网络结构"></p><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>前文提到，由于<font color="#9932CC"><strong>梯度消失/梯度爆炸</strong></font>的问题传统RNN在实际中很难处理长期依赖，而LSTM（Long Short Term Memory）则绕开了这些问题依然可以从语料中学习到长期依赖关系。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626699453120.png" alt="传统RNN每一步的隐藏单元只是执行一个简单的tanh或ReLU操作"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626699501199.png" alt=" LSTM每个循环的模块内又有4层结构:3个sigmoid层，1个tanh层"></p><p><strong><font color="#D2691E">解释LSTM模块：</font></strong></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626699615537.png" alt="图标说明"></p><ul><li>粉色的圆圈表示一个二目运算。</li><li>两个箭头汇合成一个箭头表示2个向量首尾相连拼接在一起。</li><li>一个箭头分叉成2个箭头表示一个数据被复制成2份，分发到不同的地方去。</li></ul><p> LSTM的关键是细胞状态C，一条水平线贯穿于图形的上方，这条线上只有些少量的线性操作，信息在上面流传很容易保持。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626699717681.png" alt="忘记层"></p><ol><li>忘记层，决定细胞状态中<strong>丢弃什么信息</strong>。把ht−1和xt拼接起来，传给一个sigmoid函数，该函数输出0到1之间的值，这个值乘到细胞状态Ct−1上去。<font color="#FF8C00">sigmoid函数的输出值直接决定了状态信息保留多少</font>。比如当我们要预测下一个词是什么时，细胞状态可能包含当前主语的性别，因此正确的代词可以被选择出来。当我们看到新的主语，我们希望忘记旧的主语。</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626700419154.png" alt="更新细胞状态"><br>2. 上一步的细胞状态Ct−1已经被忘记了一部分，接下来本步应该把哪些信息新加到细胞状态中呢？这里又包含2层：一个tanh层用来产生更新值的候选项C~t，tanh的输出在[-1,1]上，<strong>说明细胞状态在某些维度上需要加强，在某些维度上需要减弱</strong>；还有一个sigmoid层（输入门层），它的输出值要乘到tanh层的输出上，起到一个<strong>缩放</strong>的作用，<em>极端情况下sigmoid输出0说明相应维度上的细胞状态不需要更新</em>。在那个预测下一个词的例子中，我们希望增加新的主语的性别到细胞状态中，来替代旧的需要忘记的主语。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626700649763.png" alt="生成新的细胞状态"><br>3. 现在可以让旧的细胞状态Ct−1与ft（f是forget忘记门的意思）相乘来丢弃一部分信息，然后再加个需要更新的部分it∗C~t（i是input输入门的意思），这就生成了<strong>新的细胞状态Ct</strong></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626700735024.png" alt="循环模块的输出"><br>4. 最后该决定输出什么了。输出值跟细胞状态有关，把Ct输给一个tanh函数得到输出值的候选项。<strong>候选项中的哪些部分最终会被输出由一个sigmoid层来决定</strong>。在那个预测下一个词的例子中，如果细胞状态告诉我们当前代词是第三人称，那我们就可以预测下一词可能是一个第三人称的动词。</p><h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p>GRU（Gated Recurrent Unit）是LSTM最流行的一个变体，比LSTM模型要简单。没有了存储单元</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021719/1626700963311.png" alt="GRU"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>RNN 结构的一个吸引人之处在于其可以利用之前的输入信息。但是一个关键的需要解决的问题是当前的信息与之前的信息的关联度有长有短。<br>LSTM的内部结构。通过门控状态来控制传输状态，<strong>记住需要长时间记忆的，忘记不重要的信息</strong>；而不像普通的RNN那样只能够“呆萌”地仅有一种记忆叠加方式。对很多需要“长期记忆”的任务来说，尤其好用。、<br>但也因为引入了很多内容，导致参数变多，也使得训练难度加大了很多。因此很多时候我们往往会使用效果和LSTM相当但参数更少的GRU来构建大训练量的模型。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=1842</summary>
      
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="process mining" scheme="https://merlynr.github.io/tags/process-mining/"/>
    
  </entry>
  
  <entry>
    <title>基于LSTM神经网络的业务过程预测监控</title>
    <link href="https://merlynr.github.io/2021/07/19/%E5%9F%BA%E4%BA%8ELSTM%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%9A%E5%8A%A1%E8%BF%87%E7%A8%8B%E9%A2%84%E6%B5%8B%E7%9B%91%E6%8E%A7/"/>
    <id>https://merlynr.github.io/2021/07/19/%E5%9F%BA%E4%BA%8ELSTM%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%B8%9A%E5%8A%A1%E8%BF%87%E7%A8%8B%E9%A2%84%E6%B5%8B%E7%9B%91%E6%8E%A7/</id>
    <published>2021-07-18T16:00:00.000Z</published>
    <updated>2021-07-27T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=VAPwMDAm550&list=RDVAPwMDAm550&start_radio=1" title="我還年輕 我還年輕"><img src="https://res.cloudinary.com/marcomontalbano/image/upload/v1626681958/video_to_markdown/images/youtube--VAPwMDAm550-c05b58ac6eb4c4700831b2b3070cd403.jpg" alt="我還年輕 我還年輕"></a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>本文研究了长-短期记忆（LSTM）神经网络作为一种预测模型。并证明LSTMs在预测运行案例的下一个事件及其时间戳方面优于现有的技术。</p><h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>作者提到他的目的是提出一个可以适用的框架。本文的研究点：</p><ol><li>LSTMs能否被用于广泛的流程预测，以及如何应用？</li><li>如何保障LSTM在不同数据中的准确度始终如一？</li></ol><p>在不同预测内容中，作者适用了4个日志数据集进行验证比较。</p><h1 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h1><p>主要讲的是目前对于三种预测的技术，包括了，时间相关预测、事件结果的预测、正在执行事件的预测</p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><h2 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h2><ul><li>数据集A</li><li>数据集A中所有序列 $A^{*}$</li><li>一个长度为n的序列 σ =&lt;  $a_{1}$ ,  $a_{2}$ ,  $a_{3}$ , …… ,  $a_{n}$  &gt;,空序列为&lt;&gt;</li><li>$σ_ { 1 } \cdot σ _ { 2 }$ 表示序列 $σ_{1}$ 与 $σ_{2}$ 的串联</li><li> $h d ^ { k } ( o ) = ( a _ { 1 } , a _ { 2 } , \cdots , a _ { k } )$ 为<strong>前缀长度</strong>为k（0&lt;k&lt;n） 的序列 σ的<strong>前缀</strong>。 $t l ^ { k } ( o ) = ( a _ { k + 1 } \cdots , a _ { n } )$ 是它的后缀。</li></ul><p>对于前缀后缀的一个<strong>栗子</strong>：<br>序列： $σ _ { 1 } = ( a , b , c , d , e )$<br>前缀长度为二的前缀： $h d ^ { 2 } ( σ _ { 1 } ) = ( a , b )$<br>后缀为： $t l ^ { 2 } ( σ _ { 1 } ) = ( c , d , e )$</p><ul><li> $\varepsilon$ 【伊普西隆】为所有事件集合，T为时域。</li><li> $\pi _{ \tau }\in \varepsilon  \rightarrow T$ 为事件分配事件戳</li><li> $\pi _{ A }\in \varepsilon  \rightarrow A$ 从事件集A种为一个流程分配活动<h2 id="RNN和LSTM"><a href="#RNN和LSTM" class="headerlink" title="RNN和LSTM"></a>RNN和LSTM</h2></li></ul><p><a href="https://blog.zuishuailcq.xyz/2021/07/19/Long%20Short%20Term%20Memory%20Networks/">Long Short Term Memory Networks | 吾辈之人，自当自强不息！</a></p><h1 id="下一个活动和时间戳预测"><a href="#下一个活动和时间戳预测" class="headerlink" title="下一个活动和时间戳预测"></a>下一个活动和时间戳预测</h1><p>介绍评估多种体系结构预测下一个事件和时间戳。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021728/1627442124718.png" alt="事件预测算法"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021728/1627442195597.png" alt="事件预测算法"></p><p>输入一个事件的前缀，然后预测下一个事件。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><blockquote><p><strong><font color="#D2691E">知识补充</font></strong><br>one hot编码是将类别变量转换为机器学习算法易于利用的一种形式的过程。<br>假设“花”的特征可能的取值为daffodil（水仙）、lily（百合）、rose（玫瑰）。one hot编码将其转换为三个特征：is_daffodil、is_lily、is_rose，这些特征都是二进制的。<br><a href="https://zhuanlan.zhihu.com/p/37471802">什么是one hot编码？为什么要使用one hot编码？ - 知乎</a></p></blockquote><ol><li>首先为LSTM构建特征向量矩阵，作为输入。</li></ol><p>事件e=σ（i）的时间特征指的是在trace中的上一个时间和当前时间之间的时间。转换函数：<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021720/1626745464036.png" alt="enter description here"></p><p>三种时间特征：fvt1表示事件的当前时间特征【与上一个时间的时间间隔】，同时也添加了包函一天时间的特征fvt2和包含一周的时间特征fvt3.这样当事件在工作日或者工作周结束的时候预测下一个活动的事件中时间间隔则会更长。</p><ul><li>LSTM可以通过fvt1学到不同节点的事件宇时间差的依赖关系。</li><li>fvt2，fvt3的加入，是为处理有些有些事件超出了工作日的特殊情况，因为传统的日志处理中只记录工作日中的。</li></ul><ol start="2"><li>对时间步长k【第k个事件的时间】的输出 $o _ { a } ^ { k }$ 进行one-hot编码。</li></ol><p><font color="#6495ED">异常情况</font></p><ul><li>当在时间k为事件的结尾，既没有新的事件可以预测。 </li></ul><p><font color="#B22222"> <strong>解决</strong></font></p><ul><li>当在时间k结束时,给输出的one-hot编码向量增加额外标记值1 </li></ul><ol start="3"><li><p>设置第二个输出值 $o_{t}^{k}$ 为下一个时间间隔的 $fv_{t1}$ 值。当知道当前时间戳，就可以计算到下一个事件的时间戳。</p></li><li><p>使用Adam算法【梯度下降算法】进行神经网络权重优化</p></li></ol><p><strong><font color="#FF1493">如何优化：</font></strong></p><ul><li>最小化基础事件one-hot编码和被预测的下一个事件的one-hot编码的<font color="#0000FF">交叉熵</font>。</li><li>最小化事件和预测事件之间时间的<font color="#FF1493">平均误差</font>（MAE）</li></ul><h2 id="模型的构建"><a href="#模型的构建" class="headerlink" title="模型的构建"></a>模型的构建</h2><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021727/1627371341818.png" alt="单任务层神经网络结构"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021727/1627371437071.png" alt="共享多任务层神经网络结构"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021727/1627371484245.png" alt="n层共享，m层单任务混合神经网络结构"></p><ol><li>使用相同的数据特征，分别单独训练两个模型，一个是预测下一步事件，另一个是预测下一个时间戳，如图a</li><li>多任务学习可以在同一个神经网络结构学习到多个模型，例如图b，同一个LSTM神经网络结构学习输出两个模型。</li><li>混合模型</li></ol><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><ul><li>使用的循环神经网络依赖库Keras构建项目</li><li>硬件是NVidia Tesla k80 GPU，每次epoch时间为15-90s。其中预测时间时间戳是以毫秒为单位的。</li></ul><h2 id="实验准备"><a href="#实验准备" class="headerlink" title="实验准备"></a>实验准备</h2><h3 id="评定标准"><a href="#评定标准" class="headerlink" title="评定标准"></a>评定标准</h3><p>本文使用的<font color="#FF8C00">MAE</font>（平均绝对误差）来作为实验结果比较的参考。在实验效果评价这块，作者通过修改 van der Aalst提出的论文中用于预测剩余时间的模型来作为baseline。</p><h3 id="实验准备-1"><a href="#实验准备-1" class="headerlink" title="实验准备"></a>实验准备</h3><p>使用两个数据集进行预测下一个活动和时间戳。其中2/3的数据用于训练模型，1/3的用于预测。</p><p><i class="fas fa-tags" ></i>这里数据中长度为2的序列进行 $2 \leq k \lt | o |$ 预测，长度小于2则不对其预测。</p><p><strong><font color="#00008B">数据集</font></strong></p><ol><li>帮助中心数据集</li></ol><p>来自于意大利软件公司的票务管理系统，主要包括9中事件，一种事务流程。其中流程总共有3804条，事件有13710个。<br>2. BPI12子数据集W</p><p>此事件日志源自Business Process Intelligence Challenge（BPI’12）2，包含来自大型金融机构金融产品应用程序的数据。此流程由三个子流程组成：一个子流程跟踪应用程序的状态，一个子流程跟踪与应用程序关联的工作项的状态，第三个子流程跟踪报价的状态。在预测未来事件及其时间戳的上下文中，我们对自动执行的事件不感兴趣。因此，我们将评估范围缩小到工作项子流程，<font color="#FF8C00">其中包含手动执行的事件</font>。此外，我们过滤日志以<strong>仅保留complete类型的事件</strong>。</p><h2 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h2><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021727/1627377231055.png" alt="result，前缀为all表示所有前缀的平均值"><br>N表示神经元。MAE为当前配置不同前缀长度的性能。</p><p>表1显示了help desk和BPI’12w子流程日志上各种LSTM体系结构在MAE预测时间和预测下一事件准确性方面的性能。由于BPI12中的流程长度较长，所以前缀较长。</p><p>TODO 前缀不是很懂<br><font color="#7FFF00">分析：</font></p><ol><li>ALL LSTM体系结构在所有前缀上都优于baseline，同时分别比较LSTM模型和baseline模型，可以发现 <strong><font color="#FF00FF">短前缀的增益要比长前缀要好。</font></strong></li><li>数据集helpdesk的预测准确度最好为71%；BPI’12 W数据集预测的最佳精度为76%，高于Breuker等人报告的71.9%的精度和Evermann等人报告的62.3%的精度。</li><li>预测精度最高的模型都为混合模型，尝试将每层的神经元数量减少到75个，对于只有一个共享层的架构，将其增加到150个，但发现这会导致两个任务的性能下降。可能有75个神经元导致模型欠拟合，而150个神经元导致模型过拟合。我们还在单层架构上对传统RNN进行了实验，发现它们在时间和活动预测方面都比LSTM差得多。</li></ol><h1 id="后缀预测"><a href="#后缀预测" class="headerlink" title="后缀预测"></a>后缀预测</h1><blockquote><p><strong><font color="#ff7500">本章理解</font></strong><br>区别于上一节，上一章是单个时间步长预测下一步，而本章是预测一个运行案例的整个延续。</p></blockquote><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021728/1627454393652.png" alt="事件预测"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021728/1627454420942.png" alt="时间预测"></p><h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>通过迭代地预测下一个事件和时间戳，然后再次进行预测直至这个案例结束。这里用 $\perp$ 表示案例结尾。</p><p><strong><font color="#006400">迭代预测：</font></strong></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021728/1627455885003.png" alt="事件预测算法"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021728/1627456583245.png" alt="时间预测算法"></p><p><font color="#FF1493">当当前事件为END，则不进行预测；否则将预测的结果输入到预测模块迭代预测。</font></p><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><p>对于预测准确度的评价这里采用的是通过计算预测结果与实际结果的编辑距离来衡量。</p><blockquote><p>知识补充：<font color="#2e4e7e">Levenshtein distance</font><br><a href="https://www.cnblogs.com/ivanyb/archive/2011/11/25/2263356.html">字符串相似度算法（编辑距离算法 Levenshtein Distance） - ZYB - 博客园</a></p></blockquote><p><strong>问题：</strong><br>  当处理并发任务的时候，Levenshtein distance并不适用于计算。例如&lt;a,b&gt;为预测的下一个事件，但实际上为&lt;b,a&gt;，这种情况只是因为ab并发顺序导致的，<font color="#DC143C">实际上并无相关</font>。但是Levenshtein distance结果为2,因为将预测序列转换为实际序列需要一次删除和一次插入操作。</p><p><strong>解决</strong><br>Damerau-Levenstein距离是一种更好地反映预测质量的评估度量，它为Levenshtein距离使用的操作集添加了交换操作。Damerau-Levenshtein距离将分配1的转换成本⟨a、 b⟩ 进入⟨b、 a⟩. 为了获得可变长度记录道的可比较结果，我们通过实际案例后缀长度和预测后缀长度的最大值对Damerau-Levenshtein距离进行归一化，归一化的Damerau-Levenshtein距离减1以获得Damerau-Levenshtein相似性（DLS）。</p><p><strong>模型</strong><br>采用的是双层架构，每层100个神经元的LSTM。</p><p><strong>数据集</strong><br>这是荷兰某市政当局环境许可程序的日志每宗个案涉及一份许可证申请。<br>该日志包含937个案例和381种事件类型的38944个事件。几乎每一种情况都遵循一个独特的路径，这使得后缀预测更具挑战性。</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021728/1627479266512.png" alt="基于Damerau-Levenshtein相似性的后缀预测结果"><br>Polato为baseline，no duplicates表示去掉案例中的重复事件，但是保留一个【案例结尾即使为重复事件也不去掉】。</p><p>观察可以发现所有数据集中的LSTM预测结果都要好于baseline。在BPI12数据集种很多案例中的事件会重复的出现，这就会导致预测的后缀长于真实案例，因此删除了BPI12中重复的事件并重新进行评估。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ol><li>利用LSTM神经网络预测运行案例的下一个活动及其时间戳的技术。表明，这种技术在现实数据集上优于现有的基线。此外，我们发现通过单一模型（多任务学习）预测下一个活动及其时间戳比使用单独的模型进行预测具有更高的准确性。</li><li>提出了一个运行种的案例整个延续的预测和预测剩余的周期时间的解决方案。</li><li>发现了LSTM模型的局限性，**<font color="#9932CC">即在一个案例种某些事件多次重复出现时，导致后缀过长，性能就会很低</font>**。</li></ol>]]></content>
    
    
    <summary type="html">Predictive Business Process Monitoring with LSTM Neural Networks</summary>
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>损失函数之交叉熵(一般用于分类问题)</title>
    <link href="https://merlynr.github.io/2021/07/18/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5(%E4%B8%80%E8%88%AC%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)/"/>
    <id>https://merlynr.github.io/2021/07/18/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5(%E4%B8%80%E8%88%AC%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)/</id>
    <published>2021-07-17T16:00:00.000Z</published>
    <updated>2021-07-17T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.youtube.com/watch?v=i73Lh3h0DpQ" title="ONE HOUR"><img src="https://res.cloudinary.com/marcomontalbano/image/upload/v1626618085/video_to_markdown/images/youtube--i73Lh3h0DpQ-c05b58ac6eb4c4700831b2b3070cd403.jpg" alt="ONE HOUR"></a></p><p><a href="https://blog.csdn.net/u014453898/article/details/81559462">损失函数之交叉熵(一般用于分类问题)_ZJE-CSDN博客</a></p><h2 id="信息量、信息熵、相对熵"><a href="#信息量、信息熵、相对熵" class="headerlink" title="信息量、信息熵、相对熵"></a>信息量、信息熵、相对熵</h2><h3 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h3><p><font color="#FF8C00">一件事发生的概率越大，其蕴含的信息量就越少，反之，若发生的几率越小，则蕴含的信息量就越大</font>。</p><p>例如，“太阳从东方升起”：这件事发生概率极大，大家都习以为常，所以不觉得有什么不妥的地方，因此蕴含信息量很小。但“国足踢入世界杯”：这就蕴含的信息量很大了，因为这件事的发生概率很小。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626617713541.png" alt="信息量"></p><p>若某事x的发生概率为P(x)，则信息量的计算公式为：<br>$I ( x ) = - \log ( P ( x ) )$</p><p>上式的log的底为2，当然也可以是e、10。在神经网络中，log的底一般是e。当log的底大于1，log的图形就像下图红色线。因为P(x)的取值范围为0<del>1，可以看到，log的图像，在0</del>1的时候是负数，且P(x)越接近0，log越接近负无穷，P(x)越接近1，log越接近0，所以信息量的公式会在log前面加个负号，让log的取值范围为0~∞。当P(x)接近0，log接近无穷，P(x)接近1，log接近0，这符合信息量的概率越小，信息量越大的定义。</p><h3 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h3><p>信息熵可以表达<strong>数据的信息量大小</strong>。<br>信息熵也被称为熵，用来表示所有信息量的<strong>期望</strong>。<br>期望是试验中每次可能结果的概率乘以其结果的总和。<br>信息熵的公式如下：<br>$H ( X ) = - \sum _ { i = 1 } ^ { n } P ( x _ { i } ) \log ( P ( x _ { i } )  )$ $( X = x _ { 1 } , x _ { 2 } , x _ { 3 } , x _ { n - 1 } , x _ { n } )$</p><p>使用明天的天气概率来计算其信息熵：</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626618468307.png" alt="天气"><br>$H ( X ) = - ( 0.5 * \log ( 0.5 ) + 0.2 * \log ( 0.2 ) + 0.3 * \log ( 0.3 ) )$</p><h3 id="KL散度（相对熵）—–用于衡量两个概率分布的差异"><a href="#KL散度（相对熵）—–用于衡量两个概率分布的差异" class="headerlink" title="KL散度（相对熵）—–用于衡量两个概率分布的差异"></a>KL散度（相对熵）—–用于衡量两个概率分布的差异</h3><p><strong><font color="#8A2BE2">如何理解 “衡量两个概率分布的差异”？</font></strong></p><p>例如在机器学习中，常常用P(x)表示样本的真实分布，用Q(x)表示模型预测的分布，比如在一个三分类任务中（例如，猫狗马分类器），[x1，x2，x3]分别表示猫，狗，马的概率，输入一张猫的图片，其真实分布为P(x)=[1，0，0]，预测分布为Q(x)=[0.7，0.2，0，1]，那么P(x)和Q(x)就是两个不同的概率分布，可以用KL散度来计算他们的差异。<br>公式为：<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626618687852.png" alt="enter description here"><br>KL散度越小，表示P(x)和Q(x)越接近，所以可以通过反复训练，来使Q(x)逼近P(x)，但KL散度有个特点，就是不对称，就是用P来你和Q和用Q来你和P的KL散度(相对熵)是不一样的，但是P和Q的距离是不变的。</p><p><strong>那KL散度(相对熵)和交叉熵有什么联系呢</strong>？</p><p>我们通过对相对熵公式进行变形：<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626618747252.png" alt="enter description here"><br>H(X)为之前的信息熵，后面那一坨其实就是交叉熵了，所以可以看到：**<font color="#FF00FF">KL散度 = 交叉熵 - 信息熵</font>**</p><p>所以交叉熵的公式如下：<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626618800801.png" alt="enter description here"><br>从信息熵的公式，我们知道，对于同一个数据集，其信息熵是不变的，所以信息熵可以看作一个常数，因此<font color="#9400D3">当KL散度最小时，也即是当交叉熵最小时</font>。在多分类任务中，KL散度(相对熵)和交叉熵是等价的。</p><h2 id="交叉熵的原理"><a href="#交叉熵的原理" class="headerlink" title="交叉熵的原理"></a>交叉熵的原理</h2><p>交叉熵是<font color="#057748">用来衡量两个 概率分布 的距离</font>(也可以叫差别)。[概率分布：即[0.1，0.5，0.2，0.1，0.1]，每个类别的概率都在0~1，且加起来为1]。</p><p>若有两个概率分布p(x)和q(x)，通过q来表示p的交叉熵为：(<strong>注意</strong>，p和q呼唤位置后，交叉熵是不同的)<br>$H ( p , q ) = - \sum p ( x ) \log q ( x )$<br>只要把p作为正确结果(如[0，0，0，1，0，0])，把q作为预测结果(如[0.1，0.1，0.4，0.1，0.2，0.1])，就可以得到两个概率分布的交叉熵了，<strong>交叉熵值越低，表示两个概率分布越靠近</strong>。</p><p><strong>交叉熵计算实例：</strong><br>假设有一个三分类问题，某个样例的正确答案是(1，0，0)，某个模型经过softmax回归之后的预测答案是(0.5，0.4，0.1)，那么他们的交叉熵为：<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626619013263.png" alt="enter description here"><br>如果另一个模型的预测概率分布为(0.8，0.1，0.1)，则这个预测与真实的交叉熵为：<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626619026409.png" alt="enter description here"><br>由于0.1小于0.3，所以第二个预测结果要由于第一个。</p><h2 id="使用交叉熵的背景"><a href="#使用交叉熵的背景" class="headerlink" title="使用交叉熵的背景"></a>使用交叉熵的背景</h2><p>通过神经网络解决分类问题时，一般会设置k个输出点，k代表类别的个数，如下图<br><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626619104894.png" alt="enter description here"><br>每个输出结点，都会输出该结点对应类别的得分，如[cat，dog，car，pedestrian] 为[44，10，22，5]</p><p>但是输出结点输出的是得分，而不是概率分布，那么就没有办法用交叉熵来衡量预测结果和真确结果了，那怎么办呢，**<font color="#FF00FF">解决方法是在输出结果后接一层 softmax，softmax的作用就是把输出得分换算为概率分布</font>**。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=i73Lh3h0DpQ&quot; title=&quot;ONE HOUR&quot;&gt;&lt;img src=&quot;https://res.cloudinary.com/marcomontalbano/image/upload/</summary>
      
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="process mining" scheme="https://merlynr.github.io/tags/process-mining/"/>
    
  </entry>
  
  <entry>
    <title>Transformer （Attention Is All You Need）</title>
    <link href="https://merlynr.github.io/2021/07/17/Transformer%20%EF%BC%88Attention%20Is%20All%20You%20Need%EF%BC%89/"/>
    <id>https://merlynr.github.io/2021/07/17/Transformer%20%EF%BC%88Attention%20Is%20All%20You%20Need%EF%BC%89/</id>
    <published>2021-07-16T16:00:00.000Z</published>
    <updated>2021-07-16T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/48508221">详解Transformer （Attention Is All You Need） - 知乎</a></p>]]></content>
    
    
    <summary type="html">【可選】文章描述</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="process mining" scheme="https://merlynr.github.io/tags/process-mining/"/>
    
  </entry>
  
  <entry>
    <title>基于注意力机制的神经网络业务过程预测分析</title>
    <link href="https://merlynr.github.io/2021/07/17/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%9A%E5%8A%A1%E8%BF%87%E7%A8%8B%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/"/>
    <id>https://merlynr.github.io/2021/07/17/%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%9A%E5%8A%A1%E8%BF%87%E7%A8%8B%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/</id>
    <published>2021-07-16T16:00:00.000Z</published>
    <updated>2021-07-17T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1842025914&auto=1&height=66"></iframe><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>提出了一种具有注意力机制的神经网络，它是使用公开的事件日志（如BPI Challenge 2013）进行训练。<br>同时使用n-gram模型对比结果和LSTM（长-短期记忆结构的神经网络）对比训练时间。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>提到使用以前较小的数据进行与之前的研究进行对比，同时也使用到了较大过程的日志进行评估。<br>本文的亮点，作者首次提出结合基于自我关注的transformer模型【NLP中常用】进行流程预测。</p><blockquote><p>Transformer：<br><a href="https://zhuanlan.zhihu.com/p/48508221">详解Transformer （Attention Is All You Need） - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/171875438">李宏毅-Attention，Self-Attention，Transformer - 知乎</a><br>Attention is All You Need<br><a href="https://finance.sina.com.cn/tech/2021-01-26/doc-ikftpnny1935086.shtml">堪比当年的LSTM，Transformer引燃机器学习圈：它是万能的|LSTM|机器学习_新浪科技_新浪网</a></p></blockquote><blockquote><p>残差网络<br>残差网络是为了解决深度神经网络（DNN）隐藏层过多时的网络退化问题而提出。退化（degradation）问题是指：当网络隐藏层变多时，网络的准确度达到饱和然后急剧退化，而且这个退化不是由于过拟合引起的。<br><a href="https://www.jiqizhixin.com/graph/technologies/738e788b-0e3b-4a8f-bd04-e407c7137694">深度残差网络 | 机器之心</a></p></blockquote><h2 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h2><ol><li><p>Attention</p></li><li><p>self-attention</p></li></ol><p><a href="https://blog.csdn.net/At_a_lost/article/details/108469516">Attention机制与Self-Attention机制的区别_At_a_lost的博客-CSDN博客_attention和self attention的区别</a></p><ol start="2"><li><p>Transformer<br>Transformer模型并没有捕捉顺序序列的能力，也就是说无论句子的结构怎么打乱，Transformer都会得到类似的结果。换句话说，Transformer只是一个功能更强大的词袋模型而已。<br>为了解决这个问题，论文中在编码词向量时引入了位置编码（Position Embedding）的特征。—自己设计编码规则</p></li><li><p>前馈神经网络也经常称为多层感知器（Multi-Layer Perceptron，MLP）</p></li></ol><h3 id="事件日志"><a href="#事件日志" class="headerlink" title="事件日志"></a>事件日志</h3><p><strong><font color="#FF8C00">简述日志结构</font></strong></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021717/1626528110183.png" alt="流程结构--UML"></p><p>一个事件日志由多个案例组成，但一个案例总是分配给一个事件日志。案件与事件的关系也是如此；事件的典型属性是<strong>活动、持续时间、优先级或成本</strong>。</p><p>事件日志与事件案例：一对多<br>事件案例与事件：一对多</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p><strong>这块需要提前学习Transformer</strong></p><ol><li>通常，序列中的每个位置可以关注序列中的任何其他位置。作者提到为了不让Softmax函数计算时不考虑位置特征，将当前事件之后的位置的值设置为无穷大【忽略位置特征】</li></ol><ul><li>我的理解是位置特征无法通过分类来实现，这也是Transformer无法捕获序列顺序的原因</li></ul><ol start="2"><li>这里为了梯度的稳定，Transformer使用了score归一化，即除以 $\sqrt{d_{k}}$</li></ol><blockquote><p>**<font color="#7FFF00">知识补充</font>**：softmax<br><a href="https://zhuanlan.zhihu.com/p/105722023">一文详解Softmax函数 - 知乎</a><br>多分类、求大</p></blockquote><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021717/1626531018697.png" alt="Attention函数"></p><p>$W _ { i } ^ { Q } , W _ { i } ^ { K } , W _ { i } ^ { V } \in R^{d_{model}*d_{k}}$ ， $W^{O}\in R^{hd_{k}*d_{model}}$ 【 $d_{model}$ 表示嵌入的长度（可以理解为词嵌入）】</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021717/1626531384298.png" alt="self-attention函数"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021717/1626531047214.png" alt="相当于h 个不同的self-attention的集成，全连接层"></p><p>通过学习线性变换将Q向量、K向量和V向量投影到h个不同的子空间中，在每个子空间上<strong>并行</strong>计算Attention值。结果被连接并投射到特征空间，使得模型能够联合处理来自不同位置的不同表示子空间的特征。</p><h2 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h2><ol><li>2016年之前主要是使用MMs（生成模型）和聚类算法（KNN或k-means）相结合；剩下的就是一些类似MMs，例如，基于贝叶斯概念的概率有限自动机（概率模型），它使用期望的极大似然估计。 数据来源2012，2013公开的BPI比赛。</li><li>2016至今，几乎都是长短期记忆结构（LSTM）然后与其它模型相结合的方法来预测，比如结合词嵌入的神经网络，使用一个热编码转换事件特征，并将其与生成的时间特征连接到单个特征向量。数据来源2012，2013公开的BPI比赛</li><li>另一种方法将事件预测视为经典的多类分类问题，并使用堆叠的自动编码器提取特征，然后使用深度前馈网络对特征进行分类。然而，这种方法只适用于简单的数据集，因为不同表示的数量随着唯一事件类的数量呈多项式增长。</li></ol><blockquote><p><font color="#FF8C00">知识补充</font> 独热编码<br><a href="https://www.cnblogs.com/zongfa/p/9305657.html">数据预处理：独热编码（One-Hot Encoding）和 LabelEncoder标签编码 - 理想几岁 - 博客园</a></p></blockquote><blockquote><p><font color="#9400D3">知识补充</font> 多项式增长<br>也就是对于变量n，5n^2+2n+1这种就叫做多项式。</p></blockquote><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><strong>本实验数据集：</strong></p><ol><li>BPI Challenge 2013</li><li>一家德国软件公司提供的额外数据集。后一个数据集的事件日志包括律师、会计师和审计员使用一种特殊软件工具进行财务核算、管理付款交易和编制年度财务报表的情况。</li></ol><p>数据集由大约2.08亿个事件组成，由会话id标识，该会话id指示它们所属的情况、事件类型和时间戳。每个用户交互（通常是点击按钮）都被视为一个事件，一个案例从应用程序启动一直持续到关闭。</p><p>难点：<br>作者通过流程挖掘，许多独特的事件类增加了预测的难度。在<strong>较小的数据集中</strong>，大多数情况下都非常短。大约四分之一的病例由五个或更少的事件组成，只有百分之一的流程长度为500或更多。<br>同时数据集中大部分数据为重复的，数据类型不均匀，最常见的五个事件类型几乎占了<strong>整个数据集</strong>的一半。</p><h2 id="建模方法"><a href="#建模方法" class="headerlink" title="建模方法"></a>建模方法</h2><p>模型是在tensorflow上实现的，使用tf.data-API作为输入管道，tf.keras-API构建我们的模型。【不懂，反正tensorflow上的组件哇】</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><ol><li>整理事件类型，并用整数标记</li><li>将一个流程的所有事件放到一个张量（tensor，多维数组，能够创造更高维度的矩阵、向量。）</li><li>为每个流程添加一个结束标记，并且通过左移一个位置来生成训练标签【没懂为啥要左移生成标签，但是目的是为了生成训练标签，感觉是<strong>通过监控位置标签判断是否为训练数据集</strong>】</li><li>在上一部分中提到数据集长短不一，作者提出通过按照长度将流程分到不同区域中。这里为了确保每个区域中的流程相似，就没有设置固定的长度，而是通过制定上限来控制长度。</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021717/1626535517054.png" alt="上限控制"></p><p>$l_{1}&lt;9$ </p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021717/1626535685586.png" alt="a为模型，b为attention模块"></p><p>N表示Transformer的数量、 $d_{model}$ 表示嵌入长度、h表示Attention的数量， $d_{l}$ 表示Q，K，V的向量维度。<br>M表示词汇的数量，这里可以理解为事件的数量，n为单个流程的长度，bsz表示流程的数量，单次样本数量。<br>Pos.Encoding  位置编码标记</p><blockquote><p><strong><font color="#DC143C">知识补充</font></strong><br><a href="https://blog.csdn.net/program_developer/article/details/78597738">神经网络中Epoch、Iteration、Batchsize相关理解和说明_Microstrong-CSDN博客_epoch</a><br><strong>epoch</strong>：中文翻译为时期。<br>一个时期=所有训练样本的一个正向传递和一个反向传递。<br>举个例子，训练集有1000个样本，batchsize=10，那么：<br>训练完整个样本集需要：<br>100次iteration，1次epoch。</p></blockquote><blockquote><p><strong>理解辅助：</strong><br>这块主要和Transformer论文中的Shared-Weight Embeddings and Softmax这一部分一样<br>与其他序列转导模型类似，使用可学习的 Embeddings 将 input tokens and output tokens 转换为维度  的向量【序列转序列转为d（model）维度的向量】。通过线性变换和 softmax 函数将解码器的输出向量转换为预测的 token 概率。在 Transformer 模型中，两个嵌入层和 pre-softmax 线性变换之间共享相同的权重矩阵，在 Embeddings 层中，将权重乘以 . 这些都是当前主流的操作。</p></blockquote><ol><li>通过被一个正态分布初始权值的可训练查找矩阵构成的<strong>嵌入层</strong>将输入映射到一个 $d_{model}$ 维度的特征空间【①使用embedding将输入转化为 $d_{model}$ 维度的向量】，同时pre-softmax函数在transformer中使用相同的查找矩阵。</li><li>由于Transformer不对顺序有预测效果，所以将位置编码加入到嵌入向量中【与第一个方法类似结合sin，cos】</li><li>通过交叉熵和标签滑动来进行拟合结果和消除过拟合</li><li>最后作者提到注意计算结果在内部缓存并复用的问题，主要在担心内存溢出，但是实验缓冲所需的空间可忽略不计。</li></ol><blockquote><p><strong><font color="#0000FF">知识补充</font></strong><br>label smoothing(标签平滑)：正则化策略，为了防止过拟合，加入噪声<br><a href="https://zhuanlan.zhihu.com/p/116466239">label smoothing(标签平滑)学习笔记 - 知乎</a><br>交叉熵即预测值与真实值之间的差值，越少越精准。<br><a href="https://blog.zuishuailcq.xyz/2021/07/18/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B9%8B%E4%BA%A4%E5%8F%89%E7%86%B5(%E4%B8%80%E8%88%AC%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98)/">损失函数之交叉熵(一般用于分类问题) | 吾辈之人，自当自强不息！</a></p></blockquote><blockquote><p><strong><font color="#00CED1">知识补充：</font></strong> 模型理解<br><a href="https://zhuanlan.zhihu.com/p/60821628">碎碎念：Transformer的细枝末节 - 知乎</a><br><a href="https://zhuanlan.zhihu.com/p/106867810">Transformer理论源码细节详解 - 知乎</a><br><a href="https://www.codenong.com/cs106837783/">Transformer论文详解，论文完整翻译（七） | 码农家园</a><br><a href="https://congchan.github.io/NLP-attention-03-self-attention/">Transformer &amp; Self-Attention (多头) 自注意力编码 | Fly Me to the Moon</a></p></blockquote><h2 id="结果验证"><a href="#结果验证" class="headerlink" title="结果验证"></a>结果验证</h2><p>作者使用了前面提到的BPI2013和DATSET，每个数据集分为三个部分：培训，验证和测试。对于BPI2013的数据集，作者选的的训练、验证、测试集之间比例80%，10%，10%。DATSET则是96%，2%，2%。<br>BPI2013用了30000个训练模型，DATSET使用了一百万个，一个epoch为1000个。</p><ol><li>BPI2013</li></ol><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626620621539.png" alt="BPI2013结果"><br>展示了四种超参数的配置 $d_{model}$ 为嵌入长度，h为Attention数量， $d_{ff}$ 规定了点式前馈神经网络的内部第一层输出节点， 为前反馈网络中的参数。</p><p>分析，在最小超参数的配置中，当结合四层Transformer后精确度达到最高。同时随着配置的增加，训练的效果却下降了，这里可能是设置超参数过大，欠拟合。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626622123211.png" alt="近几年比较"><br>【4】”Comprehensible predictive models for business processes” 2016<br>【11】“Predicting process behaviour using deep learning”2016<br>【13】“A multi-stage deep learning approach for business process event prediction”2017</p><p>实验表明，我们的注意力竞争方法通常适用于过程事件预测的任务，并且可以与现有技术相当执行。</p><ol start="2"><li>DATSET</li></ol><p>由于DATSET的数据集较大，这里直接使用较大的超参数，同时结合4和6层的transformer</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626622668082.png" alt="超参数"></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626622687231.png" alt="DATSET"></p><pre><code>    训练的时候将超过500的流程筛选出来，这些只占了数据集1%</code></pre><p>分析，最高为0.6218。超参数较小的模型显着更糟糕，这表明它们无法完全模拟数据的复杂性。但是没有以前的数据进行对比，作者使用LSTM基本模型使用相同的超参数【256，N=4】进行对比。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021718/1626622951569.png" alt="LSTM VS  Model"></p><p>证明了所提出的基于关注力的模型比基于LSTM的模型更好。此外，根据LSTM训练时间是我们的两倍。 <strong>TODO</strong> <font color="#FF00FF">这显示了注意机制对于长跟踪长度的优势，它能够一次处理整个跟踪，而不是一次处理一个元素。</font></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文，作者提出了基于注意力机制的业务流程预测模型，通过数据集验证，不仅在较小的数据集上（BPI2013）可以接近现有技术的精确度，同时在预测复杂的的数据集也可以达到良好的效果，同时训练时间更少。</p><p>提到对于复杂的模型处理的新的思路，<strong>对于流程轨迹进行分割，对于重复的事件进行预处理，缩短流程提升预测精度</strong>或者是<strong>仅部分学习预测</strong>。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=1842</summary>
      
    
    
    
    <category term="paper" scheme="https://merlynr.github.io/categories/paper/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="paper" scheme="https://merlynr.github.io/tags/paper/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>注意力机制</title>
    <link href="https://merlynr.github.io/2021/07/16/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <id>https://merlynr.github.io/2021/07/16/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</id>
    <published>2021-07-15T16:00:00.000Z</published>
    <updated>2021-07-15T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/105335191">注意力机制到底是什么——基于常识的基本结构介绍 - 知乎</a><br><a href="https://www.zhihu.com/question/304499365">(96 封私信 / 80 条消息) 「注意力机制」是什么意思？ - 知乎</a><br><a href="https://my.oschina.net/u/876354/blog/3061863">大话注意力机制（Attention Mechanism） - 雪饼的个人空间 - OSCHINA - 中文开源技术交流社区</a><br><a href="https://zhuanlan.zhihu.com/p/148737297">attention机制中的query,key,value的概念解释 - 知乎</a></p><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>注意力机制(Attention Mechanism)是人们在机器学习模型中嵌入的一种特殊结构，用来自动学习和计算输入数据对输出数据的<font color="#1E90FF"><strong>贡献</strong></font>大小。</p><blockquote><p>目前，注意力机制已经成为深度学习领域，尤其是自然语言处理领域，应用最广泛的“组件”之一。这两年曝光度极高的BERT、GPT、Transformer等等模型或结构，都采用了注意力机制。</p></blockquote><h2 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h2><p>来自于认知工程领域提出的，类似人对于信息采集的机制—特征工程。</p><h3 id="人身上的注意力机制"><a href="#人身上的注意力机制" class="headerlink" title="人身上的注意力机制"></a>人身上的注意力机制</h3><p>去超市购物，和朋友去购物，作为提东西的工具人，不仅要体力跟的上，那么我们还需要的是跟的上朋友的步伐，人山人海中要跟上步伐确实比较困难，当我们用眼睛去一个一个寻找，把路人的信息特征衣服，脸，发色，发型~都传入到脑海中一个一个比对寻找朋友的时候，这时候不仅效率极其低而且大脑表示也遭不住，那么我们只需要记到部分明显特征发型身高等，然后扩大视野，这样效率会明显提高。<br>像这种情形，**<font color="#9932CC">有选择的处理信号</font>**，包括人类很多生物在处理外界信号时的策略，这种处理机制就是注意力机制。</p><h3 id="特征工程——模型外部的注意力机制"><a href="#特征工程——模型外部的注意力机制" class="headerlink" title="特征工程——模型外部的注意力机制"></a>特征工程——模型外部的注意力机制</h3><p>严格来说，「注意力机制」更像是一种方法论。没有严格的数学定义，而是根据具体任务目标，对关注的方向和加权模型进行调整。<br>简单的理解就是，在神经网络的隐藏层，增加「注意力机制」的加权。<br>使不符合注意力模型的内容弱化或者遗忘。</p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021717/1626502007361.png" alt="特征工程"></p><h3 id="“key-query-value”理论"><a href="#“key-query-value”理论" class="headerlink" title="“key-query-value”理论"></a>“key-query-value”理论</h3><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021717/1626502458858.png" alt="Attention机制"></p><p>将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。即可以将其本质思想改写为如下公式：<br>$Attention( Q u e r y , Source ) = \sum _ { i = 1 } ^ { L _ { x } }Similarity(Query,Key_{i})*Value_{i}$</p><blockquote><p><strong><font color="#7FFF00">个人理解</font></strong><br>可以把attention机制看作一种软寻址：Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，指的不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，**<font color="#7FFF00">取出内容的重要性根据Query和Key的相似性来决定</font>**，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。</p></blockquote><p><strong><font color="#008B8B">Attention机制的具体计算过程：</font></strong></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021717/1626508762885.png" alt="计算过程"></p><ol><li>根据Query和Key计算两者的相似性或者相关性【学习】</li><li>对第一阶段的原始分值进行归一化处理【获取权重系数】</li><li>根据权重系数对Value进行加权求和</li></ol><blockquote><p><strong><font color="#00008B">知识补充</font></strong><br>点乘又叫向量的内积、数量积，是一个向量和它在另一个向量上的投影的长度的乘积；是标量。 <strong>点乘反映着两个向量的“相似度”</strong>，两个向量越“相似”，它们的点乘越大。<br>向量叉乘求的是<strong>垂直</strong>于这两个向量<br>Cosine相似性，求余弦<br>多层感知器（Multilayer Perceptron,缩写MLP）一种通用的函数近似方法，可以被用来拟合复杂的函数，或解决分类问题</p></blockquote><p>第一阶段中根据Query和Key求相似度目前常见的方法包括：求两者的向量点积、求两者的向量 Cosine相似性或者通过再引入额外的<br>神经网络来求值如下：<br>点积：<br>$Similarity(Query,Key_{i})=Query*Key_{i}$</p><p>Cosine相似性：<br>$Similarity(Query,Key_{i})=\frac { Query<em>Key_{i} } { ||Query||</em>||Key_{i}||}$</p><p>MLP网络：<br>$Similarity(Query,Key_{i})=MLP（Query*Key_{i}$</p><p>第二阶段引入类似SoftMax的计算方式对第一阶段的相似度得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。<br>$a_{i}=Softmax(Sim_{i})= \frac{e^{Sim_{i}}}{ \textstyle \sum_{j=1}^{L_{x}}e^{Sim_{j}} }$</p><p>最后一阶段是加权求和求Attention数值：<br>$Attention(Query,Source)={\textstyle \sum_{i=1}^{L_{x}}a_{i}\cdot Value_{i}}$</p><h2 id="深度学习领域的注意力机制"><a href="#深度学习领域的注意力机制" class="headerlink" title="深度学习领域的注意力机制"></a>深度学习领域的注意力机制</h2><h3 id="注意力机制的思想和基本框架"><a href="#注意力机制的思想和基本框架" class="headerlink" title="注意力机制的思想和基本框架"></a>注意力机制的思想和基本框架</h3><blockquote><p>一些学者尝试让<font color="#9400D3">模型自己学习如何分配自己的注意力</font>，即为输入信号加权。<em>他们用注意力机制的直接目的，就是为输入的各个维度打分，然后按照得分对特征加权，以突出重要特征对下游模型或模块的影响。这也是注意力机制的基本思想。</em></p></blockquote><p><strong>一般会采用”key-query-value”理论来描述注意力机制的机理。</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/105335191&quot;&gt;注意力机制到底是什么——基于常识的基本结构介绍 - 知乎&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://www.zhihu.com/question/304499365&quot;&gt;(</summary>
      
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="process mining" scheme="https://merlynr.github.io/tags/process-mining/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://merlynr.github.io/2021/07/13/attachments/1626158803254.table/"/>
    <id>https://merlynr.github.io/2021/07/13/attachments/1626158803254.table/</id>
    <published>2021-07-13T06:46:43.362Z</published>
    <updated>2021-07-13T06:46:43.362Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>强化学习</title>
    <link href="https://merlynr.github.io/2021/07/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    <id>https://merlynr.github.io/2021/07/13/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</id>
    <published>2021-07-12T16:00:00.000Z</published>
    <updated>2021-07-12T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/steven-yang/p/6481772.html">强化学习读书笔记 - 00 - 术语和数学符号 - SNYang - 博客园</a><br><a href="https://www.cnblogs.com/wacc/p/5391209.html">强化学习笔记1 - Hiroki - 博客园</a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>监督学习在机器学习中取得了重大的成功，然而在<strong>顺序决策制定</strong>和<strong>控制问题</strong>中，比如无人直升机、无人汽车等，难以给出显式的监督信息，因此这类问题中<strong>监督模型无法学习</strong>。<br>强化学习就是为了解决这类问题而产生的。在强化学习框架中，学习算法被称为一个agent，假设这个agent处于一个环境中，两者之间存在交互。<font color="#9400D3">agent通过与环境交互不断增强对环境的适应力，故得名强化学习。</font></p><p><img src="https://gitee.com/merlynr/img-store/raw/master/2021713/1626145210759.png" alt="强化学习"></p><p>在每个时间步 $t$ ，agent：</p><ul><li>接受状态 $s _ { t }$</li><li>接受标量回报 $r _ { t }$</li><li>执行行动 $a _ { t }$</li></ul><p>环境：</p><ul><li>接受动作 $a _ { t }$</li><li>产生状态 $s _ { t }$</li><li>产生标量回报 $r _ { t }$</li></ul><h2 id="MDP-马尔科夫决策过程"><a href="#MDP-马尔科夫决策过程" class="headerlink" title="MDP(马尔科夫决策过程)"></a>MDP(马尔科夫决策过程)</h2><p>通常我们都是从MDP（马尔科夫决策过程）来了解强化学习的。MDP问题中，我们有一个五元组： $( S , A , P , \gamma , P )$</p><ul><li>$S$ :状态集，由agent所有可能的状态组成</li><li>$A$ :动作集，由agent所有可能的行动构成</li><li>$P ( s , a , s ^ { \prime } )$ :转移概率分布，表示状态s下执行动作a后下个时刻状态的概率分布</li><li>$\gamma$ :折扣因子，0≤ $\gamma$ ≤1，表示未来回报相对于当前回报的重要程度。如果 $\gamma$ =0，表示只重视当前立即回报； $\gamma$ =1表示将未来回报视为与当前回报同等重要。【<font color="#FF8C00">这块不懂，可以看后面下围棋的栗子</font>】</li><li>$R ( s , a , s ^ { \prime } )$ :标量立即回报函数。执行动作a，导致状态s转移到s′产生的回报。可以是关于状态-动作的函数 $S \times A \rightarrow R$ ，也可以是只关于状态的函数 $S \rightarrow R$ 。记t时刻的回报为 $r _ { t }$ ，为了后续表述方便，假设我们感兴趣的问题中回报函数只取决于状态，而状态-动作函数可以很容易地推广，这里暂不涉及。</li></ul><p><strong><font color="#8B008B">注：</font></strong> 这里阐述的MDP称为discounted MDP，即<font color="#00008B">带折扣因子的MDP</font>。有些MDP也可以定义为四元组： $( S , A , P , R )$ ，这是<em>因为这类MDP中使用的值函数不考虑折扣因子</em>。</p><blockquote><p>**<font color="#9932CC">马尔可夫性质</font>*<em>：当一个随机过程在给定现在状态及所有过去状态情况下，其未来状态的条件概率分布<font color="#006400">仅依赖于当前状态</font>；换句话说，在给定现在状态时，它与过去状态（即该过程的历史路径）是<font color="#1E90FF">条件独立</font>的，那么此随机过程即具有马尔可夫性质。<br>例如：</em>明天的天气（是否下大雨）仅与今天的天气（是否刮大风）有关，而与前天及以前的天气无关*。</p></blockquote><p>MDP过程具有马尔科夫性质，即给定当前状态，未来的状态与过去的状态无关。但与马尔科夫链不同的是，MDP还考虑了<strong>动作</strong>，也就是说MDP中状态的转移不仅和状态有关，还依赖于agent采取的动作。</p><p>我们可以通过下面表格了解各种马尔科夫模型的区别：</p><table><thead><tr><th></th><th>不考虑动作</th><th>考虑动作</th></tr></thead><tbody><tr><td>状态可观测</td><td>马尔科夫链（MC）</td><td>马尔科夫决策过程（MDP）</td></tr><tr><td>状态不完全可观测</td><td>隐马尔科夫模型（HMM）</td><td>不完全可观察马尔可夫决策过程（POMDP）</td></tr></tbody></table><p><strong><font color="#FF8C00">MDP的运行过程：</font></strong><br><img src="https://gitee.com/merlynr/img-store/raw/master/2021713/1626158954807.png" alt="MDP"></p><p>我们从初始状态 $s _ { 0 }$ 出发，执行某个动作 $a _ { 0 }$ ，根据转移概率分布确定下一个状态 $s _ { 1 }$ ∼ $P _ { s0a0 }$ ，接着执行动作 $a _ { 1 }$ ，再根据 $P _ { s1a1 }$ 确定 $s _ { 2 }$ …。</p><p>一个discounted MDP中，我们的目标最大化一个<font color="#1E90FF">累积未来折扣回报</font>:<br>$R _ { t } = \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } r _ { t + k + 1 }$</p><p>具体地，我们希望学得一个<strong>策略</strong>（policy），通过执行这个策略使上式最大化。策略一般可以表示为一个函数，它以<font color="#9932CC">状态</font>为输入，输出<font color="#9932CC">对应的动作</font>。策略函数可以是确定的 $\pi ( s ) = a$ ，也可以是不确定的  $\pi ( s , a ) = p ( a | s )$ （这时策略函数是一个<em>条件概率分布</em>，表示给定状态s下执行下一个动作a的概率）。当agent执行一个策略时，每个状态下agent都执行策略指定的动作。</p><p>强化学习通常具有<strong>延迟回报</strong>的特点，以下围棋为例，只有在最终决定胜负的那个时刻才有回报（赢棋为1，输棋为-1），而之前的时刻立即回报均为0。这种情况下， $R _ { t }$ 等于1或-1，这将导致我们很难衡量策略的优劣，因为即使赢了一盘棋，未必能说明策略中每一步都是好棋；同样输了一盘棋也未必能说明每一步都是坏棋。因此我们需要一个目标函数来刻画策略的长期效用。<br>为此，我们可以为策略定义一个<strong>值函数</strong>（value function）来综合评估某个策略的好坏。这个函数既可以是只关于状态的值函数 $V ^ { \pi } ( s )$ ，也可以状态-动作值函数 $Q ^ { \pi } ( s , a )$ 。<font color="#2F4F4F">状态值函数评估agent处于某个状态下的长期收益</font>， 动作值函数评估agent在某个状态下执行某个动作的长期收益。</p><p>本文后续都将以 <strong><font color="#FF8C00">状态值函</font></strong> 数为例，进行阐述。一般常用的有三种形式：</p><ol><li>$V ^ { \pi } ( s ) = E _ { \pi } [ \sum _ { k = 0 } ^ { \infty } r _ { t + k + 1 } | s _ { t } = s ]$</li><li>$V ^ { \pi } ( s ) = E _ { \pi } [ \lim _ { k \rightarrow \infty } \frac { 1 } { k } \sum _ { i = 0 } ^ { k } T _ { t + i + 1 } | s _ { t } = s ]$</li><li>$V ^ { \pi } ( s ) = E _ { \pi } [ \sum _ { k = 0 } ^ { \infty } \gamma ^ { k } r _ { t + k + 1 } | s _ { t } = s ]$</li></ol><p>其中 $E _ { \pi } [ \cdot | s _ { t } = s ]$ 表示从状态s开始，通过执行策略 $π$  得到的累积回报的期望。有些情况下，agent和环境的交互是无止境的，比如一些控制问题，这样的问题称为 <strong><font color="#9400D3">continuing task</font></strong> 。还有一种情况是我们可以把交互过程打散成一个个 **<font color="#9400D3">片段式任务</font>**（episodic task），每个片段有一个起始态和一个终止态（或称为吸收态，absorbing state），比如下棋。当每个episode结束时，我们对整个过程重启随机设置一个起始态或者从某个随机起始分布采样决定一个起始态。<br>上面三种值函数中，我们一般常用第三种形式，我把它叫做 **<font color="#9400D3">折扣值函数</font>**（discounted value function）。</p>]]></content>
    
    
    <summary type="html">【可選】文章描述</summary>
    
    
    
    <category term="algorithm" scheme="https://merlynr.github.io/categories/algorithm/"/>
    
    
    <category term="study" scheme="https://merlynr.github.io/tags/study/"/>
    
    <category term="graduate student" scheme="https://merlynr.github.io/tags/graduate-student/"/>
    
    <category term="algorithm" scheme="https://merlynr.github.io/tags/algorithm/"/>
    
    <category term="deep learning" scheme="https://merlynr.github.io/tags/deep-learning/"/>
    
    <category term="machine leaning" scheme="https://merlynr.github.io/tags/machine-leaning/"/>
    
  </entry>
  
</feed>
